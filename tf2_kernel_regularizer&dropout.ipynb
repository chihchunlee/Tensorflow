{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.355628\n",
      "batch 1: loss 2.182877\n",
      "batch 2: loss 2.110263\n",
      "batch 3: loss 2.015832\n",
      "batch 4: loss 1.922021\n",
      "batch 5: loss 1.978993\n",
      "batch 6: loss 2.069015\n",
      "batch 7: loss 1.865456\n",
      "batch 8: loss 1.859898\n",
      "batch 9: loss 1.712597\n",
      "batch 10: loss 1.659049\n",
      "batch 11: loss 1.665439\n",
      "batch 12: loss 1.547476\n",
      "batch 13: loss 1.508293\n",
      "batch 14: loss 1.659315\n",
      "batch 15: loss 1.489922\n",
      "batch 16: loss 1.312801\n",
      "batch 17: loss 1.291167\n",
      "batch 18: loss 1.151451\n",
      "batch 19: loss 1.298414\n",
      "batch 20: loss 1.188966\n",
      "batch 21: loss 1.162551\n",
      "batch 22: loss 1.093741\n",
      "batch 23: loss 1.136553\n",
      "batch 24: loss 1.126664\n",
      "batch 25: loss 0.899033\n",
      "batch 26: loss 1.176222\n",
      "batch 27: loss 0.872691\n",
      "batch 28: loss 0.983082\n",
      "batch 29: loss 0.858800\n",
      "batch 30: loss 0.971009\n",
      "batch 31: loss 0.811347\n",
      "batch 32: loss 0.705345\n",
      "batch 33: loss 0.935347\n",
      "batch 34: loss 0.716346\n",
      "batch 35: loss 0.925834\n",
      "batch 36: loss 0.683442\n",
      "batch 37: loss 0.670496\n",
      "batch 38: loss 0.636420\n",
      "batch 39: loss 0.765775\n",
      "batch 40: loss 0.751668\n",
      "batch 41: loss 0.574440\n",
      "batch 42: loss 0.628535\n",
      "batch 43: loss 0.538687\n",
      "batch 44: loss 0.564701\n",
      "batch 45: loss 0.632644\n",
      "batch 46: loss 0.564710\n",
      "batch 47: loss 0.460164\n",
      "batch 48: loss 0.592807\n",
      "batch 49: loss 0.649865\n",
      "batch 50: loss 0.632823\n",
      "batch 51: loss 0.611291\n",
      "batch 52: loss 0.564503\n",
      "batch 53: loss 0.685565\n",
      "batch 54: loss 0.725019\n",
      "batch 55: loss 0.674294\n",
      "batch 56: loss 0.423701\n",
      "batch 57: loss 0.789666\n",
      "batch 58: loss 0.634448\n",
      "batch 59: loss 0.536071\n",
      "batch 60: loss 0.544219\n",
      "batch 61: loss 0.760420\n",
      "batch 62: loss 0.402272\n",
      "batch 63: loss 0.456139\n",
      "batch 64: loss 0.455368\n",
      "batch 65: loss 0.687685\n",
      "batch 66: loss 0.511675\n",
      "batch 67: loss 0.408131\n",
      "batch 68: loss 0.571216\n",
      "batch 69: loss 0.480938\n",
      "batch 70: loss 0.617711\n",
      "batch 71: loss 0.434884\n",
      "batch 72: loss 0.568036\n",
      "batch 73: loss 0.429909\n",
      "batch 74: loss 0.393827\n",
      "batch 75: loss 0.585122\n",
      "batch 76: loss 0.508015\n",
      "batch 77: loss 0.418705\n",
      "batch 78: loss 0.529425\n",
      "batch 79: loss 0.662515\n",
      "batch 80: loss 0.568012\n",
      "batch 81: loss 0.483995\n",
      "batch 82: loss 0.402792\n",
      "batch 83: loss 0.310399\n",
      "batch 84: loss 0.388006\n",
      "batch 85: loss 0.261131\n",
      "batch 86: loss 0.557948\n",
      "batch 87: loss 0.435347\n",
      "batch 88: loss 0.446811\n",
      "batch 89: loss 0.683968\n",
      "batch 90: loss 0.404806\n",
      "batch 91: loss 0.766859\n",
      "batch 92: loss 0.633395\n",
      "batch 93: loss 0.329104\n",
      "batch 94: loss 0.510709\n",
      "batch 95: loss 0.415985\n",
      "batch 96: loss 0.508909\n",
      "batch 97: loss 0.480044\n",
      "batch 98: loss 0.383168\n",
      "batch 99: loss 0.465189\n",
      "batch 100: loss 0.429182\n",
      "batch 101: loss 0.467422\n",
      "batch 102: loss 0.374609\n",
      "batch 103: loss 0.439132\n",
      "batch 104: loss 0.682544\n",
      "batch 105: loss 0.417786\n",
      "batch 106: loss 0.403286\n",
      "batch 107: loss 0.363756\n",
      "batch 108: loss 0.357069\n",
      "batch 109: loss 0.370834\n",
      "batch 110: loss 0.288708\n",
      "batch 111: loss 0.465544\n",
      "batch 112: loss 0.420782\n",
      "batch 113: loss 0.234804\n",
      "batch 114: loss 0.450076\n",
      "batch 115: loss 0.512473\n",
      "batch 116: loss 0.480293\n",
      "batch 117: loss 0.364903\n",
      "batch 118: loss 0.316699\n",
      "batch 119: loss 0.471038\n",
      "batch 120: loss 0.522140\n",
      "batch 121: loss 0.449052\n",
      "batch 122: loss 0.447156\n",
      "batch 123: loss 0.480623\n",
      "batch 124: loss 0.259112\n",
      "batch 125: loss 0.355736\n",
      "batch 126: loss 0.277777\n",
      "batch 127: loss 0.612450\n",
      "batch 128: loss 0.284701\n",
      "batch 129: loss 0.267034\n",
      "batch 130: loss 0.524086\n",
      "batch 131: loss 0.400837\n",
      "batch 132: loss 0.351486\n",
      "batch 133: loss 0.292358\n",
      "batch 134: loss 0.465361\n",
      "batch 135: loss 0.308419\n",
      "batch 136: loss 0.505829\n",
      "batch 137: loss 0.425691\n",
      "batch 138: loss 0.401170\n",
      "batch 139: loss 0.336674\n",
      "batch 140: loss 0.253794\n",
      "batch 141: loss 0.553580\n",
      "batch 142: loss 0.549928\n",
      "batch 143: loss 0.338160\n",
      "batch 144: loss 0.356474\n",
      "batch 145: loss 0.419008\n",
      "batch 146: loss 0.325505\n",
      "batch 147: loss 0.286553\n",
      "batch 148: loss 0.334565\n",
      "batch 149: loss 0.554134\n",
      "batch 150: loss 0.589756\n",
      "batch 151: loss 0.311767\n",
      "batch 152: loss 0.405393\n",
      "batch 153: loss 0.396619\n",
      "batch 154: loss 0.446363\n",
      "batch 155: loss 0.370960\n",
      "batch 156: loss 0.271114\n",
      "batch 157: loss 0.410372\n",
      "batch 158: loss 0.681465\n",
      "batch 159: loss 0.254134\n",
      "batch 160: loss 0.481986\n",
      "batch 161: loss 0.428441\n",
      "batch 162: loss 0.343259\n",
      "batch 163: loss 0.592906\n",
      "batch 164: loss 0.447791\n",
      "batch 165: loss 0.256946\n",
      "batch 166: loss 0.621910\n",
      "batch 167: loss 0.304489\n",
      "batch 168: loss 0.781775\n",
      "batch 169: loss 0.379332\n",
      "batch 170: loss 0.482942\n",
      "batch 171: loss 0.503505\n",
      "batch 172: loss 0.307325\n",
      "batch 173: loss 0.409231\n",
      "batch 174: loss 0.358708\n",
      "batch 175: loss 0.339114\n",
      "batch 176: loss 0.334917\n",
      "batch 177: loss 0.362131\n",
      "batch 178: loss 0.274647\n",
      "batch 179: loss 0.352221\n",
      "batch 180: loss 0.232132\n",
      "batch 181: loss 0.537649\n",
      "batch 182: loss 0.260489\n",
      "batch 183: loss 0.397189\n",
      "batch 184: loss 0.381234\n",
      "batch 185: loss 0.607815\n",
      "batch 186: loss 0.424633\n",
      "batch 187: loss 0.379936\n",
      "batch 188: loss 0.459724\n",
      "batch 189: loss 0.322195\n",
      "batch 190: loss 0.158018\n",
      "batch 191: loss 0.406309\n",
      "batch 192: loss 0.344306\n",
      "batch 193: loss 0.263158\n",
      "batch 194: loss 0.281802\n",
      "batch 195: loss 0.642117\n",
      "batch 196: loss 0.207221\n",
      "batch 197: loss 0.334659\n",
      "batch 198: loss 0.273551\n",
      "batch 199: loss 0.183561\n",
      "batch 200: loss 0.510184\n",
      "batch 201: loss 0.396229\n",
      "batch 202: loss 0.323391\n",
      "batch 203: loss 0.225869\n",
      "batch 204: loss 0.245430\n",
      "batch 205: loss 0.542341\n",
      "batch 206: loss 0.307874\n",
      "batch 207: loss 0.161707\n",
      "batch 208: loss 0.331836\n",
      "batch 209: loss 0.497951\n",
      "batch 210: loss 0.235610\n",
      "batch 211: loss 0.206251\n",
      "batch 212: loss 0.497831\n",
      "batch 213: loss 0.305369\n",
      "batch 214: loss 0.302588\n",
      "batch 215: loss 0.210849\n",
      "batch 216: loss 0.309320\n",
      "batch 217: loss 0.382884\n",
      "batch 218: loss 0.349392\n",
      "batch 219: loss 0.465816\n",
      "batch 220: loss 0.312977\n",
      "batch 221: loss 0.351754\n",
      "batch 222: loss 0.229413\n",
      "batch 223: loss 0.301690\n",
      "batch 224: loss 0.305768\n",
      "batch 225: loss 0.285144\n",
      "batch 226: loss 0.320052\n",
      "batch 227: loss 0.367545\n",
      "batch 228: loss 0.425450\n",
      "batch 229: loss 0.466975\n",
      "batch 230: loss 0.241751\n",
      "batch 231: loss 0.431156\n",
      "batch 232: loss 0.281467\n",
      "batch 233: loss 0.438200\n",
      "batch 234: loss 0.602036\n",
      "batch 235: loss 0.509849\n",
      "batch 236: loss 0.627169\n",
      "batch 237: loss 0.285270\n",
      "batch 238: loss 0.308366\n",
      "batch 239: loss 0.460802\n",
      "batch 240: loss 0.259753\n",
      "batch 241: loss 0.259748\n",
      "batch 242: loss 0.165587\n",
      "batch 243: loss 0.180334\n",
      "batch 244: loss 0.222404\n",
      "batch 245: loss 0.330884\n",
      "batch 246: loss 0.205674\n",
      "batch 247: loss 0.363853\n",
      "batch 248: loss 0.383279\n",
      "batch 249: loss 0.327416\n",
      "batch 250: loss 0.333412\n",
      "batch 251: loss 0.272257\n",
      "batch 252: loss 0.164648\n",
      "batch 253: loss 0.329181\n",
      "batch 254: loss 0.275272\n",
      "batch 255: loss 0.306561\n",
      "batch 256: loss 0.414515\n",
      "batch 257: loss 0.331009\n",
      "batch 258: loss 0.458401\n",
      "batch 259: loss 0.260594\n",
      "batch 260: loss 0.164567\n",
      "batch 261: loss 0.212059\n",
      "batch 262: loss 0.282983\n",
      "batch 263: loss 0.297578\n",
      "batch 264: loss 0.368988\n",
      "batch 265: loss 0.449989\n",
      "batch 266: loss 0.547949\n",
      "batch 267: loss 0.247354\n",
      "batch 268: loss 0.207441\n",
      "batch 269: loss 0.246240\n",
      "batch 270: loss 0.367345\n",
      "batch 271: loss 0.257318\n",
      "batch 272: loss 0.216319\n",
      "batch 273: loss 0.640924\n",
      "batch 274: loss 0.273129\n",
      "batch 275: loss 0.401647\n",
      "batch 276: loss 0.325119\n",
      "batch 277: loss 0.408739\n",
      "batch 278: loss 0.223406\n",
      "batch 279: loss 0.235121\n",
      "batch 280: loss 0.220661\n",
      "batch 281: loss 0.499422\n",
      "batch 282: loss 0.331757\n",
      "batch 283: loss 0.287255\n",
      "batch 284: loss 0.383093\n",
      "batch 285: loss 0.257279\n",
      "batch 286: loss 0.438859\n",
      "batch 287: loss 0.279479\n",
      "batch 288: loss 0.328677\n",
      "batch 289: loss 0.258146\n",
      "batch 290: loss 0.233406\n",
      "batch 291: loss 0.198330\n",
      "batch 292: loss 0.234838\n",
      "batch 293: loss 0.277856\n",
      "batch 294: loss 0.206417\n",
      "batch 295: loss 0.263259\n",
      "batch 296: loss 0.214441\n",
      "batch 297: loss 0.199298\n",
      "batch 298: loss 0.233097\n",
      "batch 299: loss 0.308520\n",
      "batch 300: loss 0.214838\n",
      "batch 301: loss 0.390128\n",
      "batch 302: loss 0.152883\n",
      "batch 303: loss 0.266605\n",
      "batch 304: loss 0.213345\n",
      "batch 305: loss 0.168214\n",
      "batch 306: loss 0.462677\n",
      "batch 307: loss 0.271789\n",
      "batch 308: loss 0.341956\n",
      "batch 309: loss 0.324848\n",
      "batch 310: loss 0.159434\n",
      "batch 311: loss 0.341108\n",
      "batch 312: loss 0.188425\n",
      "batch 313: loss 0.372631\n",
      "batch 314: loss 0.104537\n",
      "batch 315: loss 0.149321\n",
      "batch 316: loss 0.215032\n",
      "batch 317: loss 0.402465\n",
      "batch 318: loss 0.159671\n",
      "batch 319: loss 0.167449\n",
      "batch 320: loss 0.190074\n",
      "batch 321: loss 0.438006\n",
      "batch 322: loss 0.270318\n",
      "batch 323: loss 0.335459\n",
      "batch 324: loss 0.557439\n",
      "batch 325: loss 0.254780\n",
      "batch 326: loss 0.120618\n",
      "batch 327: loss 0.199456\n",
      "batch 328: loss 0.293371\n",
      "batch 329: loss 0.455744\n",
      "batch 330: loss 0.567474\n",
      "batch 331: loss 0.316106\n",
      "batch 332: loss 0.297397\n",
      "batch 333: loss 0.181016\n",
      "batch 334: loss 0.324423\n",
      "batch 335: loss 0.265177\n",
      "batch 336: loss 0.306032\n",
      "batch 337: loss 0.430222\n",
      "batch 338: loss 0.347364\n",
      "batch 339: loss 0.282784\n",
      "batch 340: loss 0.281464\n",
      "batch 341: loss 0.423440\n",
      "batch 342: loss 0.335292\n",
      "batch 343: loss 0.523319\n",
      "batch 344: loss 0.284232\n",
      "batch 345: loss 0.231435\n",
      "batch 346: loss 0.185271\n",
      "batch 347: loss 0.138101\n",
      "batch 348: loss 0.329939\n",
      "batch 349: loss 0.422766\n",
      "batch 350: loss 0.468074\n",
      "batch 351: loss 0.334659\n",
      "batch 352: loss 0.136764\n",
      "batch 353: loss 0.205800\n",
      "batch 354: loss 0.262733\n",
      "batch 355: loss 0.438552\n",
      "batch 356: loss 0.265792\n",
      "batch 357: loss 0.278639\n",
      "batch 358: loss 0.205657\n",
      "batch 359: loss 0.194116\n",
      "batch 360: loss 0.424639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 361: loss 0.198288\n",
      "batch 362: loss 0.204222\n",
      "batch 363: loss 0.392609\n",
      "batch 364: loss 0.429585\n",
      "batch 365: loss 0.207812\n",
      "batch 366: loss 0.226282\n",
      "batch 367: loss 0.233647\n",
      "batch 368: loss 0.177145\n",
      "batch 369: loss 0.228648\n",
      "batch 370: loss 0.187268\n",
      "batch 371: loss 0.309484\n",
      "batch 372: loss 0.389968\n",
      "batch 373: loss 0.276500\n",
      "batch 374: loss 0.421231\n",
      "batch 375: loss 0.351088\n",
      "batch 376: loss 0.151322\n",
      "batch 377: loss 0.282568\n",
      "batch 378: loss 0.289130\n",
      "batch 379: loss 0.185934\n",
      "batch 380: loss 0.363021\n",
      "batch 381: loss 0.185297\n",
      "batch 382: loss 0.293696\n",
      "batch 383: loss 0.150312\n",
      "batch 384: loss 0.205409\n",
      "batch 385: loss 0.243246\n",
      "batch 386: loss 0.534251\n",
      "batch 387: loss 0.147858\n",
      "batch 388: loss 0.162614\n",
      "batch 389: loss 0.196077\n",
      "batch 390: loss 0.307716\n",
      "batch 391: loss 0.150157\n",
      "batch 392: loss 0.254154\n",
      "batch 393: loss 0.180444\n",
      "batch 394: loss 0.227422\n",
      "batch 395: loss 0.280263\n",
      "batch 396: loss 0.178001\n",
      "batch 397: loss 0.208558\n",
      "batch 398: loss 0.302002\n",
      "batch 399: loss 0.179969\n",
      "batch 400: loss 0.167273\n",
      "batch 401: loss 0.165520\n",
      "batch 402: loss 0.174156\n",
      "batch 403: loss 0.515320\n",
      "batch 404: loss 0.151830\n",
      "batch 405: loss 0.295220\n",
      "batch 406: loss 0.258919\n",
      "batch 407: loss 0.220043\n",
      "batch 408: loss 0.245029\n",
      "batch 409: loss 0.283996\n",
      "batch 410: loss 0.279343\n",
      "batch 411: loss 0.105156\n",
      "batch 412: loss 0.156654\n",
      "batch 413: loss 0.344414\n",
      "batch 414: loss 0.336490\n",
      "batch 415: loss 0.218155\n",
      "batch 416: loss 0.297549\n",
      "batch 417: loss 0.156226\n",
      "batch 418: loss 0.240219\n",
      "batch 419: loss 0.349117\n",
      "batch 420: loss 0.125047\n",
      "batch 421: loss 0.212312\n",
      "batch 422: loss 0.276568\n",
      "batch 423: loss 0.270952\n",
      "batch 424: loss 0.283873\n",
      "batch 425: loss 0.198220\n",
      "batch 426: loss 0.175687\n",
      "batch 427: loss 0.157753\n",
      "batch 428: loss 0.216306\n",
      "batch 429: loss 0.421940\n",
      "batch 430: loss 0.166501\n",
      "batch 431: loss 0.136952\n",
      "batch 432: loss 0.260773\n",
      "batch 433: loss 0.342528\n",
      "batch 434: loss 0.240779\n",
      "batch 435: loss 0.332171\n",
      "batch 436: loss 0.407430\n",
      "batch 437: loss 0.071391\n",
      "batch 438: loss 0.344550\n",
      "batch 439: loss 0.261692\n",
      "batch 440: loss 0.094641\n",
      "batch 441: loss 0.236381\n",
      "batch 442: loss 0.125055\n",
      "batch 443: loss 0.171532\n",
      "batch 444: loss 0.231872\n",
      "batch 445: loss 0.093393\n",
      "batch 446: loss 0.142339\n",
      "batch 447: loss 0.358586\n",
      "batch 448: loss 0.263321\n",
      "batch 449: loss 0.070548\n",
      "batch 450: loss 0.131943\n",
      "batch 451: loss 0.417119\n",
      "batch 452: loss 0.184181\n",
      "batch 453: loss 0.207146\n",
      "batch 454: loss 0.153148\n",
      "batch 455: loss 0.387064\n",
      "batch 456: loss 0.093961\n",
      "batch 457: loss 0.210235\n",
      "batch 458: loss 0.290821\n",
      "batch 459: loss 0.277620\n",
      "batch 460: loss 0.338115\n",
      "batch 461: loss 0.324188\n",
      "batch 462: loss 0.266359\n",
      "batch 463: loss 0.199796\n",
      "batch 464: loss 0.087324\n",
      "batch 465: loss 0.307894\n",
      "batch 466: loss 0.152684\n",
      "batch 467: loss 0.229921\n",
      "batch 468: loss 0.267221\n",
      "batch 469: loss 0.312737\n",
      "batch 470: loss 0.654720\n",
      "batch 471: loss 0.335469\n",
      "batch 472: loss 0.255770\n",
      "batch 473: loss 0.278206\n",
      "batch 474: loss 0.170911\n",
      "batch 475: loss 0.585914\n",
      "batch 476: loss 0.175517\n",
      "batch 477: loss 0.216546\n",
      "batch 478: loss 0.209818\n",
      "batch 479: loss 0.241365\n",
      "batch 480: loss 0.278994\n",
      "batch 481: loss 0.291012\n",
      "batch 482: loss 0.277459\n",
      "batch 483: loss 0.284288\n",
      "batch 484: loss 0.271220\n",
      "batch 485: loss 0.317067\n",
      "batch 486: loss 0.151480\n",
      "batch 487: loss 0.143379\n",
      "batch 488: loss 0.431976\n",
      "batch 489: loss 0.143954\n",
      "batch 490: loss 0.217357\n",
      "batch 491: loss 0.139558\n",
      "batch 492: loss 0.321584\n",
      "batch 493: loss 0.235797\n",
      "batch 494: loss 0.188138\n",
      "batch 495: loss 0.252837\n",
      "batch 496: loss 0.164537\n",
      "batch 497: loss 0.139802\n",
      "batch 498: loss 0.170980\n",
      "batch 499: loss 0.266757\n",
      "batch 500: loss 0.162621\n",
      "batch 501: loss 0.200678\n",
      "batch 502: loss 0.292933\n",
      "batch 503: loss 0.205045\n",
      "batch 504: loss 0.242227\n",
      "batch 505: loss 0.318775\n",
      "batch 506: loss 0.218225\n",
      "batch 507: loss 0.371187\n",
      "batch 508: loss 0.270389\n",
      "batch 509: loss 0.260419\n",
      "batch 510: loss 0.118909\n",
      "batch 511: loss 0.152573\n",
      "batch 512: loss 0.081078\n",
      "batch 513: loss 0.385839\n",
      "batch 514: loss 0.211041\n",
      "batch 515: loss 0.197185\n",
      "batch 516: loss 0.337835\n",
      "batch 517: loss 0.280721\n",
      "batch 518: loss 0.467227\n",
      "batch 519: loss 0.307110\n",
      "batch 520: loss 0.329282\n",
      "batch 521: loss 0.262923\n",
      "batch 522: loss 0.529489\n",
      "batch 523: loss 0.175973\n",
      "batch 524: loss 0.115465\n",
      "batch 525: loss 0.362603\n",
      "batch 526: loss 0.340330\n",
      "batch 527: loss 0.178421\n",
      "batch 528: loss 0.185047\n",
      "batch 529: loss 0.331306\n",
      "batch 530: loss 0.363688\n",
      "batch 531: loss 0.199781\n",
      "batch 532: loss 0.262217\n",
      "batch 533: loss 0.411603\n",
      "batch 534: loss 0.420354\n",
      "batch 535: loss 0.204351\n",
      "batch 536: loss 0.225358\n",
      "batch 537: loss 0.065418\n",
      "batch 538: loss 0.128669\n",
      "batch 539: loss 0.293294\n",
      "batch 540: loss 0.246313\n",
      "batch 541: loss 0.192407\n",
      "batch 542: loss 0.433269\n",
      "batch 543: loss 0.126970\n",
      "batch 544: loss 0.347883\n",
      "batch 545: loss 0.285533\n",
      "batch 546: loss 0.246956\n",
      "batch 547: loss 0.314706\n",
      "batch 548: loss 0.283140\n",
      "batch 549: loss 0.428026\n",
      "batch 550: loss 0.374264\n",
      "batch 551: loss 0.284753\n",
      "batch 552: loss 0.191416\n",
      "batch 553: loss 0.198838\n",
      "batch 554: loss 0.239983\n",
      "batch 555: loss 0.203765\n",
      "batch 556: loss 0.154129\n",
      "batch 557: loss 0.386008\n",
      "batch 558: loss 0.177029\n",
      "batch 559: loss 0.260588\n",
      "batch 560: loss 0.112016\n",
      "batch 561: loss 0.297801\n",
      "batch 562: loss 0.481024\n",
      "batch 563: loss 0.344584\n",
      "batch 564: loss 0.184907\n",
      "batch 565: loss 0.287459\n",
      "batch 566: loss 0.177142\n",
      "batch 567: loss 0.228612\n",
      "batch 568: loss 0.172662\n",
      "batch 569: loss 0.201470\n",
      "batch 570: loss 0.229513\n",
      "batch 571: loss 0.234144\n",
      "batch 572: loss 0.174396\n",
      "batch 573: loss 0.045875\n",
      "batch 574: loss 0.220761\n",
      "batch 575: loss 0.354819\n",
      "batch 576: loss 0.246738\n",
      "batch 577: loss 0.234145\n",
      "batch 578: loss 0.296051\n",
      "batch 579: loss 0.152342\n",
      "batch 580: loss 0.247836\n",
      "batch 581: loss 0.141629\n",
      "batch 582: loss 0.203592\n",
      "batch 583: loss 0.279044\n",
      "batch 584: loss 0.316335\n",
      "batch 585: loss 0.170502\n",
      "batch 586: loss 0.288509\n",
      "batch 587: loss 0.180711\n",
      "batch 588: loss 0.095403\n",
      "batch 589: loss 0.185902\n",
      "batch 590: loss 0.197450\n",
      "batch 591: loss 0.056886\n",
      "batch 592: loss 0.280509\n",
      "batch 593: loss 0.350904\n",
      "batch 594: loss 0.277688\n",
      "batch 595: loss 0.189894\n",
      "batch 596: loss 0.376109\n",
      "batch 597: loss 0.176230\n",
      "batch 598: loss 0.216282\n",
      "batch 599: loss 0.148850\n",
      "batch 600: loss 0.138970\n",
      "batch 601: loss 0.227187\n",
      "batch 602: loss 0.130214\n",
      "batch 603: loss 0.102577\n",
      "batch 604: loss 0.174220\n",
      "batch 605: loss 0.253670\n",
      "batch 606: loss 0.090479\n",
      "batch 607: loss 0.150574\n",
      "batch 608: loss 0.136009\n",
      "batch 609: loss 0.233427\n",
      "batch 610: loss 0.359558\n",
      "batch 611: loss 0.227349\n",
      "batch 612: loss 0.240748\n",
      "batch 613: loss 0.153506\n",
      "batch 614: loss 0.277889\n",
      "batch 615: loss 0.177160\n",
      "batch 616: loss 0.157273\n",
      "batch 617: loss 0.107827\n",
      "batch 618: loss 0.121144\n",
      "batch 619: loss 0.288195\n",
      "batch 620: loss 0.113708\n",
      "batch 621: loss 0.165567\n",
      "batch 622: loss 0.111079\n",
      "batch 623: loss 0.368597\n",
      "batch 624: loss 0.072484\n",
      "batch 625: loss 0.201535\n",
      "batch 626: loss 0.093352\n",
      "batch 627: loss 0.491322\n",
      "batch 628: loss 0.190980\n",
      "batch 629: loss 0.190046\n",
      "batch 630: loss 0.147499\n",
      "batch 631: loss 0.340007\n",
      "batch 632: loss 0.267249\n",
      "batch 633: loss 0.278017\n",
      "batch 634: loss 0.182314\n",
      "batch 635: loss 0.246799\n",
      "batch 636: loss 0.138519\n",
      "batch 637: loss 0.216197\n",
      "batch 638: loss 0.145537\n",
      "batch 639: loss 0.197105\n",
      "batch 640: loss 0.305380\n",
      "batch 641: loss 0.261153\n",
      "batch 642: loss 0.188346\n",
      "batch 643: loss 0.276090\n",
      "batch 644: loss 0.307669\n",
      "batch 645: loss 0.212866\n",
      "batch 646: loss 0.218805\n",
      "batch 647: loss 0.172526\n",
      "batch 648: loss 0.413315\n",
      "batch 649: loss 0.195418\n",
      "batch 650: loss 0.251341\n",
      "batch 651: loss 0.335317\n",
      "batch 652: loss 0.100856\n",
      "batch 653: loss 0.205770\n",
      "batch 654: loss 0.204049\n",
      "batch 655: loss 0.239537\n",
      "batch 656: loss 0.274314\n",
      "batch 657: loss 0.131180\n",
      "batch 658: loss 0.129181\n",
      "batch 659: loss 0.063884\n",
      "batch 660: loss 0.529648\n",
      "batch 661: loss 0.210440\n",
      "batch 662: loss 0.519746\n",
      "batch 663: loss 0.203944\n",
      "batch 664: loss 0.240630\n",
      "batch 665: loss 0.124200\n",
      "batch 666: loss 0.153454\n",
      "batch 667: loss 0.236363\n",
      "batch 668: loss 0.158435\n",
      "batch 669: loss 0.222836\n",
      "batch 670: loss 0.137816\n",
      "batch 671: loss 0.273732\n",
      "batch 672: loss 0.338313\n",
      "batch 673: loss 0.355849\n",
      "batch 674: loss 0.466531\n",
      "batch 675: loss 0.198035\n",
      "batch 676: loss 0.104413\n",
      "batch 677: loss 0.126099\n",
      "batch 678: loss 0.241174\n",
      "batch 679: loss 0.248041\n",
      "batch 680: loss 0.208823\n",
      "batch 681: loss 0.164811\n",
      "batch 682: loss 0.061732\n",
      "batch 683: loss 0.208322\n",
      "batch 684: loss 0.127481\n",
      "batch 685: loss 0.289338\n",
      "batch 686: loss 0.186332\n",
      "batch 687: loss 0.194797\n",
      "batch 688: loss 0.143495\n",
      "batch 689: loss 0.280304\n",
      "batch 690: loss 0.178411\n",
      "batch 691: loss 0.432507\n",
      "batch 692: loss 0.273760\n",
      "batch 693: loss 0.422379\n",
      "batch 694: loss 0.329031\n",
      "batch 695: loss 0.305225\n",
      "batch 696: loss 0.185937\n",
      "batch 697: loss 0.227764\n",
      "batch 698: loss 0.170157\n",
      "batch 699: loss 0.180760\n",
      "batch 700: loss 0.172919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 701: loss 0.078842\n",
      "batch 702: loss 0.173240\n",
      "batch 703: loss 0.220704\n",
      "batch 704: loss 0.447445\n",
      "batch 705: loss 0.111000\n",
      "batch 706: loss 0.354454\n",
      "batch 707: loss 0.204895\n",
      "batch 708: loss 0.144724\n",
      "batch 709: loss 0.081692\n",
      "batch 710: loss 0.213199\n",
      "batch 711: loss 0.231329\n",
      "batch 712: loss 0.132877\n",
      "batch 713: loss 0.281272\n",
      "batch 714: loss 0.408756\n",
      "batch 715: loss 0.168518\n",
      "batch 716: loss 0.239556\n",
      "batch 717: loss 0.130466\n",
      "batch 718: loss 0.113562\n",
      "batch 719: loss 0.361526\n",
      "batch 720: loss 0.092073\n",
      "batch 721: loss 0.111096\n",
      "batch 722: loss 0.312562\n",
      "batch 723: loss 0.159329\n",
      "batch 724: loss 0.323645\n",
      "batch 725: loss 0.093828\n",
      "batch 726: loss 0.150611\n",
      "batch 727: loss 0.222713\n",
      "batch 728: loss 0.126348\n",
      "batch 729: loss 0.213858\n",
      "batch 730: loss 0.165899\n",
      "batch 731: loss 0.102664\n",
      "batch 732: loss 0.240372\n",
      "batch 733: loss 0.041348\n",
      "batch 734: loss 0.290801\n",
      "batch 735: loss 0.173212\n",
      "batch 736: loss 0.094161\n",
      "batch 737: loss 0.395226\n",
      "batch 738: loss 0.182474\n",
      "batch 739: loss 0.194468\n",
      "batch 740: loss 0.194869\n",
      "batch 741: loss 0.123795\n",
      "batch 742: loss 0.225746\n",
      "batch 743: loss 0.245021\n",
      "batch 744: loss 0.190457\n",
      "batch 745: loss 0.125905\n",
      "batch 746: loss 0.145113\n",
      "batch 747: loss 0.479711\n",
      "batch 748: loss 0.349649\n",
      "batch 749: loss 0.309176\n",
      "batch 750: loss 0.535765\n",
      "batch 751: loss 0.204174\n",
      "batch 752: loss 0.242514\n",
      "batch 753: loss 0.285027\n",
      "batch 754: loss 0.253127\n",
      "batch 755: loss 0.137431\n",
      "batch 756: loss 0.237409\n",
      "batch 757: loss 0.252868\n",
      "batch 758: loss 0.078642\n",
      "batch 759: loss 0.269039\n",
      "batch 760: loss 0.068117\n",
      "batch 761: loss 0.305436\n",
      "batch 762: loss 0.161983\n",
      "batch 763: loss 0.250443\n",
      "batch 764: loss 0.108653\n",
      "batch 765: loss 0.320198\n",
      "batch 766: loss 0.194969\n",
      "batch 767: loss 0.117560\n",
      "batch 768: loss 0.227196\n",
      "batch 769: loss 0.146337\n",
      "batch 770: loss 0.358873\n",
      "batch 771: loss 0.295371\n",
      "batch 772: loss 0.121209\n",
      "batch 773: loss 0.187825\n",
      "batch 774: loss 0.104628\n",
      "batch 775: loss 0.131760\n",
      "batch 776: loss 0.116164\n",
      "batch 777: loss 0.227947\n",
      "batch 778: loss 0.148093\n",
      "batch 779: loss 0.151885\n",
      "batch 780: loss 0.272961\n",
      "batch 781: loss 0.088326\n",
      "batch 782: loss 0.254504\n",
      "batch 783: loss 0.155311\n",
      "batch 784: loss 0.240040\n",
      "batch 785: loss 0.157706\n",
      "batch 786: loss 0.198186\n",
      "batch 787: loss 0.303193\n",
      "batch 788: loss 0.101085\n",
      "batch 789: loss 0.160585\n",
      "batch 790: loss 0.155904\n",
      "batch 791: loss 0.131153\n",
      "batch 792: loss 0.232187\n",
      "batch 793: loss 0.136988\n",
      "batch 794: loss 0.252427\n",
      "batch 795: loss 0.167174\n",
      "batch 796: loss 0.094575\n",
      "batch 797: loss 0.155281\n",
      "batch 798: loss 0.299982\n",
      "batch 799: loss 0.131230\n",
      "batch 800: loss 0.049143\n",
      "batch 801: loss 0.215347\n",
      "batch 802: loss 0.313885\n",
      "batch 803: loss 0.102267\n",
      "batch 804: loss 0.287432\n",
      "batch 805: loss 0.157472\n",
      "batch 806: loss 0.186504\n",
      "batch 807: loss 0.228285\n",
      "batch 808: loss 0.222069\n",
      "batch 809: loss 0.160490\n",
      "batch 810: loss 0.094235\n",
      "batch 811: loss 0.109206\n",
      "batch 812: loss 0.405697\n",
      "batch 813: loss 0.297295\n",
      "batch 814: loss 0.383220\n",
      "batch 815: loss 0.091722\n",
      "batch 816: loss 0.277540\n",
      "batch 817: loss 0.278450\n",
      "batch 818: loss 0.236702\n",
      "batch 819: loss 0.079789\n",
      "batch 820: loss 0.297961\n",
      "batch 821: loss 0.142825\n",
      "batch 822: loss 0.364040\n",
      "batch 823: loss 0.127998\n",
      "batch 824: loss 0.189026\n",
      "batch 825: loss 0.360161\n",
      "batch 826: loss 0.118031\n",
      "batch 827: loss 0.127955\n",
      "batch 828: loss 0.200906\n",
      "batch 829: loss 0.242654\n",
      "batch 830: loss 0.270499\n",
      "batch 831: loss 0.270242\n",
      "batch 832: loss 0.153150\n",
      "batch 833: loss 0.158401\n",
      "batch 834: loss 0.309761\n",
      "batch 835: loss 0.325285\n",
      "batch 836: loss 0.068201\n",
      "batch 837: loss 0.280787\n",
      "batch 838: loss 0.042672\n",
      "batch 839: loss 0.277120\n",
      "batch 840: loss 0.258565\n",
      "batch 841: loss 0.193493\n",
      "batch 842: loss 0.241837\n",
      "batch 843: loss 0.181395\n",
      "batch 844: loss 0.277389\n",
      "batch 845: loss 0.237126\n",
      "batch 846: loss 0.223122\n",
      "batch 847: loss 0.067762\n",
      "batch 848: loss 0.159998\n",
      "batch 849: loss 0.387886\n",
      "batch 850: loss 0.327962\n",
      "batch 851: loss 0.123502\n",
      "batch 852: loss 0.137851\n",
      "batch 853: loss 0.187691\n",
      "batch 854: loss 0.132123\n",
      "batch 855: loss 0.229201\n",
      "batch 856: loss 0.196119\n",
      "batch 857: loss 0.142473\n",
      "batch 858: loss 0.183516\n",
      "batch 859: loss 0.096835\n",
      "batch 860: loss 0.290747\n",
      "batch 861: loss 0.187610\n",
      "batch 862: loss 0.207749\n",
      "batch 863: loss 0.145096\n",
      "batch 864: loss 0.089766\n",
      "batch 865: loss 0.159233\n",
      "batch 866: loss 0.207882\n",
      "batch 867: loss 0.339092\n",
      "batch 868: loss 0.254585\n",
      "batch 869: loss 0.165999\n",
      "batch 870: loss 0.197453\n",
      "batch 871: loss 0.133747\n",
      "batch 872: loss 0.202832\n",
      "batch 873: loss 0.219469\n",
      "batch 874: loss 0.160368\n",
      "batch 875: loss 0.090577\n",
      "batch 876: loss 0.227367\n",
      "batch 877: loss 0.068838\n",
      "batch 878: loss 0.101509\n",
      "batch 879: loss 0.106498\n",
      "batch 880: loss 0.176159\n",
      "batch 881: loss 0.325303\n",
      "batch 882: loss 0.175586\n",
      "batch 883: loss 0.167252\n",
      "batch 884: loss 0.230238\n",
      "batch 885: loss 0.320094\n",
      "batch 886: loss 0.353974\n",
      "batch 887: loss 0.142037\n",
      "batch 888: loss 0.097486\n",
      "batch 889: loss 0.111216\n",
      "batch 890: loss 0.042569\n",
      "batch 891: loss 0.086633\n",
      "batch 892: loss 0.287475\n",
      "batch 893: loss 0.206968\n",
      "batch 894: loss 0.384914\n",
      "batch 895: loss 0.213592\n",
      "batch 896: loss 0.097162\n",
      "batch 897: loss 0.234337\n",
      "batch 898: loss 0.125134\n",
      "batch 899: loss 0.307486\n",
      "batch 900: loss 0.043997\n",
      "batch 901: loss 0.358942\n",
      "batch 902: loss 0.095413\n",
      "batch 903: loss 0.118263\n",
      "batch 904: loss 0.134315\n",
      "batch 905: loss 0.129946\n",
      "batch 906: loss 0.122226\n",
      "batch 907: loss 0.555302\n",
      "batch 908: loss 0.326759\n",
      "batch 909: loss 0.265375\n",
      "batch 910: loss 0.134224\n",
      "batch 911: loss 0.343741\n",
      "batch 912: loss 0.073732\n",
      "batch 913: loss 0.224732\n",
      "batch 914: loss 0.172714\n",
      "batch 915: loss 0.096244\n",
      "batch 916: loss 0.268023\n",
      "batch 917: loss 0.176576\n",
      "batch 918: loss 0.145218\n",
      "batch 919: loss 0.072577\n",
      "batch 920: loss 0.058042\n",
      "batch 921: loss 0.165622\n",
      "batch 922: loss 0.165371\n",
      "batch 923: loss 0.163650\n",
      "batch 924: loss 0.162388\n",
      "batch 925: loss 0.106242\n",
      "batch 926: loss 0.053525\n",
      "batch 927: loss 0.253530\n",
      "batch 928: loss 0.130471\n",
      "batch 929: loss 0.214603\n",
      "batch 930: loss 0.147349\n",
      "batch 931: loss 0.242256\n",
      "batch 932: loss 0.140945\n",
      "batch 933: loss 0.044997\n",
      "batch 934: loss 0.206301\n",
      "batch 935: loss 0.244081\n",
      "batch 936: loss 0.178840\n",
      "batch 937: loss 0.263807\n",
      "batch 938: loss 0.106221\n",
      "batch 939: loss 0.168820\n",
      "batch 940: loss 0.056825\n",
      "batch 941: loss 0.235121\n",
      "batch 942: loss 0.447089\n",
      "batch 943: loss 0.181148\n",
      "batch 944: loss 0.188835\n",
      "batch 945: loss 0.077981\n",
      "batch 946: loss 0.104798\n",
      "batch 947: loss 0.127183\n",
      "batch 948: loss 0.384668\n",
      "batch 949: loss 0.254191\n",
      "batch 950: loss 0.030107\n",
      "batch 951: loss 0.210883\n",
      "batch 952: loss 0.105787\n",
      "batch 953: loss 0.178328\n",
      "batch 954: loss 0.088203\n",
      "batch 955: loss 0.295724\n",
      "batch 956: loss 0.203291\n",
      "batch 957: loss 0.220735\n",
      "batch 958: loss 0.186806\n",
      "batch 959: loss 0.145146\n",
      "batch 960: loss 0.107845\n",
      "batch 961: loss 0.132072\n",
      "batch 962: loss 0.308530\n",
      "batch 963: loss 0.201369\n",
      "batch 964: loss 0.178503\n",
      "batch 965: loss 0.367752\n",
      "batch 966: loss 0.113978\n",
      "batch 967: loss 0.341984\n",
      "batch 968: loss 0.210460\n",
      "batch 969: loss 0.125356\n",
      "batch 970: loss 0.120926\n",
      "batch 971: loss 0.094617\n",
      "batch 972: loss 0.218902\n",
      "batch 973: loss 0.324580\n",
      "batch 974: loss 0.153023\n",
      "batch 975: loss 0.174250\n",
      "batch 976: loss 0.050847\n",
      "batch 977: loss 0.140521\n",
      "batch 978: loss 0.432094\n",
      "batch 979: loss 0.128815\n",
      "batch 980: loss 0.228141\n",
      "batch 981: loss 0.072014\n",
      "batch 982: loss 0.093747\n",
      "batch 983: loss 0.151589\n",
      "batch 984: loss 0.343650\n",
      "batch 985: loss 0.218406\n",
      "batch 986: loss 0.243588\n",
      "batch 987: loss 0.350990\n",
      "batch 988: loss 0.297183\n",
      "batch 989: loss 0.092491\n",
      "batch 990: loss 0.152149\n",
      "batch 991: loss 0.067513\n",
      "batch 992: loss 0.231300\n",
      "batch 993: loss 0.411783\n",
      "batch 994: loss 0.181014\n",
      "batch 995: loss 0.202284\n",
      "batch 996: loss 0.068329\n",
      "batch 997: loss 0.160872\n",
      "batch 998: loss 0.328826\n",
      "batch 999: loss 0.161698\n",
      "batch 1000: loss 0.450752\n",
      "batch 1001: loss 0.163287\n",
      "batch 1002: loss 0.177615\n",
      "batch 1003: loss 0.212435\n",
      "batch 1004: loss 0.133064\n",
      "batch 1005: loss 0.218129\n",
      "batch 1006: loss 0.181400\n",
      "batch 1007: loss 0.204477\n",
      "batch 1008: loss 0.144968\n",
      "batch 1009: loss 0.101742\n",
      "batch 1010: loss 0.083622\n",
      "batch 1011: loss 0.080828\n",
      "batch 1012: loss 0.155268\n",
      "batch 1013: loss 0.176507\n",
      "batch 1014: loss 0.120813\n",
      "batch 1015: loss 0.263974\n",
      "batch 1016: loss 0.154147\n",
      "batch 1017: loss 0.260836\n",
      "batch 1018: loss 0.089329\n",
      "batch 1019: loss 0.056995\n",
      "batch 1020: loss 0.103615\n",
      "batch 1021: loss 0.200788\n",
      "batch 1022: loss 0.248007\n",
      "batch 1023: loss 0.043191\n",
      "batch 1024: loss 0.103190\n",
      "batch 1025: loss 0.517713\n",
      "batch 1026: loss 0.260850\n",
      "batch 1027: loss 0.140450\n",
      "batch 1028: loss 0.156062\n",
      "batch 1029: loss 0.167304\n",
      "batch 1030: loss 0.180305\n",
      "batch 1031: loss 0.081610\n",
      "batch 1032: loss 0.170974\n",
      "batch 1033: loss 0.201564\n",
      "batch 1034: loss 0.090875\n",
      "batch 1035: loss 0.131850\n",
      "batch 1036: loss 0.199472\n",
      "batch 1037: loss 0.049251\n",
      "batch 1038: loss 0.065245\n",
      "batch 1039: loss 0.097491\n",
      "batch 1040: loss 0.212832\n",
      "batch 1041: loss 0.138956\n",
      "batch 1042: loss 0.159083\n",
      "batch 1043: loss 0.202880\n",
      "batch 1044: loss 0.141207\n",
      "batch 1045: loss 0.095760\n",
      "batch 1046: loss 0.102537\n",
      "batch 1047: loss 0.052821\n",
      "batch 1048: loss 0.267390\n",
      "batch 1049: loss 0.210128\n",
      "batch 1050: loss 0.134140\n",
      "batch 1051: loss 0.228937\n",
      "batch 1052: loss 0.158972\n",
      "batch 1053: loss 0.205741\n",
      "batch 1054: loss 0.047186\n",
      "batch 1055: loss 0.221128\n",
      "batch 1056: loss 0.127775\n",
      "batch 1057: loss 0.123280\n",
      "batch 1058: loss 0.117634\n",
      "batch 1059: loss 0.066343\n",
      "batch 1060: loss 0.342381\n",
      "batch 1061: loss 0.085021\n",
      "batch 1062: loss 0.198657\n",
      "batch 1063: loss 0.146343\n",
      "batch 1064: loss 0.171062\n",
      "batch 1065: loss 0.103097\n",
      "batch 1066: loss 0.101225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1067: loss 0.134571\n",
      "batch 1068: loss 0.217119\n",
      "batch 1069: loss 0.182993\n",
      "batch 1070: loss 0.134819\n",
      "batch 1071: loss 0.275676\n",
      "batch 1072: loss 0.032868\n",
      "batch 1073: loss 0.270422\n",
      "batch 1074: loss 0.166943\n",
      "batch 1075: loss 0.089149\n",
      "batch 1076: loss 0.173658\n",
      "batch 1077: loss 0.117611\n",
      "batch 1078: loss 0.263426\n",
      "batch 1079: loss 0.113881\n",
      "batch 1080: loss 0.094509\n",
      "batch 1081: loss 0.185416\n",
      "batch 1082: loss 0.369902\n",
      "batch 1083: loss 0.134503\n",
      "batch 1084: loss 0.090664\n",
      "batch 1085: loss 0.084974\n",
      "batch 1086: loss 0.130885\n",
      "batch 1087: loss 0.243194\n",
      "batch 1088: loss 0.065666\n",
      "batch 1089: loss 0.282935\n",
      "batch 1090: loss 0.059518\n",
      "batch 1091: loss 0.397819\n",
      "batch 1092: loss 0.112928\n",
      "batch 1093: loss 0.159376\n",
      "batch 1094: loss 0.078019\n",
      "batch 1095: loss 0.224094\n",
      "batch 1096: loss 0.393412\n",
      "batch 1097: loss 0.269618\n",
      "batch 1098: loss 0.206974\n",
      "batch 1099: loss 0.634742\n",
      "batch 1100: loss 0.103223\n",
      "batch 1101: loss 0.107710\n",
      "batch 1102: loss 0.047399\n",
      "batch 1103: loss 0.105512\n",
      "batch 1104: loss 0.189481\n",
      "batch 1105: loss 0.209161\n",
      "batch 1106: loss 0.062683\n",
      "batch 1107: loss 0.121071\n",
      "batch 1108: loss 0.142520\n",
      "batch 1109: loss 0.153849\n",
      "batch 1110: loss 0.165570\n",
      "batch 1111: loss 0.186823\n",
      "batch 1112: loss 0.084627\n",
      "batch 1113: loss 0.145989\n",
      "batch 1114: loss 0.184130\n",
      "batch 1115: loss 0.124522\n",
      "batch 1116: loss 0.046378\n",
      "batch 1117: loss 0.202557\n",
      "batch 1118: loss 0.132947\n",
      "batch 1119: loss 0.099866\n",
      "batch 1120: loss 0.159895\n",
      "batch 1121: loss 0.162025\n",
      "batch 1122: loss 0.153237\n",
      "batch 1123: loss 0.111141\n",
      "batch 1124: loss 0.072368\n",
      "batch 1125: loss 0.333229\n",
      "batch 1126: loss 0.122819\n",
      "batch 1127: loss 0.104159\n",
      "batch 1128: loss 0.151909\n",
      "batch 1129: loss 0.176860\n",
      "batch 1130: loss 0.080518\n",
      "batch 1131: loss 0.085458\n",
      "batch 1132: loss 0.210677\n",
      "batch 1133: loss 0.158363\n",
      "batch 1134: loss 0.183454\n",
      "batch 1135: loss 0.228935\n",
      "batch 1136: loss 0.197873\n",
      "batch 1137: loss 0.177637\n",
      "batch 1138: loss 0.374357\n",
      "batch 1139: loss 0.081664\n",
      "batch 1140: loss 0.106860\n",
      "batch 1141: loss 0.070839\n",
      "batch 1142: loss 0.240021\n",
      "batch 1143: loss 0.347914\n",
      "batch 1144: loss 0.110925\n",
      "batch 1145: loss 0.034010\n",
      "batch 1146: loss 0.117034\n",
      "batch 1147: loss 0.116870\n",
      "batch 1148: loss 0.122091\n",
      "batch 1149: loss 0.239505\n",
      "batch 1150: loss 0.106682\n",
      "batch 1151: loss 0.080297\n",
      "batch 1152: loss 0.395236\n",
      "batch 1153: loss 0.104120\n",
      "batch 1154: loss 0.164347\n",
      "batch 1155: loss 0.250869\n",
      "batch 1156: loss 0.117660\n",
      "batch 1157: loss 0.171969\n",
      "batch 1158: loss 0.062824\n",
      "batch 1159: loss 0.065525\n",
      "batch 1160: loss 0.205036\n",
      "batch 1161: loss 0.073145\n",
      "batch 1162: loss 0.129995\n",
      "batch 1163: loss 0.056909\n",
      "batch 1164: loss 0.191654\n",
      "batch 1165: loss 0.213387\n",
      "batch 1166: loss 0.214133\n",
      "batch 1167: loss 0.069832\n",
      "batch 1168: loss 0.071880\n",
      "batch 1169: loss 0.111374\n",
      "batch 1170: loss 0.179374\n",
      "batch 1171: loss 0.043736\n",
      "batch 1172: loss 0.178975\n",
      "batch 1173: loss 0.234638\n",
      "batch 1174: loss 0.102364\n",
      "batch 1175: loss 0.211639\n",
      "batch 1176: loss 0.063242\n",
      "batch 1177: loss 0.165545\n",
      "batch 1178: loss 0.045347\n",
      "batch 1179: loss 0.206871\n",
      "batch 1180: loss 0.106677\n",
      "batch 1181: loss 0.592028\n",
      "batch 1182: loss 0.100947\n",
      "batch 1183: loss 0.103503\n",
      "batch 1184: loss 0.076948\n",
      "batch 1185: loss 0.104199\n",
      "batch 1186: loss 0.247900\n",
      "batch 1187: loss 0.221807\n",
      "batch 1188: loss 0.210346\n",
      "batch 1189: loss 0.130285\n",
      "batch 1190: loss 0.263766\n",
      "batch 1191: loss 0.255189\n",
      "batch 1192: loss 0.254144\n",
      "batch 1193: loss 0.131073\n",
      "batch 1194: loss 0.080354\n",
      "batch 1195: loss 0.188186\n",
      "batch 1196: loss 0.180268\n",
      "batch 1197: loss 0.174750\n",
      "batch 1198: loss 0.152107\n",
      "batch 1199: loss 0.160997\n",
      "batch 1200: loss 0.242758\n",
      "batch 1201: loss 0.216302\n",
      "batch 1202: loss 0.180701\n",
      "batch 1203: loss 0.106163\n",
      "batch 1204: loss 0.213464\n",
      "batch 1205: loss 0.078228\n",
      "batch 1206: loss 0.172246\n",
      "batch 1207: loss 0.175806\n",
      "batch 1208: loss 0.172855\n",
      "batch 1209: loss 0.159958\n",
      "batch 1210: loss 0.045911\n",
      "batch 1211: loss 0.248612\n",
      "batch 1212: loss 0.063024\n",
      "batch 1213: loss 0.164225\n",
      "batch 1214: loss 0.350093\n",
      "batch 1215: loss 0.198101\n",
      "batch 1216: loss 0.103564\n",
      "batch 1217: loss 0.097589\n",
      "batch 1218: loss 0.094203\n",
      "batch 1219: loss 0.190376\n",
      "batch 1220: loss 0.210084\n",
      "batch 1221: loss 0.331741\n",
      "batch 1222: loss 0.195804\n",
      "batch 1223: loss 0.309596\n",
      "batch 1224: loss 0.111735\n",
      "batch 1225: loss 0.139260\n",
      "batch 1226: loss 0.146015\n",
      "batch 1227: loss 0.256060\n",
      "batch 1228: loss 0.049985\n",
      "batch 1229: loss 0.127061\n",
      "batch 1230: loss 0.362312\n",
      "batch 1231: loss 0.166978\n",
      "batch 1232: loss 0.086806\n",
      "batch 1233: loss 0.050446\n",
      "batch 1234: loss 0.147063\n",
      "batch 1235: loss 0.251935\n",
      "batch 1236: loss 0.084172\n",
      "batch 1237: loss 0.097244\n",
      "batch 1238: loss 0.153477\n",
      "batch 1239: loss 0.165696\n",
      "batch 1240: loss 0.113013\n",
      "batch 1241: loss 0.056014\n",
      "batch 1242: loss 0.131082\n",
      "batch 1243: loss 0.091424\n",
      "batch 1244: loss 0.141318\n",
      "batch 1245: loss 0.286269\n",
      "batch 1246: loss 0.067726\n",
      "batch 1247: loss 0.236412\n",
      "batch 1248: loss 0.232429\n",
      "batch 1249: loss 0.302709\n",
      "batch 1250: loss 0.216570\n",
      "batch 1251: loss 0.220373\n",
      "batch 1252: loss 0.084771\n",
      "batch 1253: loss 0.081575\n",
      "batch 1254: loss 0.230466\n",
      "batch 1255: loss 0.085156\n",
      "batch 1256: loss 0.061106\n",
      "batch 1257: loss 0.153811\n",
      "batch 1258: loss 0.165605\n",
      "batch 1259: loss 0.154026\n",
      "batch 1260: loss 0.090981\n",
      "batch 1261: loss 0.107311\n",
      "batch 1262: loss 0.141013\n",
      "batch 1263: loss 0.371968\n",
      "batch 1264: loss 0.109893\n",
      "batch 1265: loss 0.079719\n",
      "batch 1266: loss 0.086591\n",
      "batch 1267: loss 0.177946\n",
      "batch 1268: loss 0.040496\n",
      "batch 1269: loss 0.127993\n",
      "batch 1270: loss 0.269151\n",
      "batch 1271: loss 0.195838\n",
      "batch 1272: loss 0.198821\n",
      "batch 1273: loss 0.168189\n",
      "batch 1274: loss 0.150830\n",
      "batch 1275: loss 0.139344\n",
      "batch 1276: loss 0.158027\n",
      "batch 1277: loss 0.127867\n",
      "batch 1278: loss 0.119463\n",
      "batch 1279: loss 0.067172\n",
      "batch 1280: loss 0.081263\n",
      "batch 1281: loss 0.102218\n",
      "batch 1282: loss 0.124564\n",
      "batch 1283: loss 0.138705\n",
      "batch 1284: loss 0.203013\n",
      "batch 1285: loss 0.176385\n",
      "batch 1286: loss 0.121361\n",
      "batch 1287: loss 0.120918\n",
      "batch 1288: loss 0.169452\n",
      "batch 1289: loss 0.040777\n",
      "batch 1290: loss 0.162396\n",
      "batch 1291: loss 0.168062\n",
      "batch 1292: loss 0.145489\n",
      "batch 1293: loss 0.196692\n",
      "batch 1294: loss 0.154485\n",
      "batch 1295: loss 0.163286\n",
      "batch 1296: loss 0.324634\n",
      "batch 1297: loss 0.261166\n",
      "batch 1298: loss 0.045578\n",
      "batch 1299: loss 0.123238\n",
      "batch 1300: loss 0.198044\n",
      "batch 1301: loss 0.041067\n",
      "batch 1302: loss 0.143835\n",
      "batch 1303: loss 0.288935\n",
      "batch 1304: loss 0.079111\n",
      "batch 1305: loss 0.053590\n",
      "batch 1306: loss 0.113185\n",
      "batch 1307: loss 0.048464\n",
      "batch 1308: loss 0.351955\n",
      "batch 1309: loss 0.115849\n",
      "batch 1310: loss 0.101772\n",
      "batch 1311: loss 0.159380\n",
      "batch 1312: loss 0.214617\n",
      "batch 1313: loss 0.130977\n",
      "batch 1314: loss 0.050210\n",
      "batch 1315: loss 0.065992\n",
      "batch 1316: loss 0.055219\n",
      "batch 1317: loss 0.205916\n",
      "batch 1318: loss 0.056623\n",
      "batch 1319: loss 0.242650\n",
      "batch 1320: loss 0.129582\n",
      "batch 1321: loss 0.191666\n",
      "batch 1322: loss 0.211072\n",
      "batch 1323: loss 0.089825\n",
      "batch 1324: loss 0.126549\n",
      "batch 1325: loss 0.280253\n",
      "batch 1326: loss 0.094613\n",
      "batch 1327: loss 0.104655\n",
      "batch 1328: loss 0.124823\n",
      "batch 1329: loss 0.034051\n",
      "batch 1330: loss 0.108633\n",
      "batch 1331: loss 0.084346\n",
      "batch 1332: loss 0.134572\n",
      "batch 1333: loss 0.218018\n",
      "batch 1334: loss 0.257809\n",
      "batch 1335: loss 0.074017\n",
      "batch 1336: loss 0.071329\n",
      "batch 1337: loss 0.170396\n",
      "batch 1338: loss 0.134439\n",
      "batch 1339: loss 0.096035\n",
      "batch 1340: loss 0.233538\n",
      "batch 1341: loss 0.079772\n",
      "batch 1342: loss 0.187597\n",
      "batch 1343: loss 0.128145\n",
      "batch 1344: loss 0.102344\n",
      "batch 1345: loss 0.212098\n",
      "batch 1346: loss 0.102488\n",
      "batch 1347: loss 0.084679\n",
      "batch 1348: loss 0.178231\n",
      "batch 1349: loss 0.253571\n",
      "batch 1350: loss 0.323639\n",
      "batch 1351: loss 0.148089\n",
      "batch 1352: loss 0.142209\n",
      "batch 1353: loss 0.119051\n",
      "batch 1354: loss 0.177671\n",
      "batch 1355: loss 0.244627\n",
      "batch 1356: loss 0.115177\n",
      "batch 1357: loss 0.086277\n",
      "batch 1358: loss 0.148658\n",
      "batch 1359: loss 0.062854\n",
      "batch 1360: loss 0.239457\n",
      "batch 1361: loss 0.244820\n",
      "batch 1362: loss 0.131511\n",
      "batch 1363: loss 0.110552\n",
      "batch 1364: loss 0.282527\n",
      "batch 1365: loss 0.081998\n",
      "batch 1366: loss 0.136821\n",
      "batch 1367: loss 0.087394\n",
      "batch 1368: loss 0.125228\n",
      "batch 1369: loss 0.054397\n",
      "batch 1370: loss 0.054772\n",
      "batch 1371: loss 0.324112\n",
      "batch 1372: loss 0.125128\n",
      "batch 1373: loss 0.149140\n",
      "batch 1374: loss 0.200859\n",
      "batch 1375: loss 0.098413\n",
      "batch 1376: loss 0.194651\n",
      "batch 1377: loss 0.076456\n",
      "batch 1378: loss 0.239544\n",
      "batch 1379: loss 0.214847\n",
      "batch 1380: loss 0.109888\n",
      "batch 1381: loss 0.176776\n",
      "batch 1382: loss 0.123185\n",
      "batch 1383: loss 0.115123\n",
      "batch 1384: loss 0.058611\n",
      "batch 1385: loss 0.145960\n",
      "batch 1386: loss 0.195316\n",
      "batch 1387: loss 0.191611\n",
      "batch 1388: loss 0.048143\n",
      "batch 1389: loss 0.085956\n",
      "batch 1390: loss 0.214486\n",
      "batch 1391: loss 0.084703\n",
      "batch 1392: loss 0.234893\n",
      "batch 1393: loss 0.037162\n",
      "batch 1394: loss 0.289461\n",
      "batch 1395: loss 0.095124\n",
      "batch 1396: loss 0.093713\n",
      "batch 1397: loss 0.090679\n",
      "batch 1398: loss 0.209202\n",
      "batch 1399: loss 0.289850\n",
      "batch 1400: loss 0.111580\n",
      "batch 1401: loss 0.236545\n",
      "batch 1402: loss 0.111357\n",
      "batch 1403: loss 0.218101\n",
      "batch 1404: loss 0.044734\n",
      "batch 1405: loss 0.157549\n",
      "batch 1406: loss 0.196155\n",
      "batch 1407: loss 0.235891\n",
      "batch 1408: loss 0.084817\n",
      "batch 1409: loss 0.290748\n",
      "batch 1410: loss 0.148276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1411: loss 0.053276\n",
      "batch 1412: loss 0.086477\n",
      "batch 1413: loss 0.267957\n",
      "batch 1414: loss 0.148571\n",
      "batch 1415: loss 0.087308\n",
      "batch 1416: loss 0.121278\n",
      "batch 1417: loss 0.210576\n",
      "batch 1418: loss 0.218443\n",
      "batch 1419: loss 0.259052\n",
      "batch 1420: loss 0.094896\n",
      "batch 1421: loss 0.064542\n",
      "batch 1422: loss 0.275740\n",
      "batch 1423: loss 0.183538\n",
      "batch 1424: loss 0.131137\n",
      "batch 1425: loss 0.102233\n",
      "batch 1426: loss 0.225624\n",
      "batch 1427: loss 0.127284\n",
      "batch 1428: loss 0.105332\n",
      "batch 1429: loss 0.229272\n",
      "batch 1430: loss 0.044808\n",
      "batch 1431: loss 0.141671\n",
      "batch 1432: loss 0.045562\n",
      "batch 1433: loss 0.119901\n",
      "batch 1434: loss 0.080404\n",
      "batch 1435: loss 0.132061\n",
      "batch 1436: loss 0.261130\n",
      "batch 1437: loss 0.046229\n",
      "batch 1438: loss 0.400458\n",
      "batch 1439: loss 0.247831\n",
      "batch 1440: loss 0.117507\n",
      "batch 1441: loss 0.219576\n",
      "batch 1442: loss 0.246362\n",
      "batch 1443: loss 0.267738\n",
      "batch 1444: loss 0.186810\n",
      "batch 1445: loss 0.111919\n",
      "batch 1446: loss 0.096704\n",
      "batch 1447: loss 0.067791\n",
      "batch 1448: loss 0.135071\n",
      "batch 1449: loss 0.049477\n",
      "batch 1450: loss 0.217217\n",
      "batch 1451: loss 0.052637\n",
      "batch 1452: loss 0.205645\n",
      "batch 1453: loss 0.119121\n",
      "batch 1454: loss 0.068880\n",
      "batch 1455: loss 0.184899\n",
      "batch 1456: loss 0.153374\n",
      "batch 1457: loss 0.144517\n",
      "batch 1458: loss 0.219347\n",
      "batch 1459: loss 0.149801\n",
      "batch 1460: loss 0.255035\n",
      "batch 1461: loss 0.242522\n",
      "batch 1462: loss 0.158708\n",
      "batch 1463: loss 0.188541\n",
      "batch 1464: loss 0.092533\n",
      "batch 1465: loss 0.169873\n",
      "batch 1466: loss 0.019626\n",
      "batch 1467: loss 0.261280\n",
      "batch 1468: loss 0.141234\n",
      "batch 1469: loss 0.185752\n",
      "batch 1470: loss 0.078622\n",
      "batch 1471: loss 0.102757\n",
      "batch 1472: loss 0.221868\n",
      "batch 1473: loss 0.064250\n",
      "batch 1474: loss 0.131786\n",
      "batch 1475: loss 0.243799\n",
      "batch 1476: loss 0.054571\n",
      "batch 1477: loss 0.059843\n",
      "batch 1478: loss 0.337129\n",
      "batch 1479: loss 0.123226\n",
      "batch 1480: loss 0.172655\n",
      "batch 1481: loss 0.226667\n",
      "batch 1482: loss 0.105043\n",
      "batch 1483: loss 0.076613\n",
      "batch 1484: loss 0.100094\n",
      "batch 1485: loss 0.262172\n",
      "batch 1486: loss 0.196756\n",
      "batch 1487: loss 0.150472\n",
      "batch 1488: loss 0.053599\n",
      "batch 1489: loss 0.103295\n",
      "batch 1490: loss 0.208019\n",
      "batch 1491: loss 0.242662\n",
      "batch 1492: loss 0.172323\n",
      "batch 1493: loss 0.102757\n",
      "batch 1494: loss 0.080043\n",
      "batch 1495: loss 0.178838\n",
      "batch 1496: loss 0.196547\n",
      "batch 1497: loss 0.078628\n",
      "batch 1498: loss 0.048455\n",
      "batch 1499: loss 0.094124\n",
      "batch 1500: loss 0.255100\n",
      "batch 1501: loss 0.232714\n",
      "batch 1502: loss 0.308756\n",
      "batch 1503: loss 0.191763\n",
      "batch 1504: loss 0.126723\n",
      "batch 1505: loss 0.052105\n",
      "batch 1506: loss 0.256190\n",
      "batch 1507: loss 0.176295\n",
      "batch 1508: loss 0.091568\n",
      "batch 1509: loss 0.097115\n",
      "batch 1510: loss 0.111841\n",
      "batch 1511: loss 0.120241\n",
      "batch 1512: loss 0.222368\n",
      "batch 1513: loss 0.199483\n",
      "batch 1514: loss 0.061210\n",
      "batch 1515: loss 0.215805\n",
      "batch 1516: loss 0.043712\n",
      "batch 1517: loss 0.192667\n",
      "batch 1518: loss 0.137424\n",
      "batch 1519: loss 0.072549\n",
      "batch 1520: loss 0.121464\n",
      "batch 1521: loss 0.135009\n",
      "batch 1522: loss 0.172892\n",
      "batch 1523: loss 0.157694\n",
      "batch 1524: loss 0.226614\n",
      "batch 1525: loss 0.114800\n",
      "batch 1526: loss 0.085606\n",
      "batch 1527: loss 0.177567\n",
      "batch 1528: loss 0.116166\n",
      "batch 1529: loss 0.235311\n",
      "batch 1530: loss 0.050020\n",
      "batch 1531: loss 0.053901\n",
      "batch 1532: loss 0.176689\n",
      "batch 1533: loss 0.092549\n",
      "batch 1534: loss 0.072791\n",
      "batch 1535: loss 0.062170\n",
      "batch 1536: loss 0.135947\n",
      "batch 1537: loss 0.245036\n",
      "batch 1538: loss 0.170946\n",
      "batch 1539: loss 0.115505\n",
      "batch 1540: loss 0.401231\n",
      "batch 1541: loss 0.044795\n",
      "batch 1542: loss 0.328690\n",
      "batch 1543: loss 0.080822\n",
      "batch 1544: loss 0.031917\n",
      "batch 1545: loss 0.177087\n",
      "batch 1546: loss 0.217344\n",
      "batch 1547: loss 0.146390\n",
      "batch 1548: loss 0.096684\n",
      "batch 1549: loss 0.131292\n",
      "batch 1550: loss 0.089212\n",
      "batch 1551: loss 0.139908\n",
      "batch 1552: loss 0.160697\n",
      "batch 1553: loss 0.327367\n",
      "batch 1554: loss 0.410740\n",
      "batch 1555: loss 0.132604\n",
      "batch 1556: loss 0.143329\n",
      "batch 1557: loss 0.158533\n",
      "batch 1558: loss 0.063709\n",
      "batch 1559: loss 0.032710\n",
      "batch 1560: loss 0.102024\n",
      "batch 1561: loss 0.112475\n",
      "batch 1562: loss 0.119001\n",
      "batch 1563: loss 0.114596\n",
      "batch 1564: loss 0.218974\n",
      "batch 1565: loss 0.106091\n",
      "batch 1566: loss 0.113988\n",
      "batch 1567: loss 0.110967\n",
      "batch 1568: loss 0.143443\n",
      "batch 1569: loss 0.157490\n",
      "batch 1570: loss 0.121230\n",
      "batch 1571: loss 0.130544\n",
      "batch 1572: loss 0.221266\n",
      "batch 1573: loss 0.130728\n",
      "batch 1574: loss 0.098329\n",
      "batch 1575: loss 0.300830\n",
      "batch 1576: loss 0.141062\n",
      "batch 1577: loss 0.107813\n",
      "batch 1578: loss 0.098152\n",
      "batch 1579: loss 0.191191\n",
      "batch 1580: loss 0.069128\n",
      "batch 1581: loss 0.124949\n",
      "batch 1582: loss 0.158340\n",
      "batch 1583: loss 0.086356\n",
      "batch 1584: loss 0.174934\n",
      "batch 1585: loss 0.153518\n",
      "batch 1586: loss 0.405391\n",
      "batch 1587: loss 0.093590\n",
      "batch 1588: loss 0.283066\n",
      "batch 1589: loss 0.087763\n",
      "batch 1590: loss 0.154997\n",
      "batch 1591: loss 0.053141\n",
      "batch 1592: loss 0.061823\n",
      "batch 1593: loss 0.055561\n",
      "batch 1594: loss 0.068372\n",
      "batch 1595: loss 0.275069\n",
      "batch 1596: loss 0.061919\n",
      "batch 1597: loss 0.082937\n",
      "batch 1598: loss 0.143224\n",
      "batch 1599: loss 0.215897\n",
      "batch 1600: loss 0.129091\n",
      "batch 1601: loss 0.054323\n",
      "batch 1602: loss 0.170670\n",
      "batch 1603: loss 0.078623\n",
      "batch 1604: loss 0.094255\n",
      "batch 1605: loss 0.055549\n",
      "batch 1606: loss 0.076726\n",
      "batch 1607: loss 0.048213\n",
      "batch 1608: loss 0.153216\n",
      "batch 1609: loss 0.067713\n",
      "batch 1610: loss 0.295654\n",
      "batch 1611: loss 0.069077\n",
      "batch 1612: loss 0.093932\n",
      "batch 1613: loss 0.216573\n",
      "batch 1614: loss 0.185957\n",
      "batch 1615: loss 0.073024\n",
      "batch 1616: loss 0.165705\n",
      "batch 1617: loss 0.129285\n",
      "batch 1618: loss 0.122287\n",
      "batch 1619: loss 0.198352\n",
      "batch 1620: loss 0.143021\n",
      "batch 1621: loss 0.116471\n",
      "batch 1622: loss 0.176717\n",
      "batch 1623: loss 0.053310\n",
      "batch 1624: loss 0.041933\n",
      "batch 1625: loss 0.081314\n",
      "batch 1626: loss 0.051486\n",
      "batch 1627: loss 0.317326\n",
      "batch 1628: loss 0.180317\n",
      "batch 1629: loss 0.127970\n",
      "batch 1630: loss 0.047161\n",
      "batch 1631: loss 0.174134\n",
      "batch 1632: loss 0.116734\n",
      "batch 1633: loss 0.070011\n",
      "batch 1634: loss 0.087173\n",
      "batch 1635: loss 0.165774\n",
      "batch 1636: loss 0.167273\n",
      "batch 1637: loss 0.083967\n",
      "batch 1638: loss 0.135652\n",
      "batch 1639: loss 0.086820\n",
      "batch 1640: loss 0.139018\n",
      "batch 1641: loss 0.113492\n",
      "batch 1642: loss 0.056744\n",
      "batch 1643: loss 0.140516\n",
      "batch 1644: loss 0.196558\n",
      "batch 1645: loss 0.057152\n",
      "batch 1646: loss 0.130004\n",
      "batch 1647: loss 0.041790\n",
      "batch 1648: loss 0.232098\n",
      "batch 1649: loss 0.166725\n",
      "batch 1650: loss 0.169001\n",
      "batch 1651: loss 0.103285\n",
      "batch 1652: loss 0.107721\n",
      "batch 1653: loss 0.073110\n",
      "batch 1654: loss 0.166186\n",
      "batch 1655: loss 0.117531\n",
      "batch 1656: loss 0.057649\n",
      "batch 1657: loss 0.096318\n",
      "batch 1658: loss 0.116979\n",
      "batch 1659: loss 0.122521\n",
      "batch 1660: loss 0.173418\n",
      "batch 1661: loss 0.089420\n",
      "batch 1662: loss 0.031402\n",
      "batch 1663: loss 0.177904\n",
      "batch 1664: loss 0.151177\n",
      "batch 1665: loss 0.097859\n",
      "batch 1666: loss 0.196377\n",
      "batch 1667: loss 0.202109\n",
      "batch 1668: loss 0.120918\n",
      "batch 1669: loss 0.190518\n",
      "batch 1670: loss 0.089955\n",
      "batch 1671: loss 0.114618\n",
      "batch 1672: loss 0.184426\n",
      "batch 1673: loss 0.349652\n",
      "batch 1674: loss 0.208848\n",
      "batch 1675: loss 0.113423\n",
      "batch 1676: loss 0.140515\n",
      "batch 1677: loss 0.053988\n",
      "batch 1678: loss 0.269794\n",
      "batch 1679: loss 0.137415\n",
      "batch 1680: loss 0.079864\n",
      "batch 1681: loss 0.106527\n",
      "batch 1682: loss 0.335286\n",
      "batch 1683: loss 0.198724\n",
      "batch 1684: loss 0.054045\n",
      "batch 1685: loss 0.085524\n",
      "batch 1686: loss 0.372208\n",
      "batch 1687: loss 0.089105\n",
      "batch 1688: loss 0.351122\n",
      "batch 1689: loss 0.226762\n",
      "batch 1690: loss 0.132540\n",
      "batch 1691: loss 0.100116\n",
      "batch 1692: loss 0.176320\n",
      "batch 1693: loss 0.073248\n",
      "batch 1694: loss 0.126290\n",
      "batch 1695: loss 0.061683\n",
      "batch 1696: loss 0.141018\n",
      "batch 1697: loss 0.161492\n",
      "batch 1698: loss 0.263072\n",
      "batch 1699: loss 0.075793\n",
      "batch 1700: loss 0.064122\n",
      "batch 1701: loss 0.106919\n",
      "batch 1702: loss 0.117948\n",
      "batch 1703: loss 0.082005\n",
      "batch 1704: loss 0.052497\n",
      "batch 1705: loss 0.170376\n",
      "batch 1706: loss 0.165271\n",
      "batch 1707: loss 0.235186\n",
      "batch 1708: loss 0.022607\n",
      "batch 1709: loss 0.090555\n",
      "batch 1710: loss 0.076318\n",
      "batch 1711: loss 0.194724\n",
      "batch 1712: loss 0.061737\n",
      "batch 1713: loss 0.128715\n",
      "batch 1714: loss 0.050839\n",
      "batch 1715: loss 0.101330\n",
      "batch 1716: loss 0.142290\n",
      "batch 1717: loss 0.130103\n",
      "batch 1718: loss 0.085619\n",
      "batch 1719: loss 0.136937\n",
      "batch 1720: loss 0.110500\n",
      "batch 1721: loss 0.243342\n",
      "batch 1722: loss 0.058459\n",
      "batch 1723: loss 0.183990\n",
      "batch 1724: loss 0.082465\n",
      "batch 1725: loss 0.147207\n",
      "batch 1726: loss 0.037973\n",
      "batch 1727: loss 0.106882\n",
      "batch 1728: loss 0.095281\n",
      "batch 1729: loss 0.075586\n",
      "batch 1730: loss 0.070177\n",
      "batch 1731: loss 0.186746\n",
      "batch 1732: loss 0.118629\n",
      "batch 1733: loss 0.156749\n",
      "batch 1734: loss 0.165290\n",
      "batch 1735: loss 0.204572\n",
      "batch 1736: loss 0.094429\n",
      "batch 1737: loss 0.074322\n",
      "batch 1738: loss 0.062007\n",
      "batch 1739: loss 0.300841\n",
      "batch 1740: loss 0.102878\n",
      "batch 1741: loss 0.112357\n",
      "batch 1742: loss 0.107317\n",
      "batch 1743: loss 0.072458\n",
      "batch 1744: loss 0.035078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1745: loss 0.200638\n",
      "batch 1746: loss 0.095865\n",
      "batch 1747: loss 0.059877\n",
      "batch 1748: loss 0.080962\n",
      "batch 1749: loss 0.116163\n",
      "batch 1750: loss 0.120429\n",
      "batch 1751: loss 0.080854\n",
      "batch 1752: loss 0.098912\n",
      "batch 1753: loss 0.129841\n",
      "batch 1754: loss 0.193209\n",
      "batch 1755: loss 0.336082\n",
      "batch 1756: loss 0.084969\n",
      "batch 1757: loss 0.208545\n",
      "batch 1758: loss 0.038920\n",
      "batch 1759: loss 0.176589\n",
      "batch 1760: loss 0.042646\n",
      "batch 1761: loss 0.045145\n",
      "batch 1762: loss 0.131183\n",
      "batch 1763: loss 0.078262\n",
      "batch 1764: loss 0.123118\n",
      "batch 1765: loss 0.128311\n",
      "batch 1766: loss 0.214126\n",
      "batch 1767: loss 0.094980\n",
      "batch 1768: loss 0.056668\n",
      "batch 1769: loss 0.088236\n",
      "batch 1770: loss 0.041018\n",
      "batch 1771: loss 0.046241\n",
      "batch 1772: loss 0.056153\n",
      "batch 1773: loss 0.168603\n",
      "batch 1774: loss 0.089199\n",
      "batch 1775: loss 0.059297\n",
      "batch 1776: loss 0.131241\n",
      "batch 1777: loss 0.166587\n",
      "batch 1778: loss 0.176661\n",
      "batch 1779: loss 0.114837\n",
      "batch 1780: loss 0.227022\n",
      "batch 1781: loss 0.088265\n",
      "batch 1782: loss 0.089389\n",
      "batch 1783: loss 0.210891\n",
      "batch 1784: loss 0.148465\n",
      "batch 1785: loss 0.256494\n",
      "batch 1786: loss 0.207423\n",
      "batch 1787: loss 0.072837\n",
      "batch 1788: loss 0.066096\n",
      "batch 1789: loss 0.183445\n",
      "batch 1790: loss 0.064449\n",
      "batch 1791: loss 0.057764\n",
      "batch 1792: loss 0.139353\n",
      "batch 1793: loss 0.161513\n",
      "batch 1794: loss 0.084971\n",
      "batch 1795: loss 0.021118\n",
      "batch 1796: loss 0.031976\n",
      "batch 1797: loss 0.101297\n",
      "batch 1798: loss 0.095686\n",
      "batch 1799: loss 0.197734\n",
      "batch 1800: loss 0.054221\n",
      "batch 1801: loss 0.135304\n",
      "batch 1802: loss 0.092483\n",
      "batch 1803: loss 0.291264\n",
      "batch 1804: loss 0.208155\n",
      "batch 1805: loss 0.157118\n",
      "batch 1806: loss 0.131471\n",
      "batch 1807: loss 0.113823\n",
      "batch 1808: loss 0.192864\n",
      "batch 1809: loss 0.275062\n",
      "batch 1810: loss 0.093682\n",
      "batch 1811: loss 0.053117\n",
      "batch 1812: loss 0.082608\n",
      "batch 1813: loss 0.028564\n",
      "batch 1814: loss 0.082920\n",
      "batch 1815: loss 0.118117\n",
      "batch 1816: loss 0.035151\n",
      "batch 1817: loss 0.048977\n",
      "batch 1818: loss 0.303204\n",
      "batch 1819: loss 0.296800\n",
      "batch 1820: loss 0.140974\n",
      "batch 1821: loss 0.100260\n",
      "batch 1822: loss 0.108534\n",
      "batch 1823: loss 0.171358\n",
      "batch 1824: loss 0.213172\n",
      "batch 1825: loss 0.103597\n",
      "batch 1826: loss 0.163642\n",
      "batch 1827: loss 0.243861\n",
      "batch 1828: loss 0.110699\n",
      "batch 1829: loss 0.101570\n",
      "batch 1830: loss 0.071079\n",
      "batch 1831: loss 0.208354\n",
      "batch 1832: loss 0.094177\n",
      "batch 1833: loss 0.149331\n",
      "batch 1834: loss 0.233993\n",
      "batch 1835: loss 0.073300\n",
      "batch 1836: loss 0.078882\n",
      "batch 1837: loss 0.184894\n",
      "batch 1838: loss 0.039924\n",
      "batch 1839: loss 0.228233\n",
      "batch 1840: loss 0.181036\n",
      "batch 1841: loss 0.327984\n",
      "batch 1842: loss 0.121526\n",
      "batch 1843: loss 0.216521\n",
      "batch 1844: loss 0.079562\n",
      "batch 1845: loss 0.246164\n",
      "batch 1846: loss 0.073090\n",
      "batch 1847: loss 0.104576\n",
      "batch 1848: loss 0.249754\n",
      "batch 1849: loss 0.061922\n",
      "batch 1850: loss 0.111241\n",
      "batch 1851: loss 0.032021\n",
      "batch 1852: loss 0.102977\n",
      "batch 1853: loss 0.135140\n",
      "batch 1854: loss 0.143120\n",
      "batch 1855: loss 0.119173\n",
      "batch 1856: loss 0.037935\n",
      "batch 1857: loss 0.146299\n",
      "batch 1858: loss 0.113527\n",
      "batch 1859: loss 0.062780\n",
      "batch 1860: loss 0.125115\n",
      "batch 1861: loss 0.123150\n",
      "batch 1862: loss 0.158402\n",
      "batch 1863: loss 0.159286\n",
      "batch 1864: loss 0.116929\n",
      "batch 1865: loss 0.104037\n",
      "batch 1866: loss 0.036916\n",
      "batch 1867: loss 0.188419\n",
      "batch 1868: loss 0.075319\n",
      "batch 1869: loss 0.117903\n",
      "batch 1870: loss 0.132672\n",
      "batch 1871: loss 0.034397\n",
      "batch 1872: loss 0.356273\n",
      "batch 1873: loss 0.146976\n",
      "batch 1874: loss 0.091397\n",
      "batch 1875: loss 0.224087\n",
      "batch 1876: loss 0.124921\n",
      "batch 1877: loss 0.132620\n",
      "batch 1878: loss 0.074979\n",
      "batch 1879: loss 0.040038\n",
      "batch 1880: loss 0.049319\n",
      "batch 1881: loss 0.116164\n",
      "batch 1882: loss 0.243563\n",
      "batch 1883: loss 0.103360\n",
      "batch 1884: loss 0.095536\n",
      "batch 1885: loss 0.038098\n",
      "batch 1886: loss 0.053896\n",
      "batch 1887: loss 0.166425\n",
      "batch 1888: loss 0.028564\n",
      "batch 1889: loss 0.130675\n",
      "batch 1890: loss 0.103386\n",
      "batch 1891: loss 0.053024\n",
      "batch 1892: loss 0.187194\n",
      "batch 1893: loss 0.188115\n",
      "batch 1894: loss 0.036435\n",
      "batch 1895: loss 0.075621\n",
      "batch 1896: loss 0.029037\n",
      "batch 1897: loss 0.270739\n",
      "batch 1898: loss 0.047834\n",
      "batch 1899: loss 0.119660\n",
      "batch 1900: loss 0.089414\n",
      "batch 1901: loss 0.043927\n",
      "batch 1902: loss 0.084141\n",
      "batch 1903: loss 0.171131\n",
      "batch 1904: loss 0.037795\n",
      "batch 1905: loss 0.286339\n",
      "batch 1906: loss 0.129540\n",
      "batch 1907: loss 0.106544\n",
      "batch 1908: loss 0.086939\n",
      "batch 1909: loss 0.145172\n",
      "batch 1910: loss 0.087415\n",
      "batch 1911: loss 0.035029\n",
      "batch 1912: loss 0.063689\n",
      "batch 1913: loss 0.084253\n",
      "batch 1914: loss 0.288291\n",
      "batch 1915: loss 0.053001\n",
      "batch 1916: loss 0.314034\n",
      "batch 1917: loss 0.204248\n",
      "batch 1918: loss 0.099745\n",
      "batch 1919: loss 0.134319\n",
      "batch 1920: loss 0.172770\n",
      "batch 1921: loss 0.103680\n",
      "batch 1922: loss 0.077634\n",
      "batch 1923: loss 0.099722\n",
      "batch 1924: loss 0.101388\n",
      "batch 1925: loss 0.194199\n",
      "batch 1926: loss 0.109628\n",
      "batch 1927: loss 0.096095\n",
      "batch 1928: loss 0.114887\n",
      "batch 1929: loss 0.044254\n",
      "batch 1930: loss 0.068577\n",
      "batch 1931: loss 0.015705\n",
      "batch 1932: loss 0.054670\n",
      "batch 1933: loss 0.110505\n",
      "batch 1934: loss 0.154629\n",
      "batch 1935: loss 0.071817\n",
      "batch 1936: loss 0.201129\n",
      "batch 1937: loss 0.210342\n",
      "batch 1938: loss 0.122477\n",
      "batch 1939: loss 0.024459\n",
      "batch 1940: loss 0.197112\n",
      "batch 1941: loss 0.078821\n",
      "batch 1942: loss 0.074245\n",
      "batch 1943: loss 0.059986\n",
      "batch 1944: loss 0.089001\n",
      "batch 1945: loss 0.113625\n",
      "batch 1946: loss 0.073245\n",
      "batch 1947: loss 0.160226\n",
      "batch 1948: loss 0.055984\n",
      "batch 1949: loss 0.272597\n",
      "batch 1950: loss 0.081534\n",
      "batch 1951: loss 0.108184\n",
      "batch 1952: loss 0.149889\n",
      "batch 1953: loss 0.099863\n",
      "batch 1954: loss 0.157545\n",
      "batch 1955: loss 0.054881\n",
      "batch 1956: loss 0.085888\n",
      "batch 1957: loss 0.243234\n",
      "batch 1958: loss 0.218689\n",
      "batch 1959: loss 0.128575\n",
      "batch 1960: loss 0.275331\n",
      "batch 1961: loss 0.080123\n",
      "batch 1962: loss 0.153248\n",
      "batch 1963: loss 0.080316\n",
      "batch 1964: loss 0.036580\n",
      "batch 1965: loss 0.092166\n",
      "batch 1966: loss 0.187246\n",
      "batch 1967: loss 0.126673\n",
      "batch 1968: loss 0.056584\n",
      "batch 1969: loss 0.183888\n",
      "batch 1970: loss 0.095453\n",
      "batch 1971: loss 0.143982\n",
      "batch 1972: loss 0.215612\n",
      "batch 1973: loss 0.120802\n",
      "batch 1974: loss 0.203956\n",
      "batch 1975: loss 0.301642\n",
      "batch 1976: loss 0.050862\n",
      "batch 1977: loss 0.182870\n",
      "batch 1978: loss 0.226715\n",
      "batch 1979: loss 0.061381\n",
      "batch 1980: loss 0.074233\n",
      "batch 1981: loss 0.044495\n",
      "batch 1982: loss 0.061886\n",
      "batch 1983: loss 0.022753\n",
      "batch 1984: loss 0.045217\n",
      "batch 1985: loss 0.151361\n",
      "batch 1986: loss 0.033091\n",
      "batch 1987: loss 0.176705\n",
      "batch 1988: loss 0.091591\n",
      "batch 1989: loss 0.212811\n",
      "batch 1990: loss 0.031469\n",
      "batch 1991: loss 0.106872\n",
      "batch 1992: loss 0.066104\n",
      "batch 1993: loss 0.210604\n",
      "batch 1994: loss 0.119135\n",
      "batch 1995: loss 0.090636\n",
      "batch 1996: loss 0.090099\n",
      "batch 1997: loss 0.158981\n",
      "batch 1998: loss 0.147130\n",
      "batch 1999: loss 0.066757\n",
      "batch 2000: loss 0.045203\n",
      "batch 2001: loss 0.248271\n",
      "batch 2002: loss 0.171262\n",
      "batch 2003: loss 0.072260\n",
      "batch 2004: loss 0.109065\n",
      "batch 2005: loss 0.099361\n",
      "batch 2006: loss 0.144374\n",
      "batch 2007: loss 0.146836\n",
      "batch 2008: loss 0.126670\n",
      "batch 2009: loss 0.065454\n",
      "batch 2010: loss 0.103401\n",
      "batch 2011: loss 0.092023\n",
      "batch 2012: loss 0.295275\n",
      "batch 2013: loss 0.157295\n",
      "batch 2014: loss 0.059556\n",
      "batch 2015: loss 0.052181\n",
      "batch 2016: loss 0.034145\n",
      "batch 2017: loss 0.126102\n",
      "batch 2018: loss 0.171198\n",
      "batch 2019: loss 0.066976\n",
      "batch 2020: loss 0.047397\n",
      "batch 2021: loss 0.079593\n",
      "batch 2022: loss 0.124791\n",
      "batch 2023: loss 0.166458\n",
      "batch 2024: loss 0.200240\n",
      "batch 2025: loss 0.065319\n",
      "batch 2026: loss 0.136189\n",
      "batch 2027: loss 0.176371\n",
      "batch 2028: loss 0.071602\n",
      "batch 2029: loss 0.091695\n",
      "batch 2030: loss 0.149709\n",
      "batch 2031: loss 0.145641\n",
      "batch 2032: loss 0.128572\n",
      "batch 2033: loss 0.056329\n",
      "batch 2034: loss 0.078490\n",
      "batch 2035: loss 0.107047\n",
      "batch 2036: loss 0.083318\n",
      "batch 2037: loss 0.115050\n",
      "batch 2038: loss 0.061271\n",
      "batch 2039: loss 0.206433\n",
      "batch 2040: loss 0.108365\n",
      "batch 2041: loss 0.156368\n",
      "batch 2042: loss 0.109013\n",
      "batch 2043: loss 0.080199\n",
      "batch 2044: loss 0.199644\n",
      "batch 2045: loss 0.191734\n",
      "batch 2046: loss 0.041309\n",
      "batch 2047: loss 0.176167\n",
      "batch 2048: loss 0.123382\n",
      "batch 2049: loss 0.095750\n",
      "batch 2050: loss 0.103633\n",
      "batch 2051: loss 0.087876\n",
      "batch 2052: loss 0.081973\n",
      "batch 2053: loss 0.076117\n",
      "batch 2054: loss 0.052275\n",
      "batch 2055: loss 0.059502\n",
      "batch 2056: loss 0.045578\n",
      "batch 2057: loss 0.169879\n",
      "batch 2058: loss 0.120032\n",
      "batch 2059: loss 0.097849\n",
      "batch 2060: loss 0.037812\n",
      "batch 2061: loss 0.134226\n",
      "batch 2062: loss 0.199293\n",
      "batch 2063: loss 0.144840\n",
      "batch 2064: loss 0.118837\n",
      "batch 2065: loss 0.123152\n",
      "batch 2066: loss 0.186245\n",
      "batch 2067: loss 0.112042\n",
      "batch 2068: loss 0.075950\n",
      "batch 2069: loss 0.092579\n",
      "batch 2070: loss 0.250221\n",
      "batch 2071: loss 0.095876\n",
      "batch 2072: loss 0.075534\n",
      "batch 2073: loss 0.112636\n",
      "batch 2074: loss 0.113114\n",
      "batch 2075: loss 0.234043\n",
      "batch 2076: loss 0.211088\n",
      "batch 2077: loss 0.123046\n",
      "batch 2078: loss 0.215049\n",
      "batch 2079: loss 0.270004\n",
      "batch 2080: loss 0.159343\n",
      "batch 2081: loss 0.092549\n",
      "batch 2082: loss 0.128876\n",
      "batch 2083: loss 0.047916\n",
      "batch 2084: loss 0.121188\n",
      "batch 2085: loss 0.139819\n",
      "batch 2086: loss 0.153626\n",
      "batch 2087: loss 0.053357\n",
      "batch 2088: loss 0.032613\n",
      "batch 2089: loss 0.252330\n",
      "batch 2090: loss 0.137850\n",
      "batch 2091: loss 0.152332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2092: loss 0.078894\n",
      "batch 2093: loss 0.086149\n",
      "batch 2094: loss 0.136060\n",
      "batch 2095: loss 0.066341\n",
      "batch 2096: loss 0.073639\n",
      "batch 2097: loss 0.134368\n",
      "batch 2098: loss 0.028928\n",
      "batch 2099: loss 0.034954\n",
      "batch 2100: loss 0.291795\n",
      "batch 2101: loss 0.031101\n",
      "batch 2102: loss 0.017809\n",
      "batch 2103: loss 0.114474\n",
      "batch 2104: loss 0.151301\n",
      "batch 2105: loss 0.032342\n",
      "batch 2106: loss 0.217680\n",
      "batch 2107: loss 0.126201\n",
      "batch 2108: loss 0.121448\n",
      "batch 2109: loss 0.021626\n",
      "batch 2110: loss 0.163116\n",
      "batch 2111: loss 0.066725\n",
      "batch 2112: loss 0.074153\n",
      "batch 2113: loss 0.108764\n",
      "batch 2114: loss 0.210369\n",
      "batch 2115: loss 0.226893\n",
      "batch 2116: loss 0.114991\n",
      "batch 2117: loss 0.166502\n",
      "batch 2118: loss 0.058992\n",
      "batch 2119: loss 0.153011\n",
      "batch 2120: loss 0.182712\n",
      "batch 2121: loss 0.089573\n",
      "batch 2122: loss 0.053213\n",
      "batch 2123: loss 0.108049\n",
      "batch 2124: loss 0.077180\n",
      "batch 2125: loss 0.087246\n",
      "batch 2126: loss 0.205444\n",
      "batch 2127: loss 0.202767\n",
      "batch 2128: loss 0.149517\n",
      "batch 2129: loss 0.019549\n",
      "batch 2130: loss 0.032759\n",
      "batch 2131: loss 0.019259\n",
      "batch 2132: loss 0.044965\n",
      "batch 2133: loss 0.112776\n",
      "batch 2134: loss 0.149415\n",
      "batch 2135: loss 0.220397\n",
      "batch 2136: loss 0.060864\n",
      "batch 2137: loss 0.189528\n",
      "batch 2138: loss 0.091692\n",
      "batch 2139: loss 0.077969\n",
      "batch 2140: loss 0.051338\n",
      "batch 2141: loss 0.045706\n",
      "batch 2142: loss 0.141196\n",
      "batch 2143: loss 0.043638\n",
      "batch 2144: loss 0.113378\n",
      "batch 2145: loss 0.050732\n",
      "batch 2146: loss 0.081129\n",
      "batch 2147: loss 0.306999\n",
      "batch 2148: loss 0.102705\n",
      "batch 2149: loss 0.169297\n",
      "batch 2150: loss 0.066905\n",
      "batch 2151: loss 0.044078\n",
      "batch 2152: loss 0.025110\n",
      "batch 2153: loss 0.215661\n",
      "batch 2154: loss 0.126067\n",
      "batch 2155: loss 0.158311\n",
      "batch 2156: loss 0.177937\n",
      "batch 2157: loss 0.220697\n",
      "batch 2158: loss 0.082302\n",
      "batch 2159: loss 0.185818\n",
      "batch 2160: loss 0.129941\n",
      "batch 2161: loss 0.083585\n",
      "batch 2162: loss 0.031724\n",
      "batch 2163: loss 0.041266\n",
      "batch 2164: loss 0.172423\n",
      "batch 2165: loss 0.047652\n",
      "batch 2166: loss 0.173807\n",
      "batch 2167: loss 0.151274\n",
      "batch 2168: loss 0.090468\n",
      "batch 2169: loss 0.168711\n",
      "batch 2170: loss 0.069342\n",
      "batch 2171: loss 0.047959\n",
      "batch 2172: loss 0.065581\n",
      "batch 2173: loss 0.111477\n",
      "batch 2174: loss 0.080741\n",
      "batch 2175: loss 0.129347\n",
      "batch 2176: loss 0.053974\n",
      "batch 2177: loss 0.066544\n",
      "batch 2178: loss 0.082850\n",
      "batch 2179: loss 0.051156\n",
      "batch 2180: loss 0.190113\n",
      "batch 2181: loss 0.194008\n",
      "batch 2182: loss 0.056354\n",
      "batch 2183: loss 0.205259\n",
      "batch 2184: loss 0.170406\n",
      "batch 2185: loss 0.090476\n",
      "batch 2186: loss 0.246372\n",
      "batch 2187: loss 0.073147\n",
      "batch 2188: loss 0.085466\n",
      "batch 2189: loss 0.046200\n",
      "batch 2190: loss 0.122022\n",
      "batch 2191: loss 0.302828\n",
      "batch 2192: loss 0.022567\n",
      "batch 2193: loss 0.022034\n",
      "batch 2194: loss 0.076124\n",
      "batch 2195: loss 0.144327\n",
      "batch 2196: loss 0.158479\n",
      "batch 2197: loss 0.067808\n",
      "batch 2198: loss 0.081015\n",
      "batch 2199: loss 0.089167\n",
      "batch 2200: loss 0.110211\n",
      "batch 2201: loss 0.246714\n",
      "batch 2202: loss 0.061188\n",
      "batch 2203: loss 0.052266\n",
      "batch 2204: loss 0.197677\n",
      "batch 2205: loss 0.366375\n",
      "batch 2206: loss 0.051731\n",
      "batch 2207: loss 0.121802\n",
      "batch 2208: loss 0.068308\n",
      "batch 2209: loss 0.093638\n",
      "batch 2210: loss 0.088075\n",
      "batch 2211: loss 0.086310\n",
      "batch 2212: loss 0.063116\n",
      "batch 2213: loss 0.036424\n",
      "batch 2214: loss 0.191675\n",
      "batch 2215: loss 0.060479\n",
      "batch 2216: loss 0.130832\n",
      "batch 2217: loss 0.037777\n",
      "batch 2218: loss 0.170719\n",
      "batch 2219: loss 0.042390\n",
      "batch 2220: loss 0.091361\n",
      "batch 2221: loss 0.056324\n",
      "batch 2222: loss 0.146833\n",
      "batch 2223: loss 0.039477\n",
      "batch 2224: loss 0.102458\n",
      "batch 2225: loss 0.094587\n",
      "batch 2226: loss 0.233846\n",
      "batch 2227: loss 0.099698\n",
      "batch 2228: loss 0.105886\n",
      "batch 2229: loss 0.224453\n",
      "batch 2230: loss 0.077592\n",
      "batch 2231: loss 0.127521\n",
      "batch 2232: loss 0.093296\n",
      "batch 2233: loss 0.333533\n",
      "batch 2234: loss 0.137544\n",
      "batch 2235: loss 0.082625\n",
      "batch 2236: loss 0.035591\n",
      "batch 2237: loss 0.158270\n",
      "batch 2238: loss 0.052242\n",
      "batch 2239: loss 0.179473\n",
      "batch 2240: loss 0.011268\n",
      "batch 2241: loss 0.090831\n",
      "batch 2242: loss 0.065399\n",
      "batch 2243: loss 0.037535\n",
      "batch 2244: loss 0.141691\n",
      "batch 2245: loss 0.043796\n",
      "batch 2246: loss 0.056689\n",
      "batch 2247: loss 0.150138\n",
      "batch 2248: loss 0.100259\n",
      "batch 2249: loss 0.117759\n",
      "batch 2250: loss 0.059512\n",
      "batch 2251: loss 0.142425\n",
      "batch 2252: loss 0.140708\n",
      "batch 2253: loss 0.054586\n",
      "batch 2254: loss 0.058408\n",
      "batch 2255: loss 0.025842\n",
      "batch 2256: loss 0.210955\n",
      "batch 2257: loss 0.123011\n",
      "batch 2258: loss 0.187271\n",
      "batch 2259: loss 0.174202\n",
      "batch 2260: loss 0.041863\n",
      "batch 2261: loss 0.124695\n",
      "batch 2262: loss 0.074296\n",
      "batch 2263: loss 0.078592\n",
      "batch 2264: loss 0.020568\n",
      "batch 2265: loss 0.016672\n",
      "batch 2266: loss 0.186744\n",
      "batch 2267: loss 0.170523\n",
      "batch 2268: loss 0.098164\n",
      "batch 2269: loss 0.058005\n",
      "batch 2270: loss 0.039456\n",
      "batch 2271: loss 0.047767\n",
      "batch 2272: loss 0.257772\n",
      "batch 2273: loss 0.065005\n",
      "batch 2274: loss 0.091147\n",
      "batch 2275: loss 0.155273\n",
      "batch 2276: loss 0.237086\n",
      "batch 2277: loss 0.055832\n",
      "batch 2278: loss 0.021599\n",
      "batch 2279: loss 0.069330\n",
      "batch 2280: loss 0.044813\n",
      "batch 2281: loss 0.152993\n",
      "batch 2282: loss 0.148461\n",
      "batch 2283: loss 0.049711\n",
      "batch 2284: loss 0.061669\n",
      "batch 2285: loss 0.041355\n",
      "batch 2286: loss 0.092472\n",
      "batch 2287: loss 0.040488\n",
      "batch 2288: loss 0.186315\n",
      "batch 2289: loss 0.136107\n",
      "batch 2290: loss 0.114978\n",
      "batch 2291: loss 0.102770\n",
      "batch 2292: loss 0.051748\n",
      "batch 2293: loss 0.107999\n",
      "batch 2294: loss 0.113382\n",
      "batch 2295: loss 0.057565\n",
      "batch 2296: loss 0.055368\n",
      "batch 2297: loss 0.062624\n",
      "batch 2298: loss 0.049118\n",
      "batch 2299: loss 0.052640\n",
      "batch 2300: loss 0.027528\n",
      "batch 2301: loss 0.089090\n",
      "batch 2302: loss 0.332144\n",
      "batch 2303: loss 0.124301\n",
      "batch 2304: loss 0.039449\n",
      "batch 2305: loss 0.180642\n",
      "batch 2306: loss 0.064042\n",
      "batch 2307: loss 0.074079\n",
      "batch 2308: loss 0.104686\n",
      "batch 2309: loss 0.041281\n",
      "batch 2310: loss 0.116884\n",
      "batch 2311: loss 0.077547\n",
      "batch 2312: loss 0.151628\n",
      "batch 2313: loss 0.139184\n",
      "batch 2314: loss 0.074371\n",
      "batch 2315: loss 0.258056\n",
      "batch 2316: loss 0.075329\n",
      "batch 2317: loss 0.105482\n",
      "batch 2318: loss 0.049924\n",
      "batch 2319: loss 0.090940\n",
      "batch 2320: loss 0.053647\n",
      "batch 2321: loss 0.168047\n",
      "batch 2322: loss 0.153647\n",
      "batch 2323: loss 0.134973\n",
      "batch 2324: loss 0.077215\n",
      "batch 2325: loss 0.033229\n",
      "batch 2326: loss 0.111475\n",
      "batch 2327: loss 0.238410\n",
      "batch 2328: loss 0.122180\n",
      "batch 2329: loss 0.091884\n",
      "batch 2330: loss 0.175472\n",
      "batch 2331: loss 0.035263\n",
      "batch 2332: loss 0.081839\n",
      "batch 2333: loss 0.135141\n",
      "batch 2334: loss 0.041928\n",
      "batch 2335: loss 0.147789\n",
      "batch 2336: loss 0.177601\n",
      "batch 2337: loss 0.219316\n",
      "batch 2338: loss 0.156555\n",
      "batch 2339: loss 0.120840\n",
      "batch 2340: loss 0.247625\n",
      "batch 2341: loss 0.097640\n",
      "batch 2342: loss 0.035462\n",
      "batch 2343: loss 0.181469\n",
      "batch 2344: loss 0.024283\n",
      "batch 2345: loss 0.350228\n",
      "batch 2346: loss 0.084885\n",
      "batch 2347: loss 0.037832\n",
      "batch 2348: loss 0.028413\n",
      "batch 2349: loss 0.088916\n",
      "batch 2350: loss 0.104133\n",
      "batch 2351: loss 0.024586\n",
      "batch 2352: loss 0.088459\n",
      "batch 2353: loss 0.142570\n",
      "batch 2354: loss 0.125281\n",
      "batch 2355: loss 0.086338\n",
      "batch 2356: loss 0.047817\n",
      "batch 2357: loss 0.092871\n",
      "batch 2358: loss 0.162808\n",
      "batch 2359: loss 0.141603\n",
      "batch 2360: loss 0.025230\n",
      "batch 2361: loss 0.171865\n",
      "batch 2362: loss 0.043944\n",
      "batch 2363: loss 0.155044\n",
      "batch 2364: loss 0.036854\n",
      "batch 2365: loss 0.207492\n",
      "batch 2366: loss 0.100609\n",
      "batch 2367: loss 0.048701\n",
      "batch 2368: loss 0.034124\n",
      "batch 2369: loss 0.022883\n",
      "batch 2370: loss 0.105386\n",
      "batch 2371: loss 0.053538\n",
      "batch 2372: loss 0.076292\n",
      "batch 2373: loss 0.020592\n",
      "batch 2374: loss 0.281567\n",
      "batch 2375: loss 0.028481\n",
      "batch 2376: loss 0.090701\n",
      "batch 2377: loss 0.063258\n",
      "batch 2378: loss 0.044655\n",
      "batch 2379: loss 0.058967\n",
      "batch 2380: loss 0.088523\n",
      "batch 2381: loss 0.046462\n",
      "batch 2382: loss 0.073318\n",
      "batch 2383: loss 0.132782\n",
      "batch 2384: loss 0.137308\n",
      "batch 2385: loss 0.083693\n",
      "batch 2386: loss 0.101510\n",
      "batch 2387: loss 0.066946\n",
      "batch 2388: loss 0.168518\n",
      "batch 2389: loss 0.070871\n",
      "batch 2390: loss 0.117932\n",
      "batch 2391: loss 0.130658\n",
      "batch 2392: loss 0.038155\n",
      "batch 2393: loss 0.064528\n",
      "batch 2394: loss 0.121996\n",
      "batch 2395: loss 0.056670\n",
      "batch 2396: loss 0.061953\n",
      "batch 2397: loss 0.136776\n",
      "batch 2398: loss 0.210967\n",
      "batch 2399: loss 0.170739\n",
      "batch 2400: loss 0.053031\n",
      "batch 2401: loss 0.082221\n",
      "batch 2402: loss 0.109588\n",
      "batch 2403: loss 0.053254\n",
      "batch 2404: loss 0.111051\n",
      "batch 2405: loss 0.061881\n",
      "batch 2406: loss 0.093918\n",
      "batch 2407: loss 0.025452\n",
      "batch 2408: loss 0.100440\n",
      "batch 2409: loss 0.141973\n",
      "batch 2410: loss 0.069130\n",
      "batch 2411: loss 0.283675\n",
      "batch 2412: loss 0.123361\n",
      "batch 2413: loss 0.291639\n",
      "batch 2414: loss 0.085883\n",
      "batch 2415: loss 0.086159\n",
      "batch 2416: loss 0.063306\n",
      "batch 2417: loss 0.025746\n",
      "batch 2418: loss 0.077603\n",
      "batch 2419: loss 0.158474\n",
      "batch 2420: loss 0.062932\n",
      "batch 2421: loss 0.162149\n",
      "batch 2422: loss 0.029757\n",
      "batch 2423: loss 0.058897\n",
      "batch 2424: loss 0.092760\n",
      "batch 2425: loss 0.188567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2426: loss 0.092342\n",
      "batch 2427: loss 0.042075\n",
      "batch 2428: loss 0.118304\n",
      "batch 2429: loss 0.031642\n",
      "batch 2430: loss 0.081528\n",
      "batch 2431: loss 0.109851\n",
      "batch 2432: loss 0.150151\n",
      "batch 2433: loss 0.069280\n",
      "batch 2434: loss 0.346908\n",
      "batch 2435: loss 0.083859\n",
      "batch 2436: loss 0.146617\n",
      "batch 2437: loss 0.057186\n",
      "batch 2438: loss 0.122216\n",
      "batch 2439: loss 0.037322\n",
      "batch 2440: loss 0.128428\n",
      "batch 2441: loss 0.059479\n",
      "batch 2442: loss 0.147311\n",
      "batch 2443: loss 0.144581\n",
      "batch 2444: loss 0.110834\n",
      "batch 2445: loss 0.163967\n",
      "batch 2446: loss 0.124461\n",
      "batch 2447: loss 0.111820\n",
      "batch 2448: loss 0.187663\n",
      "batch 2449: loss 0.143985\n",
      "batch 2450: loss 0.039973\n",
      "batch 2451: loss 0.070139\n",
      "batch 2452: loss 0.069986\n",
      "batch 2453: loss 0.139983\n",
      "batch 2454: loss 0.062261\n",
      "batch 2455: loss 0.013478\n",
      "batch 2456: loss 0.063562\n",
      "batch 2457: loss 0.079477\n",
      "batch 2458: loss 0.137453\n",
      "batch 2459: loss 0.233212\n",
      "batch 2460: loss 0.118460\n",
      "batch 2461: loss 0.225574\n",
      "batch 2462: loss 0.105603\n",
      "batch 2463: loss 0.130811\n",
      "batch 2464: loss 0.192559\n",
      "batch 2465: loss 0.149434\n",
      "batch 2466: loss 0.156678\n",
      "batch 2467: loss 0.080892\n",
      "batch 2468: loss 0.132640\n",
      "batch 2469: loss 0.102739\n",
      "batch 2470: loss 0.062384\n",
      "batch 2471: loss 0.117413\n",
      "batch 2472: loss 0.109702\n",
      "batch 2473: loss 0.236231\n",
      "batch 2474: loss 0.193036\n",
      "batch 2475: loss 0.125677\n",
      "batch 2476: loss 0.108954\n",
      "batch 2477: loss 0.148627\n",
      "batch 2478: loss 0.148955\n",
      "batch 2479: loss 0.162648\n",
      "batch 2480: loss 0.102980\n",
      "batch 2481: loss 0.328614\n",
      "batch 2482: loss 0.079264\n",
      "batch 2483: loss 0.227019\n",
      "batch 2484: loss 0.073017\n",
      "batch 2485: loss 0.264505\n",
      "batch 2486: loss 0.091248\n",
      "batch 2487: loss 0.069448\n",
      "batch 2488: loss 0.131529\n",
      "batch 2489: loss 0.142944\n",
      "batch 2490: loss 0.053510\n",
      "batch 2491: loss 0.095151\n",
      "batch 2492: loss 0.022949\n",
      "batch 2493: loss 0.087667\n",
      "batch 2494: loss 0.035248\n",
      "batch 2495: loss 0.244316\n",
      "batch 2496: loss 0.176157\n",
      "batch 2497: loss 0.078956\n",
      "batch 2498: loss 0.159168\n",
      "batch 2499: loss 0.093820\n",
      "batch 2500: loss 0.152270\n",
      "batch 2501: loss 0.069678\n",
      "batch 2502: loss 0.037304\n",
      "batch 2503: loss 0.090989\n",
      "batch 2504: loss 0.072096\n",
      "batch 2505: loss 0.176212\n",
      "batch 2506: loss 0.165558\n",
      "batch 2507: loss 0.052717\n",
      "batch 2508: loss 0.079504\n",
      "batch 2509: loss 0.101935\n",
      "batch 2510: loss 0.050465\n",
      "batch 2511: loss 0.057859\n",
      "batch 2512: loss 0.051092\n",
      "batch 2513: loss 0.154090\n",
      "batch 2514: loss 0.438353\n",
      "batch 2515: loss 0.185261\n",
      "batch 2516: loss 0.092555\n",
      "batch 2517: loss 0.020930\n",
      "batch 2518: loss 0.121107\n",
      "batch 2519: loss 0.107530\n",
      "batch 2520: loss 0.195706\n",
      "batch 2521: loss 0.131245\n",
      "batch 2522: loss 0.052542\n",
      "batch 2523: loss 0.046524\n",
      "batch 2524: loss 0.037927\n",
      "batch 2525: loss 0.180671\n",
      "batch 2526: loss 0.030296\n",
      "batch 2527: loss 0.190670\n",
      "batch 2528: loss 0.030459\n",
      "batch 2529: loss 0.070375\n",
      "batch 2530: loss 0.071436\n",
      "batch 2531: loss 0.031504\n",
      "batch 2532: loss 0.353264\n",
      "batch 2533: loss 0.025813\n",
      "batch 2534: loss 0.143874\n",
      "batch 2535: loss 0.066426\n",
      "batch 2536: loss 0.203435\n",
      "batch 2537: loss 0.145267\n",
      "batch 2538: loss 0.052243\n",
      "batch 2539: loss 0.075164\n",
      "batch 2540: loss 0.047328\n",
      "batch 2541: loss 0.049394\n",
      "batch 2542: loss 0.131170\n",
      "batch 2543: loss 0.161758\n",
      "batch 2544: loss 0.129939\n",
      "batch 2545: loss 0.194696\n",
      "batch 2546: loss 0.066622\n",
      "batch 2547: loss 0.065587\n",
      "batch 2548: loss 0.157557\n",
      "batch 2549: loss 0.282002\n",
      "batch 2550: loss 0.283841\n",
      "batch 2551: loss 0.131739\n",
      "batch 2552: loss 0.131270\n",
      "batch 2553: loss 0.026573\n",
      "batch 2554: loss 0.155620\n",
      "batch 2555: loss 0.080893\n",
      "batch 2556: loss 0.092741\n",
      "batch 2557: loss 0.050542\n",
      "batch 2558: loss 0.078154\n",
      "batch 2559: loss 0.223309\n",
      "batch 2560: loss 0.050213\n",
      "batch 2561: loss 0.115312\n",
      "batch 2562: loss 0.049980\n",
      "batch 2563: loss 0.114293\n",
      "batch 2564: loss 0.088929\n",
      "batch 2565: loss 0.044621\n",
      "batch 2566: loss 0.081013\n",
      "batch 2567: loss 0.106273\n",
      "batch 2568: loss 0.056829\n",
      "batch 2569: loss 0.122148\n",
      "batch 2570: loss 0.075722\n",
      "batch 2571: loss 0.078619\n",
      "batch 2572: loss 0.038047\n",
      "batch 2573: loss 0.167505\n",
      "batch 2574: loss 0.126196\n",
      "batch 2575: loss 0.101363\n",
      "batch 2576: loss 0.144348\n",
      "batch 2577: loss 0.080943\n",
      "batch 2578: loss 0.155492\n",
      "batch 2579: loss 0.062266\n",
      "batch 2580: loss 0.092550\n",
      "batch 2581: loss 0.071075\n",
      "batch 2582: loss 0.316462\n",
      "batch 2583: loss 0.025034\n",
      "batch 2584: loss 0.052295\n",
      "batch 2585: loss 0.097233\n",
      "batch 2586: loss 0.138491\n",
      "batch 2587: loss 0.240972\n",
      "batch 2588: loss 0.095207\n",
      "batch 2589: loss 0.091158\n",
      "batch 2590: loss 0.062275\n",
      "batch 2591: loss 0.120597\n",
      "batch 2592: loss 0.023519\n",
      "batch 2593: loss 0.033616\n",
      "batch 2594: loss 0.158707\n",
      "batch 2595: loss 0.021353\n",
      "batch 2596: loss 0.264596\n",
      "batch 2597: loss 0.084034\n",
      "batch 2598: loss 0.095013\n",
      "batch 2599: loss 0.064839\n",
      "batch 2600: loss 0.152073\n",
      "batch 2601: loss 0.047213\n",
      "batch 2602: loss 0.063262\n",
      "batch 2603: loss 0.138658\n",
      "batch 2604: loss 0.023297\n",
      "batch 2605: loss 0.025262\n",
      "batch 2606: loss 0.104967\n",
      "batch 2607: loss 0.131647\n",
      "batch 2608: loss 0.167112\n",
      "batch 2609: loss 0.093691\n",
      "batch 2610: loss 0.306853\n",
      "batch 2611: loss 0.111302\n",
      "batch 2612: loss 0.097013\n",
      "batch 2613: loss 0.043435\n",
      "batch 2614: loss 0.030980\n",
      "batch 2615: loss 0.064392\n",
      "batch 2616: loss 0.143841\n",
      "batch 2617: loss 0.055632\n",
      "batch 2618: loss 0.057409\n",
      "batch 2619: loss 0.014682\n",
      "batch 2620: loss 0.084572\n",
      "batch 2621: loss 0.059182\n",
      "batch 2622: loss 0.035506\n",
      "batch 2623: loss 0.040361\n",
      "batch 2624: loss 0.071205\n",
      "batch 2625: loss 0.179912\n",
      "batch 2626: loss 0.027938\n",
      "batch 2627: loss 0.048298\n",
      "batch 2628: loss 0.217797\n",
      "batch 2629: loss 0.143076\n",
      "batch 2630: loss 0.068935\n",
      "batch 2631: loss 0.140530\n",
      "batch 2632: loss 0.123334\n",
      "batch 2633: loss 0.177872\n",
      "batch 2634: loss 0.063944\n",
      "batch 2635: loss 0.120386\n",
      "batch 2636: loss 0.084781\n",
      "batch 2637: loss 0.134595\n",
      "batch 2638: loss 0.015503\n",
      "batch 2639: loss 0.167238\n",
      "batch 2640: loss 0.174434\n",
      "batch 2641: loss 0.161544\n",
      "batch 2642: loss 0.058310\n",
      "batch 2643: loss 0.162768\n",
      "batch 2644: loss 0.055484\n",
      "batch 2645: loss 0.171256\n",
      "batch 2646: loss 0.050150\n",
      "batch 2647: loss 0.061632\n",
      "batch 2648: loss 0.052717\n",
      "batch 2649: loss 0.070130\n",
      "batch 2650: loss 0.054894\n",
      "batch 2651: loss 0.072277\n",
      "batch 2652: loss 0.231273\n",
      "batch 2653: loss 0.037370\n",
      "batch 2654: loss 0.130098\n",
      "batch 2655: loss 0.058150\n",
      "batch 2656: loss 0.180016\n",
      "batch 2657: loss 0.034784\n",
      "batch 2658: loss 0.170457\n",
      "batch 2659: loss 0.162453\n",
      "batch 2660: loss 0.104393\n",
      "batch 2661: loss 0.037274\n",
      "batch 2662: loss 0.080156\n",
      "batch 2663: loss 0.090070\n",
      "batch 2664: loss 0.186577\n",
      "batch 2665: loss 0.045487\n",
      "batch 2666: loss 0.113006\n",
      "batch 2667: loss 0.070300\n",
      "batch 2668: loss 0.060312\n",
      "batch 2669: loss 0.109133\n",
      "batch 2670: loss 0.055746\n",
      "batch 2671: loss 0.049589\n",
      "batch 2672: loss 0.093069\n",
      "batch 2673: loss 0.046465\n",
      "batch 2674: loss 0.065329\n",
      "batch 2675: loss 0.078937\n",
      "batch 2676: loss 0.127357\n",
      "batch 2677: loss 0.154940\n",
      "batch 2678: loss 0.172305\n",
      "batch 2679: loss 0.079655\n",
      "batch 2680: loss 0.060638\n",
      "batch 2681: loss 0.154919\n",
      "batch 2682: loss 0.107205\n",
      "batch 2683: loss 0.050265\n",
      "batch 2684: loss 0.093312\n",
      "batch 2685: loss 0.038540\n",
      "batch 2686: loss 0.082059\n",
      "batch 2687: loss 0.079606\n",
      "batch 2688: loss 0.014815\n",
      "batch 2689: loss 0.122359\n",
      "batch 2690: loss 0.035611\n",
      "batch 2691: loss 0.118983\n",
      "batch 2692: loss 0.056224\n",
      "batch 2693: loss 0.054809\n",
      "batch 2694: loss 0.185248\n",
      "batch 2695: loss 0.099732\n",
      "batch 2696: loss 0.088460\n",
      "batch 2697: loss 0.184206\n",
      "batch 2698: loss 0.038037\n",
      "batch 2699: loss 0.057004\n",
      "batch 2700: loss 0.010302\n",
      "batch 2701: loss 0.079176\n",
      "batch 2702: loss 0.060214\n",
      "batch 2703: loss 0.049914\n",
      "batch 2704: loss 0.126135\n",
      "batch 2705: loss 0.160680\n",
      "batch 2706: loss 0.086575\n",
      "batch 2707: loss 0.043444\n",
      "batch 2708: loss 0.102984\n",
      "batch 2709: loss 0.045769\n",
      "batch 2710: loss 0.032649\n",
      "batch 2711: loss 0.039785\n",
      "batch 2712: loss 0.199154\n",
      "batch 2713: loss 0.042267\n",
      "batch 2714: loss 0.165152\n",
      "batch 2715: loss 0.045251\n",
      "batch 2716: loss 0.068169\n",
      "batch 2717: loss 0.084947\n",
      "batch 2718: loss 0.108567\n",
      "batch 2719: loss 0.041995\n",
      "batch 2720: loss 0.041313\n",
      "batch 2721: loss 0.041867\n",
      "batch 2722: loss 0.148061\n",
      "batch 2723: loss 0.063285\n",
      "batch 2724: loss 0.062872\n",
      "batch 2725: loss 0.096236\n",
      "batch 2726: loss 0.116604\n",
      "batch 2727: loss 0.065907\n",
      "batch 2728: loss 0.239015\n",
      "batch 2729: loss 0.082265\n",
      "batch 2730: loss 0.054086\n",
      "batch 2731: loss 0.072856\n",
      "batch 2732: loss 0.158996\n",
      "batch 2733: loss 0.038413\n",
      "batch 2734: loss 0.148596\n",
      "batch 2735: loss 0.037836\n",
      "batch 2736: loss 0.077168\n",
      "batch 2737: loss 0.055194\n",
      "batch 2738: loss 0.122601\n",
      "batch 2739: loss 0.036006\n",
      "batch 2740: loss 0.110687\n",
      "batch 2741: loss 0.091647\n",
      "batch 2742: loss 0.094299\n",
      "batch 2743: loss 0.196087\n",
      "batch 2744: loss 0.070375\n",
      "batch 2745: loss 0.037241\n",
      "batch 2746: loss 0.100036\n",
      "batch 2747: loss 0.037265\n",
      "batch 2748: loss 0.146428\n",
      "batch 2749: loss 0.150357\n",
      "batch 2750: loss 0.050026\n",
      "batch 2751: loss 0.239839\n",
      "batch 2752: loss 0.089667\n",
      "batch 2753: loss 0.045679\n",
      "batch 2754: loss 0.048491\n",
      "batch 2755: loss 0.057915\n",
      "batch 2756: loss 0.140220\n",
      "batch 2757: loss 0.081542\n",
      "batch 2758: loss 0.100792\n",
      "batch 2759: loss 0.042159\n",
      "batch 2760: loss 0.026426\n",
      "batch 2761: loss 0.183066\n",
      "batch 2762: loss 0.044814\n",
      "batch 2763: loss 0.035564\n",
      "batch 2764: loss 0.038993\n",
      "batch 2765: loss 0.091914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2766: loss 0.012767\n",
      "batch 2767: loss 0.038075\n",
      "batch 2768: loss 0.063535\n",
      "batch 2769: loss 0.031152\n",
      "batch 2770: loss 0.159058\n",
      "batch 2771: loss 0.056965\n",
      "batch 2772: loss 0.155945\n",
      "batch 2773: loss 0.028515\n",
      "batch 2774: loss 0.086616\n",
      "batch 2775: loss 0.033096\n",
      "batch 2776: loss 0.100445\n",
      "batch 2777: loss 0.228410\n",
      "batch 2778: loss 0.134527\n",
      "batch 2779: loss 0.026589\n",
      "batch 2780: loss 0.108853\n",
      "batch 2781: loss 0.164948\n",
      "batch 2782: loss 0.197621\n",
      "batch 2783: loss 0.211598\n",
      "batch 2784: loss 0.078119\n",
      "batch 2785: loss 0.054689\n",
      "batch 2786: loss 0.138103\n",
      "batch 2787: loss 0.018633\n",
      "batch 2788: loss 0.082666\n",
      "batch 2789: loss 0.258701\n",
      "batch 2790: loss 0.058435\n",
      "batch 2791: loss 0.090804\n",
      "batch 2792: loss 0.173138\n",
      "batch 2793: loss 0.123597\n",
      "batch 2794: loss 0.098430\n",
      "batch 2795: loss 0.124684\n",
      "batch 2796: loss 0.089875\n",
      "batch 2797: loss 0.061262\n",
      "batch 2798: loss 0.244196\n",
      "batch 2799: loss 0.082091\n",
      "batch 2800: loss 0.130923\n",
      "batch 2801: loss 0.141149\n",
      "batch 2802: loss 0.208882\n",
      "batch 2803: loss 0.123651\n",
      "batch 2804: loss 0.079682\n",
      "batch 2805: loss 0.269964\n",
      "batch 2806: loss 0.078229\n",
      "batch 2807: loss 0.193773\n",
      "batch 2808: loss 0.016056\n",
      "batch 2809: loss 0.058771\n",
      "batch 2810: loss 0.021068\n",
      "batch 2811: loss 0.103434\n",
      "batch 2812: loss 0.022011\n",
      "batch 2813: loss 0.122863\n",
      "batch 2814: loss 0.198184\n",
      "batch 2815: loss 0.023267\n",
      "batch 2816: loss 0.032278\n",
      "batch 2817: loss 0.062824\n",
      "batch 2818: loss 0.129245\n",
      "batch 2819: loss 0.048147\n",
      "batch 2820: loss 0.025781\n",
      "batch 2821: loss 0.058444\n",
      "batch 2822: loss 0.029467\n",
      "batch 2823: loss 0.087507\n",
      "batch 2824: loss 0.086909\n",
      "batch 2825: loss 0.046219\n",
      "batch 2826: loss 0.061116\n",
      "batch 2827: loss 0.146303\n",
      "batch 2828: loss 0.122561\n",
      "batch 2829: loss 0.069014\n",
      "batch 2830: loss 0.090245\n",
      "batch 2831: loss 0.112151\n",
      "batch 2832: loss 0.078215\n",
      "batch 2833: loss 0.247985\n",
      "batch 2834: loss 0.018008\n",
      "batch 2835: loss 0.073912\n",
      "batch 2836: loss 0.273597\n",
      "batch 2837: loss 0.075679\n",
      "batch 2838: loss 0.025001\n",
      "batch 2839: loss 0.144844\n",
      "batch 2840: loss 0.045326\n",
      "batch 2841: loss 0.133295\n",
      "batch 2842: loss 0.062162\n",
      "batch 2843: loss 0.024730\n",
      "batch 2844: loss 0.236771\n",
      "batch 2845: loss 0.067387\n",
      "batch 2846: loss 0.124880\n",
      "batch 2847: loss 0.045625\n",
      "batch 2848: loss 0.043363\n",
      "batch 2849: loss 0.163030\n",
      "batch 2850: loss 0.157972\n",
      "batch 2851: loss 0.172208\n",
      "batch 2852: loss 0.046341\n",
      "batch 2853: loss 0.019200\n",
      "batch 2854: loss 0.058544\n",
      "batch 2855: loss 0.039171\n",
      "batch 2856: loss 0.042174\n",
      "batch 2857: loss 0.114075\n",
      "batch 2858: loss 0.058853\n",
      "batch 2859: loss 0.025500\n",
      "batch 2860: loss 0.044521\n",
      "batch 2861: loss 0.051008\n",
      "batch 2862: loss 0.191447\n",
      "batch 2863: loss 0.182065\n",
      "batch 2864: loss 0.213638\n",
      "batch 2865: loss 0.110680\n",
      "batch 2866: loss 0.086876\n",
      "batch 2867: loss 0.121535\n",
      "batch 2868: loss 0.106316\n",
      "batch 2869: loss 0.120138\n",
      "batch 2870: loss 0.134046\n",
      "batch 2871: loss 0.080222\n",
      "batch 2872: loss 0.121159\n",
      "batch 2873: loss 0.082849\n",
      "batch 2874: loss 0.058735\n",
      "batch 2875: loss 0.082430\n",
      "batch 2876: loss 0.040946\n",
      "batch 2877: loss 0.091570\n",
      "batch 2878: loss 0.027152\n",
      "batch 2879: loss 0.142154\n",
      "batch 2880: loss 0.055100\n",
      "batch 2881: loss 0.291182\n",
      "batch 2882: loss 0.035477\n",
      "batch 2883: loss 0.101210\n",
      "batch 2884: loss 0.049063\n",
      "batch 2885: loss 0.117294\n",
      "batch 2886: loss 0.096756\n",
      "batch 2887: loss 0.047471\n",
      "batch 2888: loss 0.058737\n",
      "batch 2889: loss 0.050155\n",
      "batch 2890: loss 0.084145\n",
      "batch 2891: loss 0.068220\n",
      "batch 2892: loss 0.031294\n",
      "batch 2893: loss 0.039640\n",
      "batch 2894: loss 0.088841\n",
      "batch 2895: loss 0.120032\n",
      "batch 2896: loss 0.075451\n",
      "batch 2897: loss 0.059589\n",
      "batch 2898: loss 0.142726\n",
      "batch 2899: loss 0.162144\n",
      "batch 2900: loss 0.115219\n",
      "batch 2901: loss 0.120836\n",
      "batch 2902: loss 0.027322\n",
      "batch 2903: loss 0.081683\n",
      "batch 2904: loss 0.133553\n",
      "batch 2905: loss 0.023024\n",
      "batch 2906: loss 0.024751\n",
      "batch 2907: loss 0.312936\n",
      "batch 2908: loss 0.133549\n",
      "batch 2909: loss 0.080937\n",
      "batch 2910: loss 0.047665\n",
      "batch 2911: loss 0.152349\n",
      "batch 2912: loss 0.050751\n",
      "batch 2913: loss 0.089340\n",
      "batch 2914: loss 0.028912\n",
      "batch 2915: loss 0.124424\n",
      "batch 2916: loss 0.028064\n",
      "batch 2917: loss 0.081582\n",
      "batch 2918: loss 0.104105\n",
      "batch 2919: loss 0.033839\n",
      "batch 2920: loss 0.051960\n",
      "batch 2921: loss 0.098018\n",
      "batch 2922: loss 0.247102\n",
      "batch 2923: loss 0.055784\n",
      "batch 2924: loss 0.011795\n",
      "batch 2925: loss 0.066142\n",
      "batch 2926: loss 0.224348\n",
      "batch 2927: loss 0.020848\n",
      "batch 2928: loss 0.142889\n",
      "batch 2929: loss 0.100418\n",
      "batch 2930: loss 0.066610\n",
      "batch 2931: loss 0.036183\n",
      "batch 2932: loss 0.011745\n",
      "batch 2933: loss 0.024214\n",
      "batch 2934: loss 0.031222\n",
      "batch 2935: loss 0.088863\n",
      "batch 2936: loss 0.146018\n",
      "batch 2937: loss 0.092244\n",
      "batch 2938: loss 0.031609\n",
      "batch 2939: loss 0.135422\n",
      "batch 2940: loss 0.085460\n",
      "batch 2941: loss 0.017916\n",
      "batch 2942: loss 0.018712\n",
      "batch 2943: loss 0.046926\n",
      "batch 2944: loss 0.198532\n",
      "batch 2945: loss 0.067396\n",
      "batch 2946: loss 0.015481\n",
      "batch 2947: loss 0.171089\n",
      "batch 2948: loss 0.132853\n",
      "batch 2949: loss 0.072952\n",
      "batch 2950: loss 0.011078\n",
      "batch 2951: loss 0.074115\n",
      "batch 2952: loss 0.135575\n",
      "batch 2953: loss 0.262281\n",
      "batch 2954: loss 0.127306\n",
      "batch 2955: loss 0.077230\n",
      "batch 2956: loss 0.084701\n",
      "batch 2957: loss 0.083963\n",
      "batch 2958: loss 0.139103\n",
      "batch 2959: loss 0.048275\n",
      "batch 2960: loss 0.043003\n",
      "batch 2961: loss 0.044778\n",
      "batch 2962: loss 0.171500\n",
      "batch 2963: loss 0.113403\n",
      "batch 2964: loss 0.066987\n",
      "batch 2965: loss 0.205433\n",
      "batch 2966: loss 0.048509\n",
      "batch 2967: loss 0.057169\n",
      "batch 2968: loss 0.255637\n",
      "batch 2969: loss 0.148784\n",
      "batch 2970: loss 0.086192\n",
      "batch 2971: loss 0.062615\n",
      "batch 2972: loss 0.112452\n",
      "batch 2973: loss 0.124279\n",
      "batch 2974: loss 0.028474\n",
      "batch 2975: loss 0.164119\n",
      "batch 2976: loss 0.047444\n",
      "batch 2977: loss 0.044554\n",
      "batch 2978: loss 0.090202\n",
      "batch 2979: loss 0.024118\n",
      "batch 2980: loss 0.027492\n",
      "batch 2981: loss 0.052855\n",
      "batch 2982: loss 0.073628\n",
      "batch 2983: loss 0.124463\n",
      "batch 2984: loss 0.016473\n",
      "batch 2985: loss 0.125656\n",
      "batch 2986: loss 0.042007\n",
      "batch 2987: loss 0.065576\n",
      "batch 2988: loss 0.036122\n",
      "batch 2989: loss 0.072296\n",
      "batch 2990: loss 0.017289\n",
      "batch 2991: loss 0.104339\n",
      "batch 2992: loss 0.139451\n",
      "batch 2993: loss 0.027513\n",
      "batch 2994: loss 0.111466\n",
      "batch 2995: loss 0.109483\n",
      "batch 2996: loss 0.041958\n",
      "batch 2997: loss 0.210755\n",
      "batch 2998: loss 0.049603\n",
      "batch 2999: loss 0.135184\n",
      "batch 3000: loss 0.059838\n",
      "batch 3001: loss 0.023001\n",
      "batch 3002: loss 0.083183\n",
      "batch 3003: loss 0.244967\n",
      "batch 3004: loss 0.066723\n",
      "batch 3005: loss 0.059600\n",
      "batch 3006: loss 0.056280\n",
      "batch 3007: loss 0.084623\n",
      "batch 3008: loss 0.032587\n",
      "batch 3009: loss 0.054858\n",
      "batch 3010: loss 0.124836\n",
      "batch 3011: loss 0.066558\n",
      "batch 3012: loss 0.024211\n",
      "batch 3013: loss 0.052222\n",
      "batch 3014: loss 0.067196\n",
      "batch 3015: loss 0.159306\n",
      "batch 3016: loss 0.042454\n",
      "batch 3017: loss 0.095682\n",
      "batch 3018: loss 0.036684\n",
      "batch 3019: loss 0.038529\n",
      "batch 3020: loss 0.078591\n",
      "batch 3021: loss 0.033041\n",
      "batch 3022: loss 0.014498\n",
      "batch 3023: loss 0.190158\n",
      "batch 3024: loss 0.057247\n",
      "batch 3025: loss 0.132360\n",
      "batch 3026: loss 0.103642\n",
      "batch 3027: loss 0.109483\n",
      "batch 3028: loss 0.029884\n",
      "batch 3029: loss 0.035407\n",
      "batch 3030: loss 0.030394\n",
      "batch 3031: loss 0.016234\n",
      "batch 3032: loss 0.062224\n",
      "batch 3033: loss 0.053598\n",
      "batch 3034: loss 0.024598\n",
      "batch 3035: loss 0.019258\n",
      "batch 3036: loss 0.146197\n",
      "batch 3037: loss 0.047048\n",
      "batch 3038: loss 0.060501\n",
      "batch 3039: loss 0.071182\n",
      "batch 3040: loss 0.045960\n",
      "batch 3041: loss 0.028456\n",
      "batch 3042: loss 0.104490\n",
      "batch 3043: loss 0.075190\n",
      "batch 3044: loss 0.015280\n",
      "batch 3045: loss 0.042963\n",
      "batch 3046: loss 0.029455\n",
      "batch 3047: loss 0.161729\n",
      "batch 3048: loss 0.050525\n",
      "batch 3049: loss 0.025026\n",
      "batch 3050: loss 0.097613\n",
      "batch 3051: loss 0.069327\n",
      "batch 3052: loss 0.064503\n",
      "batch 3053: loss 0.083502\n",
      "batch 3054: loss 0.071419\n",
      "batch 3055: loss 0.191509\n",
      "batch 3056: loss 0.067833\n",
      "batch 3057: loss 0.150014\n",
      "batch 3058: loss 0.035934\n",
      "batch 3059: loss 0.036295\n",
      "batch 3060: loss 0.079420\n",
      "batch 3061: loss 0.103774\n",
      "batch 3062: loss 0.036497\n",
      "batch 3063: loss 0.056135\n",
      "batch 3064: loss 0.036849\n",
      "batch 3065: loss 0.039060\n",
      "batch 3066: loss 0.192540\n",
      "batch 3067: loss 0.103566\n",
      "batch 3068: loss 0.030618\n",
      "batch 3069: loss 0.036091\n",
      "batch 3070: loss 0.045583\n",
      "batch 3071: loss 0.104116\n",
      "batch 3072: loss 0.068721\n",
      "batch 3073: loss 0.071670\n",
      "batch 3074: loss 0.248947\n",
      "batch 3075: loss 0.127921\n",
      "batch 3076: loss 0.269168\n",
      "batch 3077: loss 0.032303\n",
      "batch 3078: loss 0.029266\n",
      "batch 3079: loss 0.050970\n",
      "batch 3080: loss 0.312903\n",
      "batch 3081: loss 0.156918\n",
      "batch 3082: loss 0.100318\n",
      "batch 3083: loss 0.104454\n",
      "batch 3084: loss 0.123606\n",
      "batch 3085: loss 0.043772\n",
      "batch 3086: loss 0.048832\n",
      "batch 3087: loss 0.031179\n",
      "batch 3088: loss 0.061266\n",
      "batch 3089: loss 0.044076\n",
      "batch 3090: loss 0.207600\n",
      "batch 3091: loss 0.075611\n",
      "batch 3092: loss 0.077636\n",
      "batch 3093: loss 0.088647\n",
      "batch 3094: loss 0.057931\n",
      "batch 3095: loss 0.154989\n",
      "batch 3096: loss 0.080741\n",
      "batch 3097: loss 0.046971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3098: loss 0.068820\n",
      "batch 3099: loss 0.039196\n",
      "batch 3100: loss 0.030294\n",
      "batch 3101: loss 0.074461\n",
      "batch 3102: loss 0.064643\n",
      "batch 3103: loss 0.116151\n",
      "batch 3104: loss 0.035986\n",
      "batch 3105: loss 0.074147\n",
      "batch 3106: loss 0.250962\n",
      "batch 3107: loss 0.062614\n",
      "batch 3108: loss 0.117857\n",
      "batch 3109: loss 0.130425\n",
      "batch 3110: loss 0.082339\n",
      "batch 3111: loss 0.063900\n",
      "batch 3112: loss 0.029177\n",
      "batch 3113: loss 0.057164\n",
      "batch 3114: loss 0.063341\n",
      "batch 3115: loss 0.056770\n",
      "batch 3116: loss 0.075559\n",
      "batch 3117: loss 0.019764\n",
      "batch 3118: loss 0.048118\n",
      "batch 3119: loss 0.150343\n",
      "batch 3120: loss 0.005146\n",
      "batch 3121: loss 0.125943\n",
      "batch 3122: loss 0.074121\n",
      "batch 3123: loss 0.077855\n",
      "batch 3124: loss 0.089128\n",
      "batch 3125: loss 0.175243\n",
      "batch 3126: loss 0.040726\n",
      "batch 3127: loss 0.059753\n",
      "batch 3128: loss 0.045760\n",
      "batch 3129: loss 0.103126\n",
      "batch 3130: loss 0.043852\n",
      "batch 3131: loss 0.109908\n",
      "batch 3132: loss 0.270958\n",
      "batch 3133: loss 0.142520\n",
      "batch 3134: loss 0.095833\n",
      "batch 3135: loss 0.054855\n",
      "batch 3136: loss 0.068784\n",
      "batch 3137: loss 0.025546\n",
      "batch 3138: loss 0.101232\n",
      "batch 3139: loss 0.088275\n",
      "batch 3140: loss 0.109791\n",
      "batch 3141: loss 0.188482\n",
      "batch 3142: loss 0.082869\n",
      "batch 3143: loss 0.059137\n",
      "batch 3144: loss 0.046590\n",
      "batch 3145: loss 0.027823\n",
      "batch 3146: loss 0.069031\n",
      "batch 3147: loss 0.074371\n",
      "batch 3148: loss 0.080499\n",
      "batch 3149: loss 0.020261\n",
      "batch 3150: loss 0.107369\n",
      "batch 3151: loss 0.068644\n",
      "batch 3152: loss 0.032162\n",
      "batch 3153: loss 0.079489\n",
      "batch 3154: loss 0.062571\n",
      "batch 3155: loss 0.059636\n",
      "batch 3156: loss 0.502568\n",
      "batch 3157: loss 0.037602\n",
      "batch 3158: loss 0.045051\n",
      "batch 3159: loss 0.109920\n",
      "batch 3160: loss 0.046332\n",
      "batch 3161: loss 0.030423\n",
      "batch 3162: loss 0.033432\n",
      "batch 3163: loss 0.063122\n",
      "batch 3164: loss 0.049918\n",
      "batch 3165: loss 0.070734\n",
      "batch 3166: loss 0.029993\n",
      "batch 3167: loss 0.190971\n",
      "batch 3168: loss 0.062813\n",
      "batch 3169: loss 0.171357\n",
      "batch 3170: loss 0.044129\n",
      "batch 3171: loss 0.059598\n",
      "batch 3172: loss 0.217900\n",
      "batch 3173: loss 0.104879\n",
      "batch 3174: loss 0.029390\n",
      "batch 3175: loss 0.085186\n",
      "batch 3176: loss 0.105555\n",
      "batch 3177: loss 0.036041\n",
      "batch 3178: loss 0.040219\n",
      "batch 3179: loss 0.104594\n",
      "batch 3180: loss 0.042830\n",
      "batch 3181: loss 0.157685\n",
      "batch 3182: loss 0.113241\n",
      "batch 3183: loss 0.305159\n",
      "batch 3184: loss 0.053706\n",
      "batch 3185: loss 0.021366\n",
      "batch 3186: loss 0.156210\n",
      "batch 3187: loss 0.127821\n",
      "batch 3188: loss 0.026225\n",
      "batch 3189: loss 0.055294\n",
      "batch 3190: loss 0.062089\n",
      "batch 3191: loss 0.101307\n",
      "batch 3192: loss 0.038763\n",
      "batch 3193: loss 0.116341\n",
      "batch 3194: loss 0.154166\n",
      "batch 3195: loss 0.039645\n",
      "batch 3196: loss 0.043660\n",
      "batch 3197: loss 0.074282\n",
      "batch 3198: loss 0.053924\n",
      "batch 3199: loss 0.176693\n",
      "batch 3200: loss 0.111289\n",
      "batch 3201: loss 0.077378\n",
      "batch 3202: loss 0.088271\n",
      "batch 3203: loss 0.082963\n",
      "batch 3204: loss 0.070100\n",
      "batch 3205: loss 0.053299\n",
      "batch 3206: loss 0.106744\n",
      "batch 3207: loss 0.052305\n",
      "batch 3208: loss 0.195489\n",
      "batch 3209: loss 0.146912\n",
      "batch 3210: loss 0.037894\n",
      "batch 3211: loss 0.044121\n",
      "batch 3212: loss 0.081013\n",
      "batch 3213: loss 0.029347\n",
      "batch 3214: loss 0.060082\n",
      "batch 3215: loss 0.080867\n",
      "batch 3216: loss 0.069058\n",
      "batch 3217: loss 0.039656\n",
      "batch 3218: loss 0.033014\n",
      "batch 3219: loss 0.085667\n",
      "batch 3220: loss 0.018678\n",
      "batch 3221: loss 0.203516\n",
      "batch 3222: loss 0.060107\n",
      "batch 3223: loss 0.090207\n",
      "batch 3224: loss 0.037347\n",
      "batch 3225: loss 0.031008\n",
      "batch 3226: loss 0.015825\n",
      "batch 3227: loss 0.054841\n",
      "batch 3228: loss 0.165428\n",
      "batch 3229: loss 0.233130\n",
      "batch 3230: loss 0.144559\n",
      "batch 3231: loss 0.019172\n",
      "batch 3232: loss 0.074193\n",
      "batch 3233: loss 0.042215\n",
      "batch 3234: loss 0.043248\n",
      "batch 3235: loss 0.050872\n",
      "batch 3236: loss 0.087563\n",
      "batch 3237: loss 0.037756\n",
      "batch 3238: loss 0.244768\n",
      "batch 3239: loss 0.060863\n",
      "batch 3240: loss 0.042653\n",
      "batch 3241: loss 0.056018\n",
      "batch 3242: loss 0.062819\n",
      "batch 3243: loss 0.050217\n",
      "batch 3244: loss 0.370125\n",
      "batch 3245: loss 0.063517\n",
      "batch 3246: loss 0.066523\n",
      "batch 3247: loss 0.057552\n",
      "batch 3248: loss 0.031958\n",
      "batch 3249: loss 0.116302\n",
      "batch 3250: loss 0.063956\n",
      "batch 3251: loss 0.101161\n",
      "batch 3252: loss 0.073996\n",
      "batch 3253: loss 0.127111\n",
      "batch 3254: loss 0.040821\n",
      "batch 3255: loss 0.029222\n",
      "batch 3256: loss 0.035207\n",
      "batch 3257: loss 0.113901\n",
      "batch 3258: loss 0.016129\n",
      "batch 3259: loss 0.090825\n",
      "batch 3260: loss 0.039157\n",
      "batch 3261: loss 0.065872\n",
      "batch 3262: loss 0.116738\n",
      "batch 3263: loss 0.111446\n",
      "batch 3264: loss 0.060352\n",
      "batch 3265: loss 0.013941\n",
      "batch 3266: loss 0.053885\n",
      "batch 3267: loss 0.202908\n",
      "batch 3268: loss 0.186107\n",
      "batch 3269: loss 0.036571\n",
      "batch 3270: loss 0.024542\n",
      "batch 3271: loss 0.074392\n",
      "batch 3272: loss 0.027376\n",
      "batch 3273: loss 0.085281\n",
      "batch 3274: loss 0.043600\n",
      "batch 3275: loss 0.104137\n",
      "batch 3276: loss 0.042942\n",
      "batch 3277: loss 0.040094\n",
      "batch 3278: loss 0.110999\n",
      "batch 3279: loss 0.047492\n",
      "batch 3280: loss 0.161962\n",
      "batch 3281: loss 0.051785\n",
      "batch 3282: loss 0.038732\n",
      "batch 3283: loss 0.031410\n",
      "batch 3284: loss 0.098088\n",
      "batch 3285: loss 0.057005\n",
      "batch 3286: loss 0.119532\n",
      "batch 3287: loss 0.056972\n",
      "batch 3288: loss 0.119490\n",
      "batch 3289: loss 0.051866\n",
      "batch 3290: loss 0.111319\n",
      "batch 3291: loss 0.039299\n",
      "batch 3292: loss 0.021711\n",
      "batch 3293: loss 0.056339\n",
      "batch 3294: loss 0.034472\n",
      "batch 3295: loss 0.219281\n",
      "batch 3296: loss 0.076853\n",
      "batch 3297: loss 0.110755\n",
      "batch 3298: loss 0.034457\n",
      "batch 3299: loss 0.076526\n",
      "batch 3300: loss 0.050096\n",
      "batch 3301: loss 0.109776\n",
      "batch 3302: loss 0.015817\n",
      "batch 3303: loss 0.088075\n",
      "batch 3304: loss 0.114906\n",
      "batch 3305: loss 0.021068\n",
      "batch 3306: loss 0.203225\n",
      "batch 3307: loss 0.020295\n",
      "batch 3308: loss 0.032874\n",
      "batch 3309: loss 0.047287\n",
      "batch 3310: loss 0.136572\n",
      "batch 3311: loss 0.036795\n",
      "batch 3312: loss 0.100066\n",
      "batch 3313: loss 0.031612\n",
      "batch 3314: loss 0.023391\n",
      "batch 3315: loss 0.122810\n",
      "batch 3316: loss 0.152180\n",
      "batch 3317: loss 0.176169\n",
      "batch 3318: loss 0.060595\n",
      "batch 3319: loss 0.141391\n",
      "batch 3320: loss 0.046099\n",
      "batch 3321: loss 0.221357\n",
      "batch 3322: loss 0.074733\n",
      "batch 3323: loss 0.095745\n",
      "batch 3324: loss 0.128783\n",
      "batch 3325: loss 0.066950\n",
      "batch 3326: loss 0.161549\n",
      "batch 3327: loss 0.074822\n",
      "batch 3328: loss 0.143190\n",
      "batch 3329: loss 0.176401\n",
      "batch 3330: loss 0.041199\n",
      "batch 3331: loss 0.017181\n",
      "batch 3332: loss 0.170465\n",
      "batch 3333: loss 0.072364\n",
      "batch 3334: loss 0.163615\n",
      "batch 3335: loss 0.042739\n",
      "batch 3336: loss 0.272249\n",
      "batch 3337: loss 0.024701\n",
      "batch 3338: loss 0.154978\n",
      "batch 3339: loss 0.101033\n",
      "batch 3340: loss 0.106190\n",
      "batch 3341: loss 0.060757\n",
      "batch 3342: loss 0.146771\n",
      "batch 3343: loss 0.020967\n",
      "batch 3344: loss 0.143612\n",
      "batch 3345: loss 0.053488\n",
      "batch 3346: loss 0.134525\n",
      "batch 3347: loss 0.049071\n",
      "batch 3348: loss 0.075218\n",
      "batch 3349: loss 0.047353\n",
      "batch 3350: loss 0.018342\n",
      "batch 3351: loss 0.139713\n",
      "batch 3352: loss 0.103824\n",
      "batch 3353: loss 0.059613\n",
      "batch 3354: loss 0.125503\n",
      "batch 3355: loss 0.017844\n",
      "batch 3356: loss 0.024316\n",
      "batch 3357: loss 0.121815\n",
      "batch 3358: loss 0.072034\n",
      "batch 3359: loss 0.109846\n",
      "batch 3360: loss 0.051031\n",
      "batch 3361: loss 0.028423\n",
      "batch 3362: loss 0.142367\n",
      "batch 3363: loss 0.140191\n",
      "batch 3364: loss 0.152367\n",
      "batch 3365: loss 0.034750\n",
      "batch 3366: loss 0.044541\n",
      "batch 3367: loss 0.117375\n",
      "batch 3368: loss 0.151054\n",
      "batch 3369: loss 0.120475\n",
      "batch 3370: loss 0.040063\n",
      "batch 3371: loss 0.018909\n",
      "batch 3372: loss 0.137824\n",
      "batch 3373: loss 0.020128\n",
      "batch 3374: loss 0.011578\n",
      "batch 3375: loss 0.128783\n",
      "batch 3376: loss 0.051643\n",
      "batch 3377: loss 0.083017\n",
      "batch 3378: loss 0.140227\n",
      "batch 3379: loss 0.032677\n",
      "batch 3380: loss 0.097992\n",
      "batch 3381: loss 0.090952\n",
      "batch 3382: loss 0.278031\n",
      "batch 3383: loss 0.124014\n",
      "batch 3384: loss 0.078258\n",
      "batch 3385: loss 0.095800\n",
      "batch 3386: loss 0.100512\n",
      "batch 3387: loss 0.092374\n",
      "batch 3388: loss 0.057701\n",
      "batch 3389: loss 0.183892\n",
      "batch 3390: loss 0.061743\n",
      "batch 3391: loss 0.073240\n",
      "batch 3392: loss 0.020992\n",
      "batch 3393: loss 0.013692\n",
      "batch 3394: loss 0.137010\n",
      "batch 3395: loss 0.034840\n",
      "batch 3396: loss 0.055163\n",
      "batch 3397: loss 0.100114\n",
      "batch 3398: loss 0.088138\n",
      "batch 3399: loss 0.038316\n",
      "batch 3400: loss 0.057063\n",
      "batch 3401: loss 0.169846\n",
      "batch 3402: loss 0.081917\n",
      "batch 3403: loss 0.021479\n",
      "batch 3404: loss 0.090270\n",
      "batch 3405: loss 0.259443\n",
      "batch 3406: loss 0.036564\n",
      "batch 3407: loss 0.021509\n",
      "batch 3408: loss 0.083226\n",
      "batch 3409: loss 0.110985\n",
      "batch 3410: loss 0.140804\n",
      "batch 3411: loss 0.061868\n",
      "batch 3412: loss 0.050124\n",
      "batch 3413: loss 0.082248\n",
      "batch 3414: loss 0.011824\n",
      "batch 3415: loss 0.025220\n",
      "batch 3416: loss 0.084810\n",
      "batch 3417: loss 0.187642\n",
      "batch 3418: loss 0.061557\n",
      "batch 3419: loss 0.020964\n",
      "batch 3420: loss 0.013682\n",
      "batch 3421: loss 0.081682\n",
      "batch 3422: loss 0.094350\n",
      "batch 3423: loss 0.094047\n",
      "batch 3424: loss 0.038451\n",
      "batch 3425: loss 0.043528\n",
      "batch 3426: loss 0.088477\n",
      "batch 3427: loss 0.102698\n",
      "batch 3428: loss 0.028002\n",
      "batch 3429: loss 0.062616\n",
      "batch 3430: loss 0.054344\n",
      "batch 3431: loss 0.135426\n",
      "batch 3432: loss 0.124276\n",
      "batch 3433: loss 0.067334\n",
      "batch 3434: loss 0.065100\n",
      "batch 3435: loss 0.137664\n",
      "batch 3436: loss 0.021128\n",
      "batch 3437: loss 0.068202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3438: loss 0.092016\n",
      "batch 3439: loss 0.112044\n",
      "batch 3440: loss 0.060249\n",
      "batch 3441: loss 0.013812\n",
      "batch 3442: loss 0.028046\n",
      "batch 3443: loss 0.061894\n",
      "batch 3444: loss 0.116213\n",
      "batch 3445: loss 0.031280\n",
      "batch 3446: loss 0.025860\n",
      "batch 3447: loss 0.030440\n",
      "batch 3448: loss 0.079613\n",
      "batch 3449: loss 0.038523\n",
      "batch 3450: loss 0.103243\n",
      "batch 3451: loss 0.031568\n",
      "batch 3452: loss 0.181674\n",
      "batch 3453: loss 0.109747\n",
      "batch 3454: loss 0.115662\n",
      "batch 3455: loss 0.036017\n",
      "batch 3456: loss 0.049928\n",
      "batch 3457: loss 0.060049\n",
      "batch 3458: loss 0.062487\n",
      "batch 3459: loss 0.009641\n",
      "batch 3460: loss 0.051450\n",
      "batch 3461: loss 0.019149\n",
      "batch 3462: loss 0.020679\n",
      "batch 3463: loss 0.023090\n",
      "batch 3464: loss 0.023732\n",
      "batch 3465: loss 0.023922\n",
      "batch 3466: loss 0.111149\n",
      "batch 3467: loss 0.043209\n",
      "batch 3468: loss 0.132074\n",
      "batch 3469: loss 0.076362\n",
      "batch 3470: loss 0.088863\n",
      "batch 3471: loss 0.059402\n",
      "batch 3472: loss 0.197303\n",
      "batch 3473: loss 0.037307\n",
      "batch 3474: loss 0.091858\n",
      "batch 3475: loss 0.034278\n",
      "batch 3476: loss 0.037259\n",
      "batch 3477: loss 0.083633\n",
      "batch 3478: loss 0.028234\n",
      "batch 3479: loss 0.022444\n",
      "batch 3480: loss 0.044296\n",
      "batch 3481: loss 0.083854\n",
      "batch 3482: loss 0.012265\n",
      "batch 3483: loss 0.018552\n",
      "batch 3484: loss 0.168807\n",
      "batch 3485: loss 0.077829\n",
      "batch 3486: loss 0.064597\n",
      "batch 3487: loss 0.047351\n",
      "batch 3488: loss 0.099495\n",
      "batch 3489: loss 0.103966\n",
      "batch 3490: loss 0.155549\n",
      "batch 3491: loss 0.127721\n",
      "batch 3492: loss 0.078270\n",
      "batch 3493: loss 0.015526\n",
      "batch 3494: loss 0.030205\n",
      "batch 3495: loss 0.041149\n",
      "batch 3496: loss 0.031893\n",
      "batch 3497: loss 0.057938\n",
      "batch 3498: loss 0.056697\n",
      "batch 3499: loss 0.171008\n",
      "batch 3500: loss 0.063630\n",
      "batch 3501: loss 0.073533\n",
      "batch 3502: loss 0.082364\n",
      "batch 3503: loss 0.099052\n",
      "batch 3504: loss 0.095527\n",
      "batch 3505: loss 0.053245\n",
      "batch 3506: loss 0.022763\n",
      "batch 3507: loss 0.012716\n",
      "batch 3508: loss 0.069933\n",
      "batch 3509: loss 0.094905\n",
      "batch 3510: loss 0.176588\n",
      "batch 3511: loss 0.087167\n",
      "batch 3512: loss 0.081471\n",
      "batch 3513: loss 0.058271\n",
      "batch 3514: loss 0.048820\n",
      "batch 3515: loss 0.082965\n",
      "batch 3516: loss 0.130517\n",
      "batch 3517: loss 0.121354\n",
      "batch 3518: loss 0.048234\n",
      "batch 3519: loss 0.106045\n",
      "batch 3520: loss 0.058861\n",
      "batch 3521: loss 0.076688\n",
      "batch 3522: loss 0.078122\n",
      "batch 3523: loss 0.038931\n",
      "batch 3524: loss 0.048584\n",
      "batch 3525: loss 0.021576\n",
      "batch 3526: loss 0.021994\n",
      "batch 3527: loss 0.069630\n",
      "batch 3528: loss 0.107569\n",
      "batch 3529: loss 0.116904\n",
      "batch 3530: loss 0.156686\n",
      "batch 3531: loss 0.034218\n",
      "batch 3532: loss 0.178395\n",
      "batch 3533: loss 0.134835\n",
      "batch 3534: loss 0.087308\n",
      "batch 3535: loss 0.024980\n",
      "batch 3536: loss 0.072481\n",
      "batch 3537: loss 0.017209\n",
      "batch 3538: loss 0.020044\n",
      "batch 3539: loss 0.030417\n",
      "batch 3540: loss 0.134488\n",
      "batch 3541: loss 0.099507\n",
      "batch 3542: loss 0.094519\n",
      "batch 3543: loss 0.042145\n",
      "batch 3544: loss 0.045488\n",
      "batch 3545: loss 0.080717\n",
      "batch 3546: loss 0.051734\n",
      "batch 3547: loss 0.051228\n",
      "batch 3548: loss 0.065478\n",
      "batch 3549: loss 0.030105\n",
      "batch 3550: loss 0.027089\n",
      "batch 3551: loss 0.062049\n",
      "batch 3552: loss 0.073478\n",
      "batch 3553: loss 0.019278\n",
      "batch 3554: loss 0.032219\n",
      "batch 3555: loss 0.032247\n",
      "batch 3556: loss 0.029632\n",
      "batch 3557: loss 0.036612\n",
      "batch 3558: loss 0.059832\n",
      "batch 3559: loss 0.158721\n",
      "batch 3560: loss 0.080259\n",
      "batch 3561: loss 0.041515\n",
      "batch 3562: loss 0.084306\n",
      "batch 3563: loss 0.095400\n",
      "batch 3564: loss 0.053944\n",
      "batch 3565: loss 0.011030\n",
      "batch 3566: loss 0.019198\n",
      "batch 3567: loss 0.155426\n",
      "batch 3568: loss 0.023418\n",
      "batch 3569: loss 0.060320\n",
      "batch 3570: loss 0.142833\n",
      "batch 3571: loss 0.095312\n",
      "batch 3572: loss 0.082633\n",
      "batch 3573: loss 0.119232\n",
      "batch 3574: loss 0.092573\n",
      "batch 3575: loss 0.055770\n",
      "batch 3576: loss 0.042386\n",
      "batch 3577: loss 0.029666\n",
      "batch 3578: loss 0.051269\n",
      "batch 3579: loss 0.249811\n",
      "batch 3580: loss 0.021183\n",
      "batch 3581: loss 0.036117\n",
      "batch 3582: loss 0.077950\n",
      "batch 3583: loss 0.043013\n",
      "batch 3584: loss 0.071364\n",
      "batch 3585: loss 0.047519\n",
      "batch 3586: loss 0.134081\n",
      "batch 3587: loss 0.081536\n",
      "batch 3588: loss 0.049210\n",
      "batch 3589: loss 0.092545\n",
      "batch 3590: loss 0.037340\n",
      "batch 3591: loss 0.114583\n",
      "batch 3592: loss 0.103329\n",
      "batch 3593: loss 0.055575\n",
      "batch 3594: loss 0.026652\n",
      "batch 3595: loss 0.079857\n",
      "batch 3596: loss 0.052070\n",
      "batch 3597: loss 0.128295\n",
      "batch 3598: loss 0.077814\n",
      "batch 3599: loss 0.167512\n",
      "batch 3600: loss 0.051967\n",
      "batch 3601: loss 0.035551\n",
      "batch 3602: loss 0.035535\n",
      "batch 3603: loss 0.065615\n",
      "batch 3604: loss 0.011529\n",
      "batch 3605: loss 0.091854\n",
      "batch 3606: loss 0.040493\n",
      "batch 3607: loss 0.024961\n",
      "batch 3608: loss 0.101216\n",
      "batch 3609: loss 0.034996\n",
      "batch 3610: loss 0.079509\n",
      "batch 3611: loss 0.085440\n",
      "batch 3612: loss 0.070647\n",
      "batch 3613: loss 0.023525\n",
      "batch 3614: loss 0.036550\n",
      "batch 3615: loss 0.106761\n",
      "batch 3616: loss 0.018349\n",
      "batch 3617: loss 0.046003\n",
      "batch 3618: loss 0.016733\n",
      "batch 3619: loss 0.102107\n",
      "batch 3620: loss 0.056866\n",
      "batch 3621: loss 0.018661\n",
      "batch 3622: loss 0.039969\n",
      "batch 3623: loss 0.182750\n",
      "batch 3624: loss 0.047687\n",
      "batch 3625: loss 0.034916\n",
      "batch 3626: loss 0.040603\n",
      "batch 3627: loss 0.010150\n",
      "batch 3628: loss 0.046064\n",
      "batch 3629: loss 0.014627\n",
      "batch 3630: loss 0.031563\n",
      "batch 3631: loss 0.016722\n",
      "batch 3632: loss 0.068578\n",
      "batch 3633: loss 0.103152\n",
      "batch 3634: loss 0.011682\n",
      "batch 3635: loss 0.036745\n",
      "batch 3636: loss 0.120704\n",
      "batch 3637: loss 0.094286\n",
      "batch 3638: loss 0.023179\n",
      "batch 3639: loss 0.041857\n",
      "batch 3640: loss 0.037849\n",
      "batch 3641: loss 0.080807\n",
      "batch 3642: loss 0.078676\n",
      "batch 3643: loss 0.010205\n",
      "batch 3644: loss 0.045399\n",
      "batch 3645: loss 0.052221\n",
      "batch 3646: loss 0.120448\n",
      "batch 3647: loss 0.045875\n",
      "batch 3648: loss 0.027616\n",
      "batch 3649: loss 0.070609\n",
      "batch 3650: loss 0.004528\n",
      "batch 3651: loss 0.116339\n",
      "batch 3652: loss 0.042710\n",
      "batch 3653: loss 0.065374\n",
      "batch 3654: loss 0.031520\n",
      "batch 3655: loss 0.025853\n",
      "batch 3656: loss 0.025318\n",
      "batch 3657: loss 0.391131\n",
      "batch 3658: loss 0.189432\n",
      "batch 3659: loss 0.035124\n",
      "batch 3660: loss 0.056951\n",
      "batch 3661: loss 0.030116\n",
      "batch 3662: loss 0.046770\n",
      "batch 3663: loss 0.116654\n",
      "batch 3664: loss 0.155400\n",
      "batch 3665: loss 0.184311\n",
      "batch 3666: loss 0.054869\n",
      "batch 3667: loss 0.015868\n",
      "batch 3668: loss 0.107317\n",
      "batch 3669: loss 0.056520\n",
      "batch 3670: loss 0.101701\n",
      "batch 3671: loss 0.307737\n",
      "batch 3672: loss 0.021245\n",
      "batch 3673: loss 0.133533\n",
      "batch 3674: loss 0.131654\n",
      "batch 3675: loss 0.032043\n",
      "batch 3676: loss 0.192302\n",
      "batch 3677: loss 0.046509\n",
      "batch 3678: loss 0.192633\n",
      "batch 3679: loss 0.024208\n",
      "batch 3680: loss 0.063717\n",
      "batch 3681: loss 0.019198\n",
      "batch 3682: loss 0.198819\n",
      "batch 3683: loss 0.013766\n",
      "batch 3684: loss 0.100335\n",
      "batch 3685: loss 0.063020\n",
      "batch 3686: loss 0.057076\n",
      "batch 3687: loss 0.097789\n",
      "batch 3688: loss 0.140345\n",
      "batch 3689: loss 0.052572\n",
      "batch 3690: loss 0.121367\n",
      "batch 3691: loss 0.092431\n",
      "batch 3692: loss 0.023537\n",
      "batch 3693: loss 0.176135\n",
      "batch 3694: loss 0.147766\n",
      "batch 3695: loss 0.079535\n",
      "batch 3696: loss 0.102406\n",
      "batch 3697: loss 0.141880\n",
      "batch 3698: loss 0.077324\n",
      "batch 3699: loss 0.086686\n",
      "batch 3700: loss 0.023016\n",
      "batch 3701: loss 0.052856\n",
      "batch 3702: loss 0.048649\n",
      "batch 3703: loss 0.219763\n",
      "batch 3704: loss 0.036015\n",
      "batch 3705: loss 0.085513\n",
      "batch 3706: loss 0.046954\n",
      "batch 3707: loss 0.100452\n",
      "batch 3708: loss 0.091637\n",
      "batch 3709: loss 0.064258\n",
      "batch 3710: loss 0.144166\n",
      "batch 3711: loss 0.058544\n",
      "batch 3712: loss 0.148129\n",
      "batch 3713: loss 0.084809\n",
      "batch 3714: loss 0.015374\n",
      "batch 3715: loss 0.033503\n",
      "batch 3716: loss 0.017569\n",
      "batch 3717: loss 0.224048\n",
      "batch 3718: loss 0.179674\n",
      "batch 3719: loss 0.133561\n",
      "batch 3720: loss 0.112529\n",
      "batch 3721: loss 0.234288\n",
      "batch 3722: loss 0.033518\n",
      "batch 3723: loss 0.050798\n",
      "batch 3724: loss 0.046536\n",
      "batch 3725: loss 0.061803\n",
      "batch 3726: loss 0.040031\n",
      "batch 3727: loss 0.057889\n",
      "batch 3728: loss 0.021021\n",
      "batch 3729: loss 0.168457\n",
      "batch 3730: loss 0.052710\n",
      "batch 3731: loss 0.024390\n",
      "batch 3732: loss 0.095485\n",
      "batch 3733: loss 0.080406\n",
      "batch 3734: loss 0.039930\n",
      "batch 3735: loss 0.062725\n",
      "batch 3736: loss 0.065419\n",
      "batch 3737: loss 0.069023\n",
      "batch 3738: loss 0.057975\n",
      "batch 3739: loss 0.118315\n",
      "batch 3740: loss 0.051329\n",
      "batch 3741: loss 0.110582\n",
      "batch 3742: loss 0.114076\n",
      "batch 3743: loss 0.091396\n",
      "batch 3744: loss 0.035167\n",
      "batch 3745: loss 0.033326\n",
      "batch 3746: loss 0.038713\n",
      "batch 3747: loss 0.060795\n",
      "batch 3748: loss 0.013344\n",
      "batch 3749: loss 0.018411\n",
      "batch 3750: loss 0.028674\n",
      "batch 3751: loss 0.296297\n",
      "batch 3752: loss 0.106214\n",
      "batch 3753: loss 0.057849\n",
      "batch 3754: loss 0.061492\n",
      "batch 3755: loss 0.092109\n",
      "batch 3756: loss 0.050846\n",
      "batch 3757: loss 0.060832\n",
      "batch 3758: loss 0.086387\n",
      "batch 3759: loss 0.097680\n",
      "batch 3760: loss 0.045707\n",
      "batch 3761: loss 0.015200\n",
      "batch 3762: loss 0.013185\n",
      "batch 3763: loss 0.071369\n",
      "batch 3764: loss 0.023138\n",
      "batch 3765: loss 0.083652\n",
      "batch 3766: loss 0.037856\n",
      "batch 3767: loss 0.027982\n",
      "batch 3768: loss 0.066252\n",
      "batch 3769: loss 0.204937\n",
      "batch 3770: loss 0.047644\n",
      "batch 3771: loss 0.056393\n",
      "batch 3772: loss 0.014347\n",
      "batch 3773: loss 0.074154\n",
      "batch 3774: loss 0.017542\n",
      "batch 3775: loss 0.063054\n",
      "batch 3776: loss 0.064505\n",
      "batch 3777: loss 0.112808\n",
      "batch 3778: loss 0.036569\n",
      "batch 3779: loss 0.110636\n",
      "batch 3780: loss 0.054941\n",
      "batch 3781: loss 0.015628\n",
      "batch 3782: loss 0.020439\n",
      "batch 3783: loss 0.070476\n",
      "batch 3784: loss 0.077249\n",
      "batch 3785: loss 0.051919\n",
      "batch 3786: loss 0.069005\n",
      "batch 3787: loss 0.015591\n",
      "batch 3788: loss 0.061555\n",
      "batch 3789: loss 0.159402\n",
      "batch 3790: loss 0.055828\n",
      "batch 3791: loss 0.076263\n",
      "batch 3792: loss 0.028643\n",
      "batch 3793: loss 0.027460\n",
      "batch 3794: loss 0.173389\n",
      "batch 3795: loss 0.007774\n",
      "batch 3796: loss 0.034442\n",
      "batch 3797: loss 0.027035\n",
      "batch 3798: loss 0.042113\n",
      "batch 3799: loss 0.005829\n",
      "batch 3800: loss 0.068186\n",
      "batch 3801: loss 0.014880\n",
      "batch 3802: loss 0.072718\n",
      "batch 3803: loss 0.092752\n",
      "batch 3804: loss 0.228692\n",
      "batch 3805: loss 0.047152\n",
      "batch 3806: loss 0.066375\n",
      "batch 3807: loss 0.030821\n",
      "batch 3808: loss 0.014735\n",
      "batch 3809: loss 0.063526\n",
      "batch 3810: loss 0.031677\n",
      "batch 3811: loss 0.013256\n",
      "batch 3812: loss 0.068352\n",
      "batch 3813: loss 0.038887\n",
      "batch 3814: loss 0.196807\n",
      "batch 3815: loss 0.034512\n",
      "batch 3816: loss 0.029471\n",
      "batch 3817: loss 0.046305\n",
      "batch 3818: loss 0.030907\n",
      "batch 3819: loss 0.021188\n",
      "batch 3820: loss 0.088778\n",
      "batch 3821: loss 0.203685\n",
      "batch 3822: loss 0.041236\n",
      "batch 3823: loss 0.026203\n",
      "batch 3824: loss 0.158816\n",
      "batch 3825: loss 0.199981\n",
      "batch 3826: loss 0.032334\n",
      "batch 3827: loss 0.020799\n",
      "batch 3828: loss 0.053606\n",
      "batch 3829: loss 0.035456\n",
      "batch 3830: loss 0.055847\n",
      "batch 3831: loss 0.016426\n",
      "batch 3832: loss 0.096972\n",
      "batch 3833: loss 0.038962\n",
      "batch 3834: loss 0.120077\n",
      "batch 3835: loss 0.021702\n",
      "batch 3836: loss 0.074728\n",
      "batch 3837: loss 0.026168\n",
      "batch 3838: loss 0.182432\n",
      "batch 3839: loss 0.038519\n",
      "batch 3840: loss 0.062538\n",
      "batch 3841: loss 0.157883\n",
      "batch 3842: loss 0.033726\n",
      "batch 3843: loss 0.066776\n",
      "batch 3844: loss 0.038798\n",
      "batch 3845: loss 0.059210\n",
      "batch 3846: loss 0.032418\n",
      "batch 3847: loss 0.194354\n",
      "batch 3848: loss 0.097351\n",
      "batch 3849: loss 0.122897\n",
      "batch 3850: loss 0.026012\n",
      "batch 3851: loss 0.041703\n",
      "batch 3852: loss 0.041306\n",
      "batch 3853: loss 0.027432\n",
      "batch 3854: loss 0.042000\n",
      "batch 3855: loss 0.072787\n",
      "batch 3856: loss 0.116470\n",
      "batch 3857: loss 0.010604\n",
      "batch 3858: loss 0.015586\n",
      "batch 3859: loss 0.054373\n",
      "batch 3860: loss 0.054416\n",
      "batch 3861: loss 0.017257\n",
      "batch 3862: loss 0.058973\n",
      "batch 3863: loss 0.009014\n",
      "batch 3864: loss 0.023165\n",
      "batch 3865: loss 0.032559\n",
      "batch 3866: loss 0.063252\n",
      "batch 3867: loss 0.037259\n",
      "batch 3868: loss 0.092984\n",
      "batch 3869: loss 0.015955\n",
      "batch 3870: loss 0.054408\n",
      "batch 3871: loss 0.100564\n",
      "batch 3872: loss 0.149942\n",
      "batch 3873: loss 0.106336\n",
      "batch 3874: loss 0.014562\n",
      "batch 3875: loss 0.032674\n",
      "batch 3876: loss 0.013575\n",
      "batch 3877: loss 0.260647\n",
      "batch 3878: loss 0.094663\n",
      "batch 3879: loss 0.100179\n",
      "batch 3880: loss 0.062314\n",
      "batch 3881: loss 0.048895\n",
      "batch 3882: loss 0.030910\n",
      "batch 3883: loss 0.050420\n",
      "batch 3884: loss 0.059321\n",
      "batch 3885: loss 0.079213\n",
      "batch 3886: loss 0.103944\n",
      "batch 3887: loss 0.023016\n",
      "batch 3888: loss 0.153317\n",
      "batch 3889: loss 0.016898\n",
      "batch 3890: loss 0.035671\n",
      "batch 3891: loss 0.113251\n",
      "batch 3892: loss 0.069046\n",
      "batch 3893: loss 0.086335\n",
      "batch 3894: loss 0.012648\n",
      "batch 3895: loss 0.071627\n",
      "batch 3896: loss 0.134464\n",
      "batch 3897: loss 0.035668\n",
      "batch 3898: loss 0.095479\n",
      "batch 3899: loss 0.017193\n",
      "batch 3900: loss 0.028135\n",
      "batch 3901: loss 0.134565\n",
      "batch 3902: loss 0.291235\n",
      "batch 3903: loss 0.077040\n",
      "batch 3904: loss 0.088554\n",
      "batch 3905: loss 0.064291\n",
      "batch 3906: loss 0.188308\n",
      "batch 3907: loss 0.151504\n",
      "batch 3908: loss 0.116828\n",
      "batch 3909: loss 0.017942\n",
      "batch 3910: loss 0.059701\n",
      "batch 3911: loss 0.098538\n",
      "batch 3912: loss 0.209401\n",
      "batch 3913: loss 0.064604\n",
      "batch 3914: loss 0.097436\n",
      "batch 3915: loss 0.097840\n",
      "batch 3916: loss 0.361281\n",
      "batch 3917: loss 0.054502\n",
      "batch 3918: loss 0.045710\n",
      "batch 3919: loss 0.053142\n",
      "batch 3920: loss 0.082073\n",
      "batch 3921: loss 0.030606\n",
      "batch 3922: loss 0.169880\n",
      "batch 3923: loss 0.054858\n",
      "batch 3924: loss 0.077736\n",
      "batch 3925: loss 0.176346\n",
      "batch 3926: loss 0.054164\n",
      "batch 3927: loss 0.051525\n",
      "batch 3928: loss 0.036472\n",
      "batch 3929: loss 0.180518\n",
      "batch 3930: loss 0.091501\n",
      "batch 3931: loss 0.032421\n",
      "batch 3932: loss 0.073441\n",
      "batch 3933: loss 0.063814\n",
      "batch 3934: loss 0.140918\n",
      "batch 3935: loss 0.118283\n",
      "batch 3936: loss 0.042769\n",
      "batch 3937: loss 0.068433\n",
      "batch 3938: loss 0.046603\n",
      "batch 3939: loss 0.027519\n",
      "batch 3940: loss 0.064276\n",
      "batch 3941: loss 0.136042\n",
      "batch 3942: loss 0.111878\n",
      "batch 3943: loss 0.109309\n",
      "batch 3944: loss 0.155644\n",
      "batch 3945: loss 0.065380\n",
      "batch 3946: loss 0.033320\n",
      "batch 3947: loss 0.016136\n",
      "batch 3948: loss 0.095545\n",
      "batch 3949: loss 0.067248\n",
      "batch 3950: loss 0.021519\n",
      "batch 3951: loss 0.030753\n",
      "batch 3952: loss 0.065400\n",
      "batch 3953: loss 0.072371\n",
      "batch 3954: loss 0.046388\n",
      "batch 3955: loss 0.165927\n",
      "batch 3956: loss 0.053092\n",
      "batch 3957: loss 0.139950\n",
      "batch 3958: loss 0.036829\n",
      "batch 3959: loss 0.036878\n",
      "batch 3960: loss 0.032762\n",
      "batch 3961: loss 0.045846\n",
      "batch 3962: loss 0.031681\n",
      "batch 3963: loss 0.028574\n",
      "batch 3964: loss 0.033293\n",
      "batch 3965: loss 0.030878\n",
      "batch 3966: loss 0.229875\n",
      "batch 3967: loss 0.038787\n",
      "batch 3968: loss 0.037257\n",
      "batch 3969: loss 0.014998\n",
      "batch 3970: loss 0.035828\n",
      "batch 3971: loss 0.152213\n",
      "batch 3972: loss 0.035395\n",
      "batch 3973: loss 0.217392\n",
      "batch 3974: loss 0.153049\n",
      "batch 3975: loss 0.083217\n",
      "batch 3976: loss 0.046567\n",
      "batch 3977: loss 0.031866\n",
      "batch 3978: loss 0.136302\n",
      "batch 3979: loss 0.108439\n",
      "batch 3980: loss 0.038171\n",
      "batch 3981: loss 0.051728\n",
      "batch 3982: loss 0.100875\n",
      "batch 3983: loss 0.115825\n",
      "batch 3984: loss 0.170585\n",
      "batch 3985: loss 0.129787\n",
      "batch 3986: loss 0.073581\n",
      "batch 3987: loss 0.072598\n",
      "batch 3988: loss 0.105815\n",
      "batch 3989: loss 0.132158\n",
      "batch 3990: loss 0.122804\n",
      "batch 3991: loss 0.041777\n",
      "batch 3992: loss 0.178098\n",
      "batch 3993: loss 0.146127\n",
      "batch 3994: loss 0.051193\n",
      "batch 3995: loss 0.043663\n",
      "batch 3996: loss 0.056926\n",
      "batch 3997: loss 0.062685\n",
      "batch 3998: loss 0.093591\n",
      "batch 3999: loss 0.055781\n",
      "batch 4000: loss 0.058809\n",
      "batch 4001: loss 0.091620\n",
      "batch 4002: loss 0.198072\n",
      "batch 4003: loss 0.055815\n",
      "batch 4004: loss 0.098221\n",
      "batch 4005: loss 0.240458\n",
      "batch 4006: loss 0.076798\n",
      "batch 4007: loss 0.022236\n",
      "batch 4008: loss 0.054326\n",
      "batch 4009: loss 0.156122\n",
      "batch 4010: loss 0.051779\n",
      "batch 4011: loss 0.123513\n",
      "batch 4012: loss 0.029779\n",
      "batch 4013: loss 0.013097\n",
      "batch 4014: loss 0.027069\n",
      "batch 4015: loss 0.320374\n",
      "batch 4016: loss 0.086372\n",
      "batch 4017: loss 0.118075\n",
      "batch 4018: loss 0.009257\n",
      "batch 4019: loss 0.035748\n",
      "batch 4020: loss 0.026054\n",
      "batch 4021: loss 0.075988\n",
      "batch 4022: loss 0.028101\n",
      "batch 4023: loss 0.029018\n",
      "batch 4024: loss 0.069174\n",
      "batch 4025: loss 0.123319\n",
      "batch 4026: loss 0.036224\n",
      "batch 4027: loss 0.139697\n",
      "batch 4028: loss 0.041050\n",
      "batch 4029: loss 0.045081\n",
      "batch 4030: loss 0.005525\n",
      "batch 4031: loss 0.090947\n",
      "batch 4032: loss 0.036209\n",
      "batch 4033: loss 0.225227\n",
      "batch 4034: loss 0.053816\n",
      "batch 4035: loss 0.088859\n",
      "batch 4036: loss 0.056210\n",
      "batch 4037: loss 0.032018\n",
      "batch 4038: loss 0.089141\n",
      "batch 4039: loss 0.160511\n",
      "batch 4040: loss 0.041042\n",
      "batch 4041: loss 0.035906\n",
      "batch 4042: loss 0.190289\n",
      "batch 4043: loss 0.107592\n",
      "batch 4044: loss 0.040748\n",
      "batch 4045: loss 0.037139\n",
      "batch 4046: loss 0.136338\n",
      "batch 4047: loss 0.031020\n",
      "batch 4048: loss 0.080340\n",
      "batch 4049: loss 0.014170\n",
      "batch 4050: loss 0.152872\n",
      "batch 4051: loss 0.060344\n",
      "batch 4052: loss 0.113373\n",
      "batch 4053: loss 0.093015\n",
      "batch 4054: loss 0.082458\n",
      "batch 4055: loss 0.059802\n",
      "batch 4056: loss 0.065231\n",
      "batch 4057: loss 0.027682\n",
      "batch 4058: loss 0.154408\n",
      "batch 4059: loss 0.019031\n",
      "batch 4060: loss 0.042625\n",
      "batch 4061: loss 0.079265\n",
      "batch 4062: loss 0.065746\n",
      "batch 4063: loss 0.069505\n",
      "batch 4064: loss 0.063125\n",
      "batch 4065: loss 0.057875\n",
      "batch 4066: loss 0.057368\n",
      "batch 4067: loss 0.229753\n",
      "batch 4068: loss 0.071390\n",
      "batch 4069: loss 0.054382\n",
      "batch 4070: loss 0.018245\n",
      "batch 4071: loss 0.057879\n",
      "batch 4072: loss 0.112429\n",
      "batch 4073: loss 0.096025\n",
      "batch 4074: loss 0.027988\n",
      "batch 4075: loss 0.051461\n",
      "batch 4076: loss 0.032215\n",
      "batch 4077: loss 0.026511\n",
      "batch 4078: loss 0.014076\n",
      "batch 4079: loss 0.061989\n",
      "batch 4080: loss 0.094685\n",
      "batch 4081: loss 0.101108\n",
      "batch 4082: loss 0.044029\n",
      "batch 4083: loss 0.167119\n",
      "batch 4084: loss 0.089355\n",
      "batch 4085: loss 0.058887\n",
      "batch 4086: loss 0.032072\n",
      "batch 4087: loss 0.019368\n",
      "batch 4088: loss 0.035612\n",
      "batch 4089: loss 0.008733\n",
      "batch 4090: loss 0.092976\n",
      "batch 4091: loss 0.052170\n",
      "batch 4092: loss 0.026462\n",
      "batch 4093: loss 0.039342\n",
      "batch 4094: loss 0.007538\n",
      "batch 4095: loss 0.057350\n",
      "batch 4096: loss 0.053867\n",
      "batch 4097: loss 0.067448\n",
      "batch 4098: loss 0.025316\n",
      "batch 4099: loss 0.025646\n",
      "batch 4100: loss 0.063579\n",
      "batch 4101: loss 0.046000\n",
      "batch 4102: loss 0.159453\n",
      "batch 4103: loss 0.049803\n",
      "batch 4104: loss 0.019741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4105: loss 0.117547\n",
      "batch 4106: loss 0.107104\n",
      "batch 4107: loss 0.128382\n",
      "batch 4108: loss 0.009036\n",
      "batch 4109: loss 0.181404\n",
      "batch 4110: loss 0.052840\n",
      "batch 4111: loss 0.079171\n",
      "batch 4112: loss 0.087171\n",
      "batch 4113: loss 0.029150\n",
      "batch 4114: loss 0.010346\n",
      "batch 4115: loss 0.052717\n",
      "batch 4116: loss 0.122202\n",
      "batch 4117: loss 0.018406\n",
      "batch 4118: loss 0.099968\n",
      "batch 4119: loss 0.128224\n",
      "batch 4120: loss 0.292793\n",
      "batch 4121: loss 0.014528\n",
      "batch 4122: loss 0.039510\n",
      "batch 4123: loss 0.103933\n",
      "batch 4124: loss 0.051691\n",
      "batch 4125: loss 0.013796\n",
      "batch 4126: loss 0.111367\n",
      "batch 4127: loss 0.032550\n",
      "batch 4128: loss 0.046131\n",
      "batch 4129: loss 0.039387\n",
      "batch 4130: loss 0.075228\n",
      "batch 4131: loss 0.025649\n",
      "batch 4132: loss 0.120380\n",
      "batch 4133: loss 0.141569\n",
      "batch 4134: loss 0.015871\n",
      "batch 4135: loss 0.015195\n",
      "batch 4136: loss 0.033848\n",
      "batch 4137: loss 0.076257\n",
      "batch 4138: loss 0.077486\n",
      "batch 4139: loss 0.024437\n",
      "batch 4140: loss 0.069400\n",
      "batch 4141: loss 0.014867\n",
      "batch 4142: loss 0.014484\n",
      "batch 4143: loss 0.030332\n",
      "batch 4144: loss 0.085326\n",
      "batch 4145: loss 0.103084\n",
      "batch 4146: loss 0.241913\n",
      "batch 4147: loss 0.046190\n",
      "batch 4148: loss 0.044283\n",
      "batch 4149: loss 0.117619\n",
      "batch 4150: loss 0.020478\n",
      "batch 4151: loss 0.105840\n",
      "batch 4152: loss 0.038528\n",
      "batch 4153: loss 0.060697\n",
      "batch 4154: loss 0.085040\n",
      "batch 4155: loss 0.050152\n",
      "batch 4156: loss 0.011864\n",
      "batch 4157: loss 0.038305\n",
      "batch 4158: loss 0.028437\n",
      "batch 4159: loss 0.029149\n",
      "batch 4160: loss 0.034922\n",
      "batch 4161: loss 0.065911\n",
      "batch 4162: loss 0.078739\n",
      "batch 4163: loss 0.060644\n",
      "batch 4164: loss 0.020436\n",
      "batch 4165: loss 0.151999\n",
      "batch 4166: loss 0.011511\n",
      "batch 4167: loss 0.110602\n",
      "batch 4168: loss 0.028901\n",
      "batch 4169: loss 0.053512\n",
      "batch 4170: loss 0.027916\n",
      "batch 4171: loss 0.045910\n",
      "batch 4172: loss 0.050560\n",
      "batch 4173: loss 0.056504\n",
      "batch 4174: loss 0.023256\n",
      "batch 4175: loss 0.029413\n",
      "batch 4176: loss 0.105599\n",
      "batch 4177: loss 0.118810\n",
      "batch 4178: loss 0.033942\n",
      "batch 4179: loss 0.040105\n",
      "batch 4180: loss 0.059926\n",
      "batch 4181: loss 0.008274\n",
      "batch 4182: loss 0.024810\n",
      "batch 4183: loss 0.113731\n",
      "batch 4184: loss 0.047582\n",
      "batch 4185: loss 0.034830\n",
      "batch 4186: loss 0.045881\n",
      "batch 4187: loss 0.036429\n",
      "batch 4188: loss 0.012248\n",
      "batch 4189: loss 0.066084\n",
      "batch 4190: loss 0.045134\n",
      "batch 4191: loss 0.374265\n",
      "batch 4192: loss 0.054954\n",
      "batch 4193: loss 0.068806\n",
      "batch 4194: loss 0.031734\n",
      "batch 4195: loss 0.106439\n",
      "batch 4196: loss 0.070709\n",
      "batch 4197: loss 0.059874\n",
      "batch 4198: loss 0.267314\n",
      "batch 4199: loss 0.072863\n",
      "batch 4200: loss 0.066854\n",
      "batch 4201: loss 0.025316\n",
      "batch 4202: loss 0.025879\n",
      "batch 4203: loss 0.037276\n",
      "batch 4204: loss 0.022540\n",
      "batch 4205: loss 0.048459\n",
      "batch 4206: loss 0.119882\n",
      "batch 4207: loss 0.036779\n",
      "batch 4208: loss 0.083171\n",
      "batch 4209: loss 0.047609\n",
      "batch 4210: loss 0.297532\n",
      "batch 4211: loss 0.077920\n",
      "batch 4212: loss 0.270326\n",
      "batch 4213: loss 0.122062\n",
      "batch 4214: loss 0.069095\n",
      "batch 4215: loss 0.047186\n",
      "batch 4216: loss 0.109373\n",
      "batch 4217: loss 0.081050\n",
      "batch 4218: loss 0.023825\n",
      "batch 4219: loss 0.144664\n",
      "batch 4220: loss 0.168865\n",
      "batch 4221: loss 0.044076\n",
      "batch 4222: loss 0.034164\n",
      "batch 4223: loss 0.019639\n",
      "batch 4224: loss 0.059073\n",
      "batch 4225: loss 0.026963\n",
      "batch 4226: loss 0.042647\n",
      "batch 4227: loss 0.061611\n",
      "batch 4228: loss 0.036774\n",
      "batch 4229: loss 0.048108\n",
      "batch 4230: loss 0.210591\n",
      "batch 4231: loss 0.037099\n",
      "batch 4232: loss 0.026696\n",
      "batch 4233: loss 0.088919\n",
      "batch 4234: loss 0.044760\n",
      "batch 4235: loss 0.044480\n",
      "batch 4236: loss 0.034250\n",
      "batch 4237: loss 0.010147\n",
      "batch 4238: loss 0.015827\n",
      "batch 4239: loss 0.112287\n",
      "batch 4240: loss 0.049942\n",
      "batch 4241: loss 0.157823\n",
      "batch 4242: loss 0.018081\n",
      "batch 4243: loss 0.011228\n",
      "batch 4244: loss 0.072078\n",
      "batch 4245: loss 0.004116\n",
      "batch 4246: loss 0.051155\n",
      "batch 4247: loss 0.121518\n",
      "batch 4248: loss 0.096442\n",
      "batch 4249: loss 0.061794\n",
      "batch 4250: loss 0.055136\n",
      "batch 4251: loss 0.020712\n",
      "batch 4252: loss 0.090516\n",
      "batch 4253: loss 0.087016\n",
      "batch 4254: loss 0.052145\n",
      "batch 4255: loss 0.135218\n",
      "batch 4256: loss 0.021647\n",
      "batch 4257: loss 0.087895\n",
      "batch 4258: loss 0.075228\n",
      "batch 4259: loss 0.076192\n",
      "batch 4260: loss 0.071143\n",
      "batch 4261: loss 0.016543\n",
      "batch 4262: loss 0.052904\n",
      "batch 4263: loss 0.099335\n",
      "batch 4264: loss 0.061716\n",
      "batch 4265: loss 0.071841\n",
      "batch 4266: loss 0.022957\n",
      "batch 4267: loss 0.073954\n",
      "batch 4268: loss 0.076155\n",
      "batch 4269: loss 0.143021\n",
      "batch 4270: loss 0.054542\n",
      "batch 4271: loss 0.070662\n",
      "batch 4272: loss 0.071019\n",
      "batch 4273: loss 0.067647\n",
      "batch 4274: loss 0.080960\n",
      "batch 4275: loss 0.032246\n",
      "batch 4276: loss 0.165509\n",
      "batch 4277: loss 0.045300\n",
      "batch 4278: loss 0.180499\n",
      "batch 4279: loss 0.071729\n",
      "batch 4280: loss 0.037281\n",
      "batch 4281: loss 0.059526\n",
      "batch 4282: loss 0.016754\n",
      "batch 4283: loss 0.014614\n",
      "batch 4284: loss 0.054952\n",
      "batch 4285: loss 0.035420\n",
      "batch 4286: loss 0.080903\n",
      "batch 4287: loss 0.149909\n",
      "batch 4288: loss 0.028943\n",
      "batch 4289: loss 0.092468\n",
      "batch 4290: loss 0.098328\n",
      "batch 4291: loss 0.023732\n",
      "batch 4292: loss 0.041733\n",
      "batch 4293: loss 0.034235\n",
      "batch 4294: loss 0.206022\n",
      "batch 4295: loss 0.022927\n",
      "batch 4296: loss 0.092838\n",
      "batch 4297: loss 0.032704\n",
      "batch 4298: loss 0.046091\n",
      "batch 4299: loss 0.041534\n",
      "batch 4300: loss 0.018385\n",
      "batch 4301: loss 0.031972\n",
      "batch 4302: loss 0.076469\n",
      "batch 4303: loss 0.024136\n",
      "batch 4304: loss 0.021743\n",
      "batch 4305: loss 0.144900\n",
      "batch 4306: loss 0.035870\n",
      "batch 4307: loss 0.046681\n",
      "batch 4308: loss 0.029712\n",
      "batch 4309: loss 0.145647\n",
      "batch 4310: loss 0.044116\n",
      "batch 4311: loss 0.112331\n",
      "batch 4312: loss 0.090645\n",
      "batch 4313: loss 0.041802\n",
      "batch 4314: loss 0.012881\n",
      "batch 4315: loss 0.054198\n",
      "batch 4316: loss 0.108961\n",
      "batch 4317: loss 0.129131\n",
      "batch 4318: loss 0.092284\n",
      "batch 4319: loss 0.128903\n",
      "batch 4320: loss 0.023207\n",
      "batch 4321: loss 0.131701\n",
      "batch 4322: loss 0.062057\n",
      "batch 4323: loss 0.075563\n",
      "batch 4324: loss 0.033128\n",
      "batch 4325: loss 0.019807\n",
      "batch 4326: loss 0.121278\n",
      "batch 4327: loss 0.133605\n",
      "batch 4328: loss 0.011932\n",
      "batch 4329: loss 0.037690\n",
      "batch 4330: loss 0.037601\n",
      "batch 4331: loss 0.028314\n",
      "batch 4332: loss 0.059030\n",
      "batch 4333: loss 0.009509\n",
      "batch 4334: loss 0.032117\n",
      "batch 4335: loss 0.030965\n",
      "batch 4336: loss 0.030182\n",
      "batch 4337: loss 0.057420\n",
      "batch 4338: loss 0.120126\n",
      "batch 4339: loss 0.119076\n",
      "batch 4340: loss 0.052348\n",
      "batch 4341: loss 0.020926\n",
      "batch 4342: loss 0.017470\n",
      "batch 4343: loss 0.058797\n",
      "batch 4344: loss 0.057076\n",
      "batch 4345: loss 0.021016\n",
      "batch 4346: loss 0.050643\n",
      "batch 4347: loss 0.113306\n",
      "batch 4348: loss 0.038236\n",
      "batch 4349: loss 0.038331\n",
      "batch 4350: loss 0.037977\n",
      "batch 4351: loss 0.054880\n",
      "batch 4352: loss 0.027747\n",
      "batch 4353: loss 0.186628\n",
      "batch 4354: loss 0.083566\n",
      "batch 4355: loss 0.025826\n",
      "batch 4356: loss 0.078945\n",
      "batch 4357: loss 0.198495\n",
      "batch 4358: loss 0.039631\n",
      "batch 4359: loss 0.072446\n",
      "batch 4360: loss 0.019997\n",
      "batch 4361: loss 0.008138\n",
      "batch 4362: loss 0.055604\n",
      "batch 4363: loss 0.039123\n",
      "batch 4364: loss 0.025234\n",
      "batch 4365: loss 0.057133\n",
      "batch 4366: loss 0.029478\n",
      "batch 4367: loss 0.078845\n",
      "batch 4368: loss 0.044034\n",
      "batch 4369: loss 0.055662\n",
      "batch 4370: loss 0.042941\n",
      "batch 4371: loss 0.054266\n",
      "batch 4372: loss 0.025472\n",
      "batch 4373: loss 0.085193\n",
      "batch 4374: loss 0.115294\n",
      "batch 4375: loss 0.020163\n",
      "batch 4376: loss 0.054552\n",
      "batch 4377: loss 0.106192\n",
      "batch 4378: loss 0.140872\n",
      "batch 4379: loss 0.104467\n",
      "batch 4380: loss 0.012672\n",
      "batch 4381: loss 0.027327\n",
      "batch 4382: loss 0.068771\n",
      "batch 4383: loss 0.055480\n",
      "batch 4384: loss 0.201296\n",
      "batch 4385: loss 0.047273\n",
      "batch 4386: loss 0.090413\n",
      "batch 4387: loss 0.044596\n",
      "batch 4388: loss 0.019438\n",
      "batch 4389: loss 0.039586\n",
      "batch 4390: loss 0.020389\n",
      "batch 4391: loss 0.022606\n",
      "batch 4392: loss 0.026434\n",
      "batch 4393: loss 0.122300\n",
      "batch 4394: loss 0.103003\n",
      "batch 4395: loss 0.074740\n",
      "batch 4396: loss 0.047584\n",
      "batch 4397: loss 0.066744\n",
      "batch 4398: loss 0.242350\n",
      "batch 4399: loss 0.046689\n",
      "batch 4400: loss 0.117520\n",
      "batch 4401: loss 0.090838\n",
      "batch 4402: loss 0.018487\n",
      "batch 4403: loss 0.032261\n",
      "batch 4404: loss 0.084682\n",
      "batch 4405: loss 0.009680\n",
      "batch 4406: loss 0.015906\n",
      "batch 4407: loss 0.037865\n",
      "batch 4408: loss 0.224044\n",
      "batch 4409: loss 0.027459\n",
      "batch 4410: loss 0.017770\n",
      "batch 4411: loss 0.032753\n",
      "batch 4412: loss 0.137025\n",
      "batch 4413: loss 0.037362\n",
      "batch 4414: loss 0.089839\n",
      "batch 4415: loss 0.072685\n",
      "batch 4416: loss 0.032643\n",
      "batch 4417: loss 0.018620\n",
      "batch 4418: loss 0.020284\n",
      "batch 4419: loss 0.035595\n",
      "batch 4420: loss 0.065373\n",
      "batch 4421: loss 0.034733\n",
      "batch 4422: loss 0.076015\n",
      "batch 4423: loss 0.041222\n",
      "batch 4424: loss 0.129472\n",
      "batch 4425: loss 0.091903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4426: loss 0.018541\n",
      "batch 4427: loss 0.028238\n",
      "batch 4428: loss 0.065711\n",
      "batch 4429: loss 0.039669\n",
      "batch 4430: loss 0.034379\n",
      "batch 4431: loss 0.110205\n",
      "batch 4432: loss 0.031028\n",
      "batch 4433: loss 0.034371\n",
      "batch 4434: loss 0.011023\n",
      "batch 4435: loss 0.198900\n",
      "batch 4436: loss 0.025627\n",
      "batch 4437: loss 0.017472\n",
      "batch 4438: loss 0.030237\n",
      "batch 4439: loss 0.044060\n",
      "batch 4440: loss 0.065467\n",
      "batch 4441: loss 0.014894\n",
      "batch 4442: loss 0.028347\n",
      "batch 4443: loss 0.038167\n",
      "batch 4444: loss 0.174668\n",
      "batch 4445: loss 0.101232\n",
      "batch 4446: loss 0.020385\n",
      "batch 4447: loss 0.061920\n",
      "batch 4448: loss 0.029190\n",
      "batch 4449: loss 0.028631\n",
      "batch 4450: loss 0.078042\n",
      "batch 4451: loss 0.030350\n",
      "batch 4452: loss 0.193013\n",
      "batch 4453: loss 0.015187\n",
      "batch 4454: loss 0.091566\n",
      "batch 4455: loss 0.057570\n",
      "batch 4456: loss 0.013041\n",
      "batch 4457: loss 0.304340\n",
      "batch 4458: loss 0.022265\n",
      "batch 4459: loss 0.087147\n",
      "batch 4460: loss 0.056082\n",
      "batch 4461: loss 0.169133\n",
      "batch 4462: loss 0.022505\n",
      "batch 4463: loss 0.019409\n",
      "batch 4464: loss 0.130214\n",
      "batch 4465: loss 0.164540\n",
      "batch 4466: loss 0.054605\n",
      "batch 4467: loss 0.076986\n",
      "batch 4468: loss 0.073551\n",
      "batch 4469: loss 0.047002\n",
      "batch 4470: loss 0.036377\n",
      "batch 4471: loss 0.106649\n",
      "batch 4472: loss 0.024493\n",
      "batch 4473: loss 0.025362\n",
      "batch 4474: loss 0.034721\n",
      "batch 4475: loss 0.088576\n",
      "batch 4476: loss 0.084135\n",
      "batch 4477: loss 0.034947\n",
      "batch 4478: loss 0.047122\n",
      "batch 4479: loss 0.045229\n",
      "batch 4480: loss 0.049249\n",
      "batch 4481: loss 0.017589\n",
      "batch 4482: loss 0.066836\n",
      "batch 4483: loss 0.211574\n",
      "batch 4484: loss 0.019452\n",
      "batch 4485: loss 0.035003\n",
      "batch 4486: loss 0.120370\n",
      "batch 4487: loss 0.071177\n",
      "batch 4488: loss 0.042709\n",
      "batch 4489: loss 0.030904\n",
      "batch 4490: loss 0.141291\n",
      "batch 4491: loss 0.027872\n",
      "batch 4492: loss 0.020929\n",
      "batch 4493: loss 0.049958\n",
      "batch 4494: loss 0.071673\n",
      "batch 4495: loss 0.353152\n",
      "batch 4496: loss 0.086941\n",
      "batch 4497: loss 0.106066\n",
      "batch 4498: loss 0.026959\n",
      "batch 4499: loss 0.032198\n",
      "batch 4500: loss 0.116863\n",
      "batch 4501: loss 0.043491\n",
      "batch 4502: loss 0.109866\n",
      "batch 4503: loss 0.042476\n",
      "batch 4504: loss 0.029843\n",
      "batch 4505: loss 0.070274\n",
      "batch 4506: loss 0.046601\n",
      "batch 4507: loss 0.126850\n",
      "batch 4508: loss 0.120297\n",
      "batch 4509: loss 0.081864\n",
      "batch 4510: loss 0.064761\n",
      "batch 4511: loss 0.030848\n",
      "batch 4512: loss 0.197086\n",
      "batch 4513: loss 0.085003\n",
      "batch 4514: loss 0.055500\n",
      "batch 4515: loss 0.062861\n",
      "batch 4516: loss 0.088717\n",
      "batch 4517: loss 0.022883\n",
      "batch 4518: loss 0.040211\n",
      "batch 4519: loss 0.032922\n",
      "batch 4520: loss 0.133392\n",
      "batch 4521: loss 0.128520\n",
      "batch 4522: loss 0.043046\n",
      "batch 4523: loss 0.082598\n",
      "batch 4524: loss 0.253237\n",
      "batch 4525: loss 0.024713\n",
      "batch 4526: loss 0.012813\n",
      "batch 4527: loss 0.018937\n",
      "batch 4528: loss 0.093383\n",
      "batch 4529: loss 0.018685\n",
      "batch 4530: loss 0.009323\n",
      "batch 4531: loss 0.186692\n",
      "batch 4532: loss 0.032534\n",
      "batch 4533: loss 0.015987\n",
      "batch 4534: loss 0.087442\n",
      "batch 4535: loss 0.049594\n",
      "batch 4536: loss 0.035432\n",
      "batch 4537: loss 0.025648\n",
      "batch 4538: loss 0.037808\n",
      "batch 4539: loss 0.181498\n",
      "batch 4540: loss 0.048601\n",
      "batch 4541: loss 0.098556\n",
      "batch 4542: loss 0.023499\n",
      "batch 4543: loss 0.025317\n",
      "batch 4544: loss 0.063207\n",
      "batch 4545: loss 0.038853\n",
      "batch 4546: loss 0.019926\n",
      "batch 4547: loss 0.017393\n",
      "batch 4548: loss 0.010709\n",
      "batch 4549: loss 0.010014\n",
      "batch 4550: loss 0.043185\n",
      "batch 4551: loss 0.007180\n",
      "batch 4552: loss 0.017567\n",
      "batch 4553: loss 0.098432\n",
      "batch 4554: loss 0.031140\n",
      "batch 4555: loss 0.015177\n",
      "batch 4556: loss 0.155587\n",
      "batch 4557: loss 0.034207\n",
      "batch 4558: loss 0.017015\n",
      "batch 4559: loss 0.061238\n",
      "batch 4560: loss 0.028750\n",
      "batch 4561: loss 0.036340\n",
      "batch 4562: loss 0.019572\n",
      "batch 4563: loss 0.045043\n",
      "batch 4564: loss 0.065109\n",
      "batch 4565: loss 0.033454\n",
      "batch 4566: loss 0.012716\n",
      "batch 4567: loss 0.036096\n",
      "batch 4568: loss 0.129867\n",
      "batch 4569: loss 0.057011\n",
      "batch 4570: loss 0.015348\n",
      "batch 4571: loss 0.027566\n",
      "batch 4572: loss 0.021737\n",
      "batch 4573: loss 0.009992\n",
      "batch 4574: loss 0.110052\n",
      "batch 4575: loss 0.029415\n",
      "batch 4576: loss 0.130296\n",
      "batch 4577: loss 0.029308\n",
      "batch 4578: loss 0.024344\n",
      "batch 4579: loss 0.120520\n",
      "batch 4580: loss 0.015481\n",
      "batch 4581: loss 0.025873\n",
      "batch 4582: loss 0.023865\n",
      "batch 4583: loss 0.036631\n",
      "batch 4584: loss 0.278174\n",
      "batch 4585: loss 0.030513\n",
      "batch 4586: loss 0.017761\n",
      "batch 4587: loss 0.060280\n",
      "batch 4588: loss 0.049446\n",
      "batch 4589: loss 0.012765\n",
      "batch 4590: loss 0.051713\n",
      "batch 4591: loss 0.044910\n",
      "batch 4592: loss 0.107008\n",
      "batch 4593: loss 0.090429\n",
      "batch 4594: loss 0.018763\n",
      "batch 4595: loss 0.032502\n",
      "batch 4596: loss 0.021924\n",
      "batch 4597: loss 0.022547\n",
      "batch 4598: loss 0.013199\n",
      "batch 4599: loss 0.046305\n",
      "batch 4600: loss 0.039522\n",
      "batch 4601: loss 0.164339\n",
      "batch 4602: loss 0.017851\n",
      "batch 4603: loss 0.125798\n",
      "batch 4604: loss 0.036258\n",
      "batch 4605: loss 0.142131\n",
      "batch 4606: loss 0.069053\n",
      "batch 4607: loss 0.009847\n",
      "batch 4608: loss 0.090078\n",
      "batch 4609: loss 0.012799\n",
      "batch 4610: loss 0.075494\n",
      "batch 4611: loss 0.015193\n",
      "batch 4612: loss 0.042891\n",
      "batch 4613: loss 0.024774\n",
      "batch 4614: loss 0.042927\n",
      "batch 4615: loss 0.175141\n",
      "batch 4616: loss 0.045837\n",
      "batch 4617: loss 0.110338\n",
      "batch 4618: loss 0.015400\n",
      "batch 4619: loss 0.020601\n",
      "batch 4620: loss 0.023570\n",
      "batch 4621: loss 0.049102\n",
      "batch 4622: loss 0.068155\n",
      "batch 4623: loss 0.021165\n",
      "batch 4624: loss 0.020497\n",
      "batch 4625: loss 0.038016\n",
      "batch 4626: loss 0.028637\n",
      "batch 4627: loss 0.020324\n",
      "batch 4628: loss 0.199724\n",
      "batch 4629: loss 0.027731\n",
      "batch 4630: loss 0.111779\n",
      "batch 4631: loss 0.023658\n",
      "batch 4632: loss 0.054159\n",
      "batch 4633: loss 0.072222\n",
      "batch 4634: loss 0.025949\n",
      "batch 4635: loss 0.018489\n",
      "batch 4636: loss 0.088292\n",
      "batch 4637: loss 0.080811\n",
      "batch 4638: loss 0.013110\n",
      "batch 4639: loss 0.033463\n",
      "batch 4640: loss 0.098303\n",
      "batch 4641: loss 0.015353\n",
      "batch 4642: loss 0.081393\n",
      "batch 4643: loss 0.103225\n",
      "batch 4644: loss 0.009512\n",
      "batch 4645: loss 0.009482\n",
      "batch 4646: loss 0.039358\n",
      "batch 4647: loss 0.103396\n",
      "batch 4648: loss 0.058461\n",
      "batch 4649: loss 0.125883\n",
      "batch 4650: loss 0.032293\n",
      "batch 4651: loss 0.105769\n",
      "batch 4652: loss 0.010760\n",
      "batch 4653: loss 0.023362\n",
      "batch 4654: loss 0.099548\n",
      "batch 4655: loss 0.068465\n",
      "batch 4656: loss 0.072535\n",
      "batch 4657: loss 0.069293\n",
      "batch 4658: loss 0.007717\n",
      "batch 4659: loss 0.025217\n",
      "batch 4660: loss 0.021848\n",
      "batch 4661: loss 0.029010\n",
      "batch 4662: loss 0.071232\n",
      "batch 4663: loss 0.042030\n",
      "batch 4664: loss 0.117433\n",
      "batch 4665: loss 0.054536\n",
      "batch 4666: loss 0.064130\n",
      "batch 4667: loss 0.027003\n",
      "batch 4668: loss 0.050284\n",
      "batch 4669: loss 0.037464\n",
      "batch 4670: loss 0.072405\n",
      "batch 4671: loss 0.049858\n",
      "batch 4672: loss 0.130133\n",
      "batch 4673: loss 0.045389\n",
      "batch 4674: loss 0.086101\n",
      "batch 4675: loss 0.072426\n",
      "batch 4676: loss 0.018439\n",
      "batch 4677: loss 0.110573\n",
      "batch 4678: loss 0.006972\n",
      "batch 4679: loss 0.096040\n",
      "batch 4680: loss 0.040248\n",
      "batch 4681: loss 0.015618\n",
      "batch 4682: loss 0.009258\n",
      "batch 4683: loss 0.034647\n",
      "batch 4684: loss 0.063126\n",
      "batch 4685: loss 0.035983\n",
      "batch 4686: loss 0.047864\n",
      "batch 4687: loss 0.028067\n",
      "batch 4688: loss 0.095049\n",
      "batch 4689: loss 0.022930\n",
      "batch 4690: loss 0.030762\n",
      "batch 4691: loss 0.069280\n",
      "batch 4692: loss 0.024913\n",
      "batch 4693: loss 0.022868\n",
      "batch 4694: loss 0.124709\n",
      "batch 4695: loss 0.012899\n",
      "batch 4696: loss 0.015540\n",
      "batch 4697: loss 0.102027\n",
      "batch 4698: loss 0.096610\n",
      "batch 4699: loss 0.086034\n",
      "batch 4700: loss 0.038322\n",
      "batch 4701: loss 0.070757\n",
      "batch 4702: loss 0.078947\n",
      "batch 4703: loss 0.196997\n",
      "batch 4704: loss 0.017216\n",
      "batch 4705: loss 0.018960\n",
      "batch 4706: loss 0.023590\n",
      "batch 4707: loss 0.220240\n",
      "batch 4708: loss 0.058306\n",
      "batch 4709: loss 0.087501\n",
      "batch 4710: loss 0.029409\n",
      "batch 4711: loss 0.064982\n",
      "batch 4712: loss 0.034733\n",
      "batch 4713: loss 0.023402\n",
      "batch 4714: loss 0.021239\n",
      "batch 4715: loss 0.060890\n",
      "batch 4716: loss 0.064931\n",
      "batch 4717: loss 0.016387\n",
      "batch 4718: loss 0.010701\n",
      "batch 4719: loss 0.051881\n",
      "batch 4720: loss 0.070491\n",
      "batch 4721: loss 0.058519\n",
      "batch 4722: loss 0.081125\n",
      "batch 4723: loss 0.051329\n",
      "batch 4724: loss 0.041753\n",
      "batch 4725: loss 0.021234\n",
      "batch 4726: loss 0.087641\n",
      "batch 4727: loss 0.055260\n",
      "batch 4728: loss 0.071950\n",
      "batch 4729: loss 0.017517\n",
      "batch 4730: loss 0.013537\n",
      "batch 4731: loss 0.033357\n",
      "batch 4732: loss 0.031811\n",
      "batch 4733: loss 0.068398\n",
      "batch 4734: loss 0.082799\n",
      "batch 4735: loss 0.019850\n",
      "batch 4736: loss 0.049251\n",
      "batch 4737: loss 0.086203\n",
      "batch 4738: loss 0.193753\n",
      "batch 4739: loss 0.021027\n",
      "batch 4740: loss 0.058394\n",
      "batch 4741: loss 0.052541\n",
      "batch 4742: loss 0.075498\n",
      "batch 4743: loss 0.086609\n",
      "batch 4744: loss 0.038426\n",
      "batch 4745: loss 0.128336\n",
      "batch 4746: loss 0.035105\n",
      "batch 4747: loss 0.068552\n",
      "batch 4748: loss 0.138301\n",
      "batch 4749: loss 0.070543\n",
      "batch 4750: loss 0.072134\n",
      "batch 4751: loss 0.012651\n",
      "batch 4752: loss 0.010663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4753: loss 0.020653\n",
      "batch 4754: loss 0.083181\n",
      "batch 4755: loss 0.010909\n",
      "batch 4756: loss 0.043500\n",
      "batch 4757: loss 0.066022\n",
      "batch 4758: loss 0.022486\n",
      "batch 4759: loss 0.267442\n",
      "batch 4760: loss 0.102723\n",
      "batch 4761: loss 0.145377\n",
      "batch 4762: loss 0.053585\n",
      "batch 4763: loss 0.029102\n",
      "batch 4764: loss 0.090092\n",
      "batch 4765: loss 0.049719\n",
      "batch 4766: loss 0.022748\n",
      "batch 4767: loss 0.039745\n",
      "batch 4768: loss 0.059553\n",
      "batch 4769: loss 0.025978\n",
      "batch 4770: loss 0.023191\n",
      "batch 4771: loss 0.079260\n",
      "batch 4772: loss 0.062416\n",
      "batch 4773: loss 0.012795\n",
      "batch 4774: loss 0.018660\n",
      "batch 4775: loss 0.311953\n",
      "batch 4776: loss 0.056313\n",
      "batch 4777: loss 0.032010\n",
      "batch 4778: loss 0.081163\n",
      "batch 4779: loss 0.092005\n",
      "batch 4780: loss 0.018106\n",
      "batch 4781: loss 0.011809\n",
      "batch 4782: loss 0.044611\n",
      "batch 4783: loss 0.083163\n",
      "batch 4784: loss 0.008340\n",
      "batch 4785: loss 0.083408\n",
      "batch 4786: loss 0.126669\n",
      "batch 4787: loss 0.012945\n",
      "batch 4788: loss 0.052477\n",
      "batch 4789: loss 0.031824\n",
      "batch 4790: loss 0.011815\n",
      "batch 4791: loss 0.111811\n",
      "batch 4792: loss 0.014766\n",
      "batch 4793: loss 0.080081\n",
      "batch 4794: loss 0.065081\n",
      "batch 4795: loss 0.005535\n",
      "batch 4796: loss 0.013805\n",
      "batch 4797: loss 0.177538\n",
      "batch 4798: loss 0.011560\n",
      "batch 4799: loss 0.064851\n",
      "batch 4800: loss 0.098013\n",
      "batch 4801: loss 0.184246\n",
      "batch 4802: loss 0.046821\n",
      "batch 4803: loss 0.112678\n",
      "batch 4804: loss 0.033466\n",
      "batch 4805: loss 0.111908\n",
      "batch 4806: loss 0.017878\n",
      "batch 4807: loss 0.008040\n",
      "batch 4808: loss 0.027447\n",
      "batch 4809: loss 0.260707\n",
      "batch 4810: loss 0.032785\n",
      "batch 4811: loss 0.089212\n",
      "batch 4812: loss 0.078091\n",
      "batch 4813: loss 0.188806\n",
      "batch 4814: loss 0.108730\n",
      "batch 4815: loss 0.034626\n",
      "batch 4816: loss 0.121930\n",
      "batch 4817: loss 0.070509\n",
      "batch 4818: loss 0.083824\n",
      "batch 4819: loss 0.039651\n",
      "batch 4820: loss 0.152731\n",
      "batch 4821: loss 0.049347\n",
      "batch 4822: loss 0.192940\n",
      "batch 4823: loss 0.129111\n",
      "batch 4824: loss 0.022421\n",
      "batch 4825: loss 0.056464\n",
      "batch 4826: loss 0.116731\n",
      "batch 4827: loss 0.020311\n",
      "batch 4828: loss 0.025804\n",
      "batch 4829: loss 0.049518\n",
      "batch 4830: loss 0.030443\n",
      "batch 4831: loss 0.020434\n",
      "batch 4832: loss 0.027843\n",
      "batch 4833: loss 0.040307\n",
      "batch 4834: loss 0.066554\n",
      "batch 4835: loss 0.023637\n",
      "batch 4836: loss 0.043901\n",
      "batch 4837: loss 0.062349\n",
      "batch 4838: loss 0.020351\n",
      "batch 4839: loss 0.025852\n",
      "batch 4840: loss 0.026459\n",
      "batch 4841: loss 0.042198\n",
      "batch 4842: loss 0.019319\n",
      "batch 4843: loss 0.032167\n",
      "batch 4844: loss 0.068358\n",
      "batch 4845: loss 0.025427\n",
      "batch 4846: loss 0.024654\n",
      "batch 4847: loss 0.086406\n",
      "batch 4848: loss 0.039133\n",
      "batch 4849: loss 0.076274\n",
      "batch 4850: loss 0.026079\n",
      "batch 4851: loss 0.089706\n",
      "batch 4852: loss 0.026583\n",
      "batch 4853: loss 0.054695\n",
      "batch 4854: loss 0.183019\n",
      "batch 4855: loss 0.122460\n",
      "batch 4856: loss 0.016175\n",
      "batch 4857: loss 0.052825\n",
      "batch 4858: loss 0.021379\n",
      "batch 4859: loss 0.044664\n",
      "batch 4860: loss 0.063690\n",
      "batch 4861: loss 0.044583\n",
      "batch 4862: loss 0.032366\n",
      "batch 4863: loss 0.053626\n",
      "batch 4864: loss 0.029753\n",
      "batch 4865: loss 0.060787\n",
      "batch 4866: loss 0.213124\n",
      "batch 4867: loss 0.036795\n",
      "batch 4868: loss 0.011612\n",
      "batch 4869: loss 0.036693\n",
      "batch 4870: loss 0.104912\n",
      "batch 4871: loss 0.089148\n",
      "batch 4872: loss 0.043330\n",
      "batch 4873: loss 0.045424\n",
      "batch 4874: loss 0.049003\n",
      "batch 4875: loss 0.081376\n",
      "batch 4876: loss 0.014025\n",
      "batch 4877: loss 0.026170\n",
      "batch 4878: loss 0.017624\n",
      "batch 4879: loss 0.075692\n",
      "batch 4880: loss 0.048751\n",
      "batch 4881: loss 0.015619\n",
      "batch 4882: loss 0.105455\n",
      "batch 4883: loss 0.043759\n",
      "batch 4884: loss 0.035360\n",
      "batch 4885: loss 0.081400\n",
      "batch 4886: loss 0.034195\n",
      "batch 4887: loss 0.041577\n",
      "batch 4888: loss 0.078678\n",
      "batch 4889: loss 0.009757\n",
      "batch 4890: loss 0.094440\n",
      "batch 4891: loss 0.078633\n",
      "batch 4892: loss 0.067606\n",
      "batch 4893: loss 0.036267\n",
      "batch 4894: loss 0.171418\n",
      "batch 4895: loss 0.033580\n",
      "batch 4896: loss 0.017285\n",
      "batch 4897: loss 0.071218\n",
      "batch 4898: loss 0.054625\n",
      "batch 4899: loss 0.044714\n",
      "batch 4900: loss 0.097736\n",
      "batch 4901: loss 0.141629\n",
      "batch 4902: loss 0.214370\n",
      "batch 4903: loss 0.111616\n",
      "batch 4904: loss 0.043394\n",
      "batch 4905: loss 0.033280\n",
      "batch 4906: loss 0.033378\n",
      "batch 4907: loss 0.040461\n",
      "batch 4908: loss 0.061599\n",
      "batch 4909: loss 0.056457\n",
      "batch 4910: loss 0.048355\n",
      "batch 4911: loss 0.018768\n",
      "batch 4912: loss 0.019366\n",
      "batch 4913: loss 0.029093\n",
      "batch 4914: loss 0.048805\n",
      "batch 4915: loss 0.031705\n",
      "batch 4916: loss 0.103446\n",
      "batch 4917: loss 0.114648\n",
      "batch 4918: loss 0.017962\n",
      "batch 4919: loss 0.028246\n",
      "batch 4920: loss 0.057705\n",
      "batch 4921: loss 0.008023\n",
      "batch 4922: loss 0.045280\n",
      "batch 4923: loss 0.065594\n",
      "batch 4924: loss 0.020993\n",
      "batch 4925: loss 0.198240\n",
      "batch 4926: loss 0.027588\n",
      "batch 4927: loss 0.040567\n",
      "batch 4928: loss 0.077607\n",
      "batch 4929: loss 0.008290\n",
      "batch 4930: loss 0.037298\n",
      "batch 4931: loss 0.280752\n",
      "batch 4932: loss 0.047753\n",
      "batch 4933: loss 0.076058\n",
      "batch 4934: loss 0.089667\n",
      "batch 4935: loss 0.051489\n",
      "batch 4936: loss 0.046139\n",
      "batch 4937: loss 0.088248\n",
      "batch 4938: loss 0.050510\n",
      "batch 4939: loss 0.008830\n",
      "batch 4940: loss 0.040484\n",
      "batch 4941: loss 0.131521\n",
      "batch 4942: loss 0.128814\n",
      "batch 4943: loss 0.045504\n",
      "batch 4944: loss 0.081603\n",
      "batch 4945: loss 0.048637\n",
      "batch 4946: loss 0.029362\n",
      "batch 4947: loss 0.052689\n",
      "batch 4948: loss 0.058513\n",
      "batch 4949: loss 0.066657\n",
      "batch 4950: loss 0.028234\n",
      "batch 4951: loss 0.076674\n",
      "batch 4952: loss 0.045192\n",
      "batch 4953: loss 0.025044\n",
      "batch 4954: loss 0.044954\n",
      "batch 4955: loss 0.173230\n",
      "batch 4956: loss 0.030176\n",
      "batch 4957: loss 0.030276\n",
      "batch 4958: loss 0.030082\n",
      "batch 4959: loss 0.029230\n",
      "batch 4960: loss 0.072823\n",
      "batch 4961: loss 0.020853\n",
      "batch 4962: loss 0.019981\n",
      "batch 4963: loss 0.024124\n",
      "batch 4964: loss 0.054209\n",
      "batch 4965: loss 0.008267\n",
      "batch 4966: loss 0.032156\n",
      "batch 4967: loss 0.029470\n",
      "batch 4968: loss 0.011987\n",
      "batch 4969: loss 0.018855\n",
      "batch 4970: loss 0.035194\n",
      "batch 4971: loss 0.033847\n",
      "batch 4972: loss 0.058905\n",
      "batch 4973: loss 0.110222\n",
      "batch 4974: loss 0.043827\n",
      "batch 4975: loss 0.047159\n",
      "batch 4976: loss 0.033065\n",
      "batch 4977: loss 0.024297\n",
      "batch 4978: loss 0.121741\n",
      "batch 4979: loss 0.043998\n",
      "batch 4980: loss 0.040308\n",
      "batch 4981: loss 0.125982\n",
      "batch 4982: loss 0.048251\n",
      "batch 4983: loss 0.091841\n",
      "batch 4984: loss 0.072201\n",
      "batch 4985: loss 0.051154\n",
      "batch 4986: loss 0.025110\n",
      "batch 4987: loss 0.057727\n",
      "batch 4988: loss 0.019964\n",
      "batch 4989: loss 0.020486\n",
      "batch 4990: loss 0.041633\n",
      "batch 4991: loss 0.099192\n",
      "batch 4992: loss 0.067547\n",
      "batch 4993: loss 0.024060\n",
      "batch 4994: loss 0.009667\n",
      "batch 4995: loss 0.016938\n",
      "batch 4996: loss 0.083431\n",
      "batch 4997: loss 0.075232\n",
      "batch 4998: loss 0.070370\n",
      "batch 4999: loss 0.030610\n",
      "batch 5000: loss 0.039052\n",
      "batch 5001: loss 0.081586\n",
      "batch 5002: loss 0.013546\n",
      "batch 5003: loss 0.011925\n",
      "batch 5004: loss 0.086574\n",
      "batch 5005: loss 0.021632\n",
      "batch 5006: loss 0.247521\n",
      "batch 5007: loss 0.222948\n",
      "batch 5008: loss 0.176938\n",
      "batch 5009: loss 0.031411\n",
      "batch 5010: loss 0.018836\n",
      "batch 5011: loss 0.008600\n",
      "batch 5012: loss 0.025205\n",
      "batch 5013: loss 0.039424\n",
      "batch 5014: loss 0.092341\n",
      "batch 5015: loss 0.054414\n",
      "batch 5016: loss 0.165151\n",
      "batch 5017: loss 0.050632\n",
      "batch 5018: loss 0.061856\n",
      "batch 5019: loss 0.063508\n",
      "batch 5020: loss 0.102771\n",
      "batch 5021: loss 0.041045\n",
      "batch 5022: loss 0.008243\n",
      "batch 5023: loss 0.065576\n",
      "batch 5024: loss 0.081659\n",
      "batch 5025: loss 0.134603\n",
      "batch 5026: loss 0.052254\n",
      "batch 5027: loss 0.210862\n",
      "batch 5028: loss 0.053927\n",
      "batch 5029: loss 0.019766\n",
      "batch 5030: loss 0.014426\n",
      "batch 5031: loss 0.011879\n",
      "batch 5032: loss 0.041630\n",
      "batch 5033: loss 0.205024\n",
      "batch 5034: loss 0.062184\n",
      "batch 5035: loss 0.023094\n",
      "batch 5036: loss 0.022901\n",
      "batch 5037: loss 0.043874\n",
      "batch 5038: loss 0.072399\n",
      "batch 5039: loss 0.067309\n",
      "batch 5040: loss 0.155935\n",
      "batch 5041: loss 0.051092\n",
      "batch 5042: loss 0.073382\n",
      "batch 5043: loss 0.090475\n",
      "batch 5044: loss 0.104390\n",
      "batch 5045: loss 0.017206\n",
      "batch 5046: loss 0.013962\n",
      "batch 5047: loss 0.122756\n",
      "batch 5048: loss 0.223362\n",
      "batch 5049: loss 0.082122\n",
      "batch 5050: loss 0.038323\n",
      "batch 5051: loss 0.024777\n",
      "batch 5052: loss 0.014691\n",
      "batch 5053: loss 0.028548\n",
      "batch 5054: loss 0.022261\n",
      "batch 5055: loss 0.045426\n",
      "batch 5056: loss 0.096135\n",
      "batch 5057: loss 0.015175\n",
      "batch 5058: loss 0.103701\n",
      "batch 5059: loss 0.043812\n",
      "batch 5060: loss 0.012346\n",
      "batch 5061: loss 0.022414\n",
      "batch 5062: loss 0.023861\n",
      "batch 5063: loss 0.028279\n",
      "batch 5064: loss 0.018556\n",
      "batch 5065: loss 0.009163\n",
      "batch 5066: loss 0.016067\n",
      "batch 5067: loss 0.032430\n",
      "batch 5068: loss 0.021066\n",
      "batch 5069: loss 0.033953\n",
      "batch 5070: loss 0.021553\n",
      "batch 5071: loss 0.051089\n",
      "batch 5072: loss 0.205102\n",
      "batch 5073: loss 0.078810\n",
      "batch 5074: loss 0.081701\n",
      "batch 5075: loss 0.012844\n",
      "batch 5076: loss 0.163992\n",
      "batch 5077: loss 0.036344\n",
      "batch 5078: loss 0.056509\n",
      "batch 5079: loss 0.056410\n",
      "batch 5080: loss 0.017058\n",
      "batch 5081: loss 0.044882\n",
      "batch 5082: loss 0.018113\n",
      "batch 5083: loss 0.165324\n",
      "batch 5084: loss 0.042398\n",
      "batch 5085: loss 0.037268\n",
      "batch 5086: loss 0.014418\n",
      "batch 5087: loss 0.032006\n",
      "batch 5088: loss 0.029308\n",
      "batch 5089: loss 0.064571\n",
      "batch 5090: loss 0.056720\n",
      "batch 5091: loss 0.025069\n",
      "batch 5092: loss 0.148724\n",
      "batch 5093: loss 0.032188\n",
      "batch 5094: loss 0.046112\n",
      "batch 5095: loss 0.032518\n",
      "batch 5096: loss 0.013757\n",
      "batch 5097: loss 0.008436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5098: loss 0.068797\n",
      "batch 5099: loss 0.044556\n",
      "batch 5100: loss 0.150175\n",
      "batch 5101: loss 0.033028\n",
      "batch 5102: loss 0.057800\n",
      "batch 5103: loss 0.040591\n",
      "batch 5104: loss 0.033216\n",
      "batch 5105: loss 0.064875\n",
      "batch 5106: loss 0.023333\n",
      "batch 5107: loss 0.024097\n",
      "batch 5108: loss 0.041645\n",
      "batch 5109: loss 0.015214\n",
      "batch 5110: loss 0.023347\n",
      "batch 5111: loss 0.015303\n",
      "batch 5112: loss 0.040533\n",
      "batch 5113: loss 0.073789\n",
      "batch 5114: loss 0.027309\n",
      "batch 5115: loss 0.028070\n",
      "batch 5116: loss 0.178180\n",
      "batch 5117: loss 0.019393\n",
      "batch 5118: loss 0.016328\n",
      "batch 5119: loss 0.026753\n",
      "batch 5120: loss 0.016176\n",
      "batch 5121: loss 0.019594\n",
      "batch 5122: loss 0.086949\n",
      "batch 5123: loss 0.014113\n",
      "batch 5124: loss 0.106383\n",
      "batch 5125: loss 0.024899\n",
      "batch 5126: loss 0.063764\n",
      "batch 5127: loss 0.015309\n",
      "batch 5128: loss 0.020666\n",
      "batch 5129: loss 0.051349\n",
      "batch 5130: loss 0.006751\n",
      "batch 5131: loss 0.142402\n",
      "batch 5132: loss 0.032188\n",
      "batch 5133: loss 0.022839\n",
      "batch 5134: loss 0.029800\n",
      "batch 5135: loss 0.070172\n",
      "batch 5136: loss 0.147172\n",
      "batch 5137: loss 0.068820\n",
      "batch 5138: loss 0.008406\n",
      "batch 5139: loss 0.238786\n",
      "batch 5140: loss 0.013119\n",
      "batch 5141: loss 0.064997\n",
      "batch 5142: loss 0.048706\n",
      "batch 5143: loss 0.053171\n",
      "batch 5144: loss 0.065500\n",
      "batch 5145: loss 0.126463\n",
      "batch 5146: loss 0.010615\n",
      "batch 5147: loss 0.076846\n",
      "batch 5148: loss 0.016979\n",
      "batch 5149: loss 0.098638\n",
      "batch 5150: loss 0.083929\n",
      "batch 5151: loss 0.093926\n",
      "batch 5152: loss 0.012399\n",
      "batch 5153: loss 0.010332\n",
      "batch 5154: loss 0.093045\n",
      "batch 5155: loss 0.056517\n",
      "batch 5156: loss 0.030289\n",
      "batch 5157: loss 0.017887\n",
      "batch 5158: loss 0.040998\n",
      "batch 5159: loss 0.022752\n",
      "batch 5160: loss 0.091926\n",
      "batch 5161: loss 0.006452\n",
      "batch 5162: loss 0.008130\n",
      "batch 5163: loss 0.039825\n",
      "batch 5164: loss 0.018124\n",
      "batch 5165: loss 0.048368\n",
      "batch 5166: loss 0.030624\n",
      "batch 5167: loss 0.022555\n",
      "batch 5168: loss 0.040804\n",
      "batch 5169: loss 0.061258\n",
      "batch 5170: loss 0.034694\n",
      "batch 5171: loss 0.026589\n",
      "batch 5172: loss 0.018564\n",
      "batch 5173: loss 0.025846\n",
      "batch 5174: loss 0.021216\n",
      "batch 5175: loss 0.036524\n",
      "batch 5176: loss 0.047807\n",
      "batch 5177: loss 0.029484\n",
      "batch 5178: loss 0.034246\n",
      "batch 5179: loss 0.066987\n",
      "batch 5180: loss 0.070275\n",
      "batch 5181: loss 0.073547\n",
      "batch 5182: loss 0.094254\n",
      "batch 5183: loss 0.055507\n",
      "batch 5184: loss 0.011136\n",
      "batch 5185: loss 0.025964\n",
      "batch 5186: loss 0.024444\n",
      "batch 5187: loss 0.151001\n",
      "batch 5188: loss 0.072291\n",
      "batch 5189: loss 0.076515\n",
      "batch 5190: loss 0.025103\n",
      "batch 5191: loss 0.142343\n",
      "batch 5192: loss 0.019054\n",
      "batch 5193: loss 0.046026\n",
      "batch 5194: loss 0.088375\n",
      "batch 5195: loss 0.025711\n",
      "batch 5196: loss 0.020247\n",
      "batch 5197: loss 0.047042\n",
      "batch 5198: loss 0.060678\n",
      "batch 5199: loss 0.083152\n",
      "batch 5200: loss 0.028506\n",
      "batch 5201: loss 0.059768\n",
      "batch 5202: loss 0.015389\n",
      "batch 5203: loss 0.068315\n",
      "batch 5204: loss 0.153949\n",
      "batch 5205: loss 0.037582\n",
      "batch 5206: loss 0.030150\n",
      "batch 5207: loss 0.046730\n",
      "batch 5208: loss 0.113101\n",
      "batch 5209: loss 0.103541\n",
      "batch 5210: loss 0.037110\n",
      "batch 5211: loss 0.023370\n",
      "batch 5212: loss 0.063180\n",
      "batch 5213: loss 0.022336\n",
      "batch 5214: loss 0.039941\n",
      "batch 5215: loss 0.037284\n",
      "batch 5216: loss 0.030281\n",
      "batch 5217: loss 0.044344\n",
      "batch 5218: loss 0.085505\n",
      "batch 5219: loss 0.083812\n",
      "batch 5220: loss 0.053565\n",
      "batch 5221: loss 0.040835\n",
      "batch 5222: loss 0.086450\n",
      "batch 5223: loss 0.033447\n",
      "batch 5224: loss 0.022344\n",
      "batch 5225: loss 0.050341\n",
      "batch 5226: loss 0.024342\n",
      "batch 5227: loss 0.010698\n",
      "batch 5228: loss 0.073565\n",
      "batch 5229: loss 0.038582\n",
      "batch 5230: loss 0.101615\n",
      "batch 5231: loss 0.071531\n",
      "batch 5232: loss 0.035674\n",
      "batch 5233: loss 0.030301\n",
      "batch 5234: loss 0.025975\n",
      "batch 5235: loss 0.027906\n",
      "batch 5236: loss 0.022945\n",
      "batch 5237: loss 0.036877\n",
      "batch 5238: loss 0.029472\n",
      "batch 5239: loss 0.034224\n",
      "batch 5240: loss 0.021661\n",
      "batch 5241: loss 0.070714\n",
      "batch 5242: loss 0.033516\n",
      "batch 5243: loss 0.010124\n",
      "batch 5244: loss 0.040798\n",
      "batch 5245: loss 0.029082\n",
      "batch 5246: loss 0.010420\n",
      "batch 5247: loss 0.049235\n",
      "batch 5248: loss 0.042784\n",
      "batch 5249: loss 0.086119\n",
      "batch 5250: loss 0.018717\n",
      "batch 5251: loss 0.017512\n",
      "batch 5252: loss 0.010639\n",
      "batch 5253: loss 0.136295\n",
      "batch 5254: loss 0.116075\n",
      "batch 5255: loss 0.021133\n",
      "batch 5256: loss 0.036483\n",
      "batch 5257: loss 0.116188\n",
      "batch 5258: loss 0.010696\n",
      "batch 5259: loss 0.006405\n",
      "batch 5260: loss 0.009466\n",
      "batch 5261: loss 0.012349\n",
      "batch 5262: loss 0.042646\n",
      "batch 5263: loss 0.092741\n",
      "batch 5264: loss 0.120314\n",
      "batch 5265: loss 0.149911\n",
      "batch 5266: loss 0.050753\n",
      "batch 5267: loss 0.098095\n",
      "batch 5268: loss 0.102377\n",
      "batch 5269: loss 0.027187\n",
      "batch 5270: loss 0.130113\n",
      "batch 5271: loss 0.009153\n",
      "batch 5272: loss 0.035377\n",
      "batch 5273: loss 0.182388\n",
      "batch 5274: loss 0.051111\n",
      "batch 5275: loss 0.006398\n",
      "batch 5276: loss 0.016939\n",
      "batch 5277: loss 0.039402\n",
      "batch 5278: loss 0.110570\n",
      "batch 5279: loss 0.165229\n",
      "batch 5280: loss 0.023092\n",
      "batch 5281: loss 0.117990\n",
      "batch 5282: loss 0.137791\n",
      "batch 5283: loss 0.119041\n",
      "batch 5284: loss 0.028353\n",
      "batch 5285: loss 0.054652\n",
      "batch 5286: loss 0.076650\n",
      "batch 5287: loss 0.191443\n",
      "batch 5288: loss 0.084194\n",
      "batch 5289: loss 0.145631\n",
      "batch 5290: loss 0.005269\n",
      "batch 5291: loss 0.072057\n",
      "batch 5292: loss 0.023363\n",
      "batch 5293: loss 0.122510\n",
      "batch 5294: loss 0.014229\n",
      "batch 5295: loss 0.020642\n",
      "batch 5296: loss 0.018270\n",
      "batch 5297: loss 0.055661\n",
      "batch 5298: loss 0.073868\n",
      "batch 5299: loss 0.016278\n",
      "batch 5300: loss 0.063052\n",
      "batch 5301: loss 0.135050\n",
      "batch 5302: loss 0.010943\n",
      "batch 5303: loss 0.050283\n",
      "batch 5304: loss 0.026025\n",
      "batch 5305: loss 0.042987\n",
      "batch 5306: loss 0.021729\n",
      "batch 5307: loss 0.078594\n",
      "batch 5308: loss 0.031997\n",
      "batch 5309: loss 0.040215\n",
      "batch 5310: loss 0.048403\n",
      "batch 5311: loss 0.065996\n",
      "batch 5312: loss 0.043742\n",
      "batch 5313: loss 0.008876\n",
      "batch 5314: loss 0.017719\n",
      "batch 5315: loss 0.028237\n",
      "batch 5316: loss 0.089706\n",
      "batch 5317: loss 0.100472\n",
      "batch 5318: loss 0.057022\n",
      "batch 5319: loss 0.103165\n",
      "batch 5320: loss 0.149431\n",
      "batch 5321: loss 0.006926\n",
      "batch 5322: loss 0.044366\n",
      "batch 5323: loss 0.050083\n",
      "batch 5324: loss 0.029337\n",
      "batch 5325: loss 0.021721\n",
      "batch 5326: loss 0.006409\n",
      "batch 5327: loss 0.029375\n",
      "batch 5328: loss 0.033760\n",
      "batch 5329: loss 0.049238\n",
      "batch 5330: loss 0.015219\n",
      "batch 5331: loss 0.144501\n",
      "batch 5332: loss 0.068503\n",
      "batch 5333: loss 0.193110\n",
      "batch 5334: loss 0.059179\n",
      "batch 5335: loss 0.044185\n",
      "batch 5336: loss 0.035219\n",
      "batch 5337: loss 0.036011\n",
      "batch 5338: loss 0.020639\n",
      "batch 5339: loss 0.014224\n",
      "batch 5340: loss 0.050520\n",
      "batch 5341: loss 0.046419\n",
      "batch 5342: loss 0.019239\n",
      "batch 5343: loss 0.196947\n",
      "batch 5344: loss 0.031602\n",
      "batch 5345: loss 0.072585\n",
      "batch 5346: loss 0.039356\n",
      "batch 5347: loss 0.083250\n",
      "batch 5348: loss 0.111393\n",
      "batch 5349: loss 0.020413\n",
      "batch 5350: loss 0.151929\n",
      "batch 5351: loss 0.016124\n",
      "batch 5352: loss 0.014266\n",
      "batch 5353: loss 0.080419\n",
      "batch 5354: loss 0.030691\n",
      "batch 5355: loss 0.031635\n",
      "batch 5356: loss 0.104330\n",
      "batch 5357: loss 0.135465\n",
      "batch 5358: loss 0.024643\n",
      "batch 5359: loss 0.024658\n",
      "batch 5360: loss 0.113412\n",
      "batch 5361: loss 0.055622\n",
      "batch 5362: loss 0.050693\n",
      "batch 5363: loss 0.084972\n",
      "batch 5364: loss 0.119654\n",
      "batch 5365: loss 0.042832\n",
      "batch 5366: loss 0.071360\n",
      "batch 5367: loss 0.014860\n",
      "batch 5368: loss 0.063192\n",
      "batch 5369: loss 0.075630\n",
      "batch 5370: loss 0.035909\n",
      "batch 5371: loss 0.023287\n",
      "batch 5372: loss 0.116059\n",
      "batch 5373: loss 0.074726\n",
      "batch 5374: loss 0.031934\n",
      "batch 5375: loss 0.120444\n",
      "batch 5376: loss 0.011955\n",
      "batch 5377: loss 0.014186\n",
      "batch 5378: loss 0.099741\n",
      "batch 5379: loss 0.004897\n",
      "batch 5380: loss 0.010641\n",
      "batch 5381: loss 0.014107\n",
      "batch 5382: loss 0.028715\n",
      "batch 5383: loss 0.014615\n",
      "batch 5384: loss 0.029234\n",
      "batch 5385: loss 0.019023\n",
      "batch 5386: loss 0.055487\n",
      "batch 5387: loss 0.019036\n",
      "batch 5388: loss 0.074054\n",
      "batch 5389: loss 0.042711\n",
      "batch 5390: loss 0.039963\n",
      "batch 5391: loss 0.025399\n",
      "batch 5392: loss 0.110025\n",
      "batch 5393: loss 0.023159\n",
      "batch 5394: loss 0.118104\n",
      "batch 5395: loss 0.012929\n",
      "batch 5396: loss 0.011603\n",
      "batch 5397: loss 0.077447\n",
      "batch 5398: loss 0.006754\n",
      "batch 5399: loss 0.046225\n",
      "batch 5400: loss 0.117934\n",
      "batch 5401: loss 0.015840\n",
      "batch 5402: loss 0.062638\n",
      "batch 5403: loss 0.104446\n",
      "batch 5404: loss 0.007471\n",
      "batch 5405: loss 0.048079\n",
      "batch 5406: loss 0.012944\n",
      "batch 5407: loss 0.042761\n",
      "batch 5408: loss 0.033536\n",
      "batch 5409: loss 0.115931\n",
      "batch 5410: loss 0.065972\n",
      "batch 5411: loss 0.056849\n",
      "batch 5412: loss 0.089963\n",
      "batch 5413: loss 0.063138\n",
      "batch 5414: loss 0.228390\n",
      "batch 5415: loss 0.188224\n",
      "batch 5416: loss 0.090242\n",
      "batch 5417: loss 0.122834\n",
      "batch 5418: loss 0.040758\n",
      "batch 5419: loss 0.033286\n",
      "batch 5420: loss 0.007738\n",
      "batch 5421: loss 0.027252\n",
      "batch 5422: loss 0.021579\n",
      "batch 5423: loss 0.080928\n",
      "batch 5424: loss 0.110241\n",
      "batch 5425: loss 0.062937\n",
      "batch 5426: loss 0.012008\n",
      "batch 5427: loss 0.031971\n",
      "batch 5428: loss 0.022754\n",
      "batch 5429: loss 0.139663\n",
      "batch 5430: loss 0.032217\n",
      "batch 5431: loss 0.049515\n",
      "batch 5432: loss 0.026959\n",
      "batch 5433: loss 0.031154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5434: loss 0.005399\n",
      "batch 5435: loss 0.140274\n",
      "batch 5436: loss 0.040588\n",
      "batch 5437: loss 0.130424\n",
      "batch 5438: loss 0.020335\n",
      "batch 5439: loss 0.005546\n",
      "batch 5440: loss 0.026691\n",
      "batch 5441: loss 0.028415\n",
      "batch 5442: loss 0.072437\n",
      "batch 5443: loss 0.042467\n",
      "batch 5444: loss 0.036198\n",
      "batch 5445: loss 0.015582\n",
      "batch 5446: loss 0.016076\n",
      "batch 5447: loss 0.038996\n",
      "batch 5448: loss 0.075612\n",
      "batch 5449: loss 0.032558\n",
      "batch 5450: loss 0.036463\n",
      "batch 5451: loss 0.081481\n",
      "batch 5452: loss 0.023301\n",
      "batch 5453: loss 0.024229\n",
      "batch 5454: loss 0.122796\n",
      "batch 5455: loss 0.009106\n",
      "batch 5456: loss 0.033761\n",
      "batch 5457: loss 0.119223\n",
      "batch 5458: loss 0.012178\n",
      "batch 5459: loss 0.049769\n",
      "batch 5460: loss 0.022803\n",
      "batch 5461: loss 0.049132\n",
      "batch 5462: loss 0.037522\n",
      "batch 5463: loss 0.034101\n",
      "batch 5464: loss 0.060980\n",
      "batch 5465: loss 0.098946\n",
      "batch 5466: loss 0.083074\n",
      "batch 5467: loss 0.015809\n",
      "batch 5468: loss 0.166464\n",
      "batch 5469: loss 0.258132\n",
      "batch 5470: loss 0.048449\n",
      "batch 5471: loss 0.249369\n",
      "batch 5472: loss 0.068150\n",
      "batch 5473: loss 0.006408\n",
      "batch 5474: loss 0.041037\n",
      "batch 5475: loss 0.100025\n",
      "batch 5476: loss 0.012125\n",
      "batch 5477: loss 0.078238\n",
      "batch 5478: loss 0.025795\n",
      "batch 5479: loss 0.027435\n",
      "batch 5480: loss 0.026708\n",
      "batch 5481: loss 0.043075\n",
      "batch 5482: loss 0.078540\n",
      "batch 5483: loss 0.008036\n",
      "batch 5484: loss 0.069309\n",
      "batch 5485: loss 0.050098\n",
      "batch 5486: loss 0.028572\n",
      "batch 5487: loss 0.024285\n",
      "batch 5488: loss 0.036337\n",
      "batch 5489: loss 0.072325\n",
      "batch 5490: loss 0.086866\n",
      "batch 5491: loss 0.046079\n",
      "batch 5492: loss 0.084956\n",
      "batch 5493: loss 0.029513\n",
      "batch 5494: loss 0.031698\n",
      "batch 5495: loss 0.054186\n",
      "batch 5496: loss 0.042815\n",
      "batch 5497: loss 0.044789\n",
      "batch 5498: loss 0.078948\n",
      "batch 5499: loss 0.062055\n",
      "batch 5500: loss 0.016266\n",
      "batch 5501: loss 0.017741\n",
      "batch 5502: loss 0.016037\n",
      "batch 5503: loss 0.058916\n",
      "batch 5504: loss 0.098606\n",
      "batch 5505: loss 0.016369\n",
      "batch 5506: loss 0.018761\n",
      "batch 5507: loss 0.052635\n",
      "batch 5508: loss 0.030067\n",
      "batch 5509: loss 0.028282\n",
      "batch 5510: loss 0.033887\n",
      "batch 5511: loss 0.041688\n",
      "batch 5512: loss 0.047645\n",
      "batch 5513: loss 0.031716\n",
      "batch 5514: loss 0.008573\n",
      "batch 5515: loss 0.028340\n",
      "batch 5516: loss 0.106300\n",
      "batch 5517: loss 0.025216\n",
      "batch 5518: loss 0.046782\n",
      "batch 5519: loss 0.026209\n",
      "batch 5520: loss 0.026595\n",
      "batch 5521: loss 0.053916\n",
      "batch 5522: loss 0.066398\n",
      "batch 5523: loss 0.018467\n",
      "batch 5524: loss 0.032397\n",
      "batch 5525: loss 0.010694\n",
      "batch 5526: loss 0.009075\n",
      "batch 5527: loss 0.084730\n",
      "batch 5528: loss 0.057308\n",
      "batch 5529: loss 0.077403\n",
      "batch 5530: loss 0.069425\n",
      "batch 5531: loss 0.040863\n",
      "batch 5532: loss 0.018793\n",
      "batch 5533: loss 0.011694\n",
      "batch 5534: loss 0.096401\n",
      "batch 5535: loss 0.019291\n",
      "batch 5536: loss 0.088850\n",
      "batch 5537: loss 0.019267\n",
      "batch 5538: loss 0.021088\n",
      "batch 5539: loss 0.011405\n",
      "batch 5540: loss 0.007621\n",
      "batch 5541: loss 0.037914\n",
      "batch 5542: loss 0.028446\n",
      "batch 5543: loss 0.060744\n",
      "batch 5544: loss 0.015883\n",
      "batch 5545: loss 0.015508\n",
      "batch 5546: loss 0.022458\n",
      "batch 5547: loss 0.093512\n",
      "batch 5548: loss 0.005931\n",
      "batch 5549: loss 0.061132\n",
      "batch 5550: loss 0.032107\n",
      "batch 5551: loss 0.047860\n",
      "batch 5552: loss 0.069561\n",
      "batch 5553: loss 0.094381\n",
      "batch 5554: loss 0.130422\n",
      "batch 5555: loss 0.004085\n",
      "batch 5556: loss 0.037488\n",
      "batch 5557: loss 0.028123\n",
      "batch 5558: loss 0.024274\n",
      "batch 5559: loss 0.056039\n",
      "batch 5560: loss 0.050221\n",
      "batch 5561: loss 0.071039\n",
      "batch 5562: loss 0.017876\n",
      "batch 5563: loss 0.030638\n",
      "batch 5564: loss 0.010191\n",
      "batch 5565: loss 0.022607\n",
      "batch 5566: loss 0.004947\n",
      "batch 5567: loss 0.022130\n",
      "batch 5568: loss 0.050953\n",
      "batch 5569: loss 0.097155\n",
      "batch 5570: loss 0.027029\n",
      "batch 5571: loss 0.016621\n",
      "batch 5572: loss 0.132372\n",
      "batch 5573: loss 0.016403\n",
      "batch 5574: loss 0.024480\n",
      "batch 5575: loss 0.027890\n",
      "batch 5576: loss 0.025059\n",
      "batch 5577: loss 0.007583\n",
      "batch 5578: loss 0.097097\n",
      "batch 5579: loss 0.025264\n",
      "batch 5580: loss 0.008347\n",
      "batch 5581: loss 0.043806\n",
      "batch 5582: loss 0.051284\n",
      "batch 5583: loss 0.008373\n",
      "batch 5584: loss 0.024947\n",
      "batch 5585: loss 0.016110\n",
      "batch 5586: loss 0.004922\n",
      "batch 5587: loss 0.020653\n",
      "batch 5588: loss 0.013452\n",
      "batch 5589: loss 0.106705\n",
      "batch 5590: loss 0.012290\n",
      "batch 5591: loss 0.010766\n",
      "batch 5592: loss 0.051578\n",
      "batch 5593: loss 0.027008\n",
      "batch 5594: loss 0.007353\n",
      "batch 5595: loss 0.004627\n",
      "batch 5596: loss 0.044255\n",
      "batch 5597: loss 0.047941\n",
      "batch 5598: loss 0.017026\n",
      "batch 5599: loss 0.055377\n",
      "batch 5600: loss 0.039314\n",
      "batch 5601: loss 0.019506\n",
      "batch 5602: loss 0.162943\n",
      "batch 5603: loss 0.109005\n",
      "batch 5604: loss 0.079022\n",
      "batch 5605: loss 0.018707\n",
      "batch 5606: loss 0.017371\n",
      "batch 5607: loss 0.062624\n",
      "batch 5608: loss 0.009089\n",
      "batch 5609: loss 0.004691\n",
      "batch 5610: loss 0.097667\n",
      "batch 5611: loss 0.006197\n",
      "batch 5612: loss 0.021521\n",
      "batch 5613: loss 0.071255\n",
      "batch 5614: loss 0.020145\n",
      "batch 5615: loss 0.019663\n",
      "batch 5616: loss 0.124262\n",
      "batch 5617: loss 0.038113\n",
      "batch 5618: loss 0.054435\n",
      "batch 5619: loss 0.038656\n",
      "batch 5620: loss 0.126213\n",
      "batch 5621: loss 0.030609\n",
      "batch 5622: loss 0.009934\n",
      "batch 5623: loss 0.067605\n",
      "batch 5624: loss 0.034242\n",
      "batch 5625: loss 0.093976\n",
      "batch 5626: loss 0.029989\n",
      "batch 5627: loss 0.040151\n",
      "batch 5628: loss 0.022249\n",
      "batch 5629: loss 0.044641\n",
      "batch 5630: loss 0.043396\n",
      "batch 5631: loss 0.013080\n",
      "batch 5632: loss 0.009605\n",
      "batch 5633: loss 0.022337\n",
      "batch 5634: loss 0.045420\n",
      "batch 5635: loss 0.024144\n",
      "batch 5636: loss 0.020029\n",
      "batch 5637: loss 0.042156\n",
      "batch 5638: loss 0.139611\n",
      "batch 5639: loss 0.005683\n",
      "batch 5640: loss 0.260715\n",
      "batch 5641: loss 0.036163\n",
      "batch 5642: loss 0.060070\n",
      "batch 5643: loss 0.017256\n",
      "batch 5644: loss 0.021069\n",
      "batch 5645: loss 0.033567\n",
      "batch 5646: loss 0.013850\n",
      "batch 5647: loss 0.012845\n",
      "batch 5648: loss 0.006994\n",
      "batch 5649: loss 0.012930\n",
      "batch 5650: loss 0.007783\n",
      "batch 5651: loss 0.171614\n",
      "batch 5652: loss 0.052908\n",
      "batch 5653: loss 0.010096\n",
      "batch 5654: loss 0.119206\n",
      "batch 5655: loss 0.031369\n",
      "batch 5656: loss 0.010756\n",
      "batch 5657: loss 0.013014\n",
      "batch 5658: loss 0.018086\n",
      "batch 5659: loss 0.040459\n",
      "batch 5660: loss 0.007571\n",
      "batch 5661: loss 0.008662\n",
      "batch 5662: loss 0.017692\n",
      "batch 5663: loss 0.017417\n",
      "batch 5664: loss 0.035623\n",
      "batch 5665: loss 0.093580\n",
      "batch 5666: loss 0.012622\n",
      "batch 5667: loss 0.063341\n",
      "batch 5668: loss 0.047747\n",
      "batch 5669: loss 0.009718\n",
      "batch 5670: loss 0.008368\n",
      "batch 5671: loss 0.060675\n",
      "batch 5672: loss 0.075378\n",
      "batch 5673: loss 0.103423\n",
      "batch 5674: loss 0.024382\n",
      "batch 5675: loss 0.087578\n",
      "batch 5676: loss 0.037275\n",
      "batch 5677: loss 0.119436\n",
      "batch 5678: loss 0.006388\n",
      "batch 5679: loss 0.072728\n",
      "batch 5680: loss 0.102873\n",
      "batch 5681: loss 0.221834\n",
      "batch 5682: loss 0.052115\n",
      "batch 5683: loss 0.038862\n",
      "batch 5684: loss 0.061871\n",
      "batch 5685: loss 0.036044\n",
      "batch 5686: loss 0.014537\n",
      "batch 5687: loss 0.044655\n",
      "batch 5688: loss 0.011895\n",
      "batch 5689: loss 0.071768\n",
      "batch 5690: loss 0.009638\n",
      "batch 5691: loss 0.037959\n",
      "batch 5692: loss 0.033388\n",
      "batch 5693: loss 0.022380\n",
      "batch 5694: loss 0.046564\n",
      "batch 5695: loss 0.074523\n",
      "batch 5696: loss 0.177185\n",
      "batch 5697: loss 0.030800\n",
      "batch 5698: loss 0.051170\n",
      "batch 5699: loss 0.079978\n",
      "batch 5700: loss 0.013987\n",
      "batch 5701: loss 0.088716\n",
      "batch 5702: loss 0.010126\n",
      "batch 5703: loss 0.014855\n",
      "batch 5704: loss 0.066380\n",
      "batch 5705: loss 0.021055\n",
      "batch 5706: loss 0.171798\n",
      "batch 5707: loss 0.129275\n",
      "batch 5708: loss 0.032459\n",
      "batch 5709: loss 0.026996\n",
      "batch 5710: loss 0.064226\n",
      "batch 5711: loss 0.046155\n",
      "batch 5712: loss 0.087512\n",
      "batch 5713: loss 0.025878\n",
      "batch 5714: loss 0.093018\n",
      "batch 5715: loss 0.032394\n",
      "batch 5716: loss 0.007067\n",
      "batch 5717: loss 0.042463\n",
      "batch 5718: loss 0.012545\n",
      "batch 5719: loss 0.178329\n",
      "batch 5720: loss 0.059658\n",
      "batch 5721: loss 0.027380\n",
      "batch 5722: loss 0.064705\n",
      "batch 5723: loss 0.061337\n",
      "batch 5724: loss 0.033354\n",
      "batch 5725: loss 0.056159\n",
      "batch 5726: loss 0.134973\n",
      "batch 5727: loss 0.009624\n",
      "batch 5728: loss 0.151740\n",
      "batch 5729: loss 0.035352\n",
      "batch 5730: loss 0.036092\n",
      "batch 5731: loss 0.049823\n",
      "batch 5732: loss 0.184388\n",
      "batch 5733: loss 0.060783\n",
      "batch 5734: loss 0.106930\n",
      "batch 5735: loss 0.052667\n",
      "batch 5736: loss 0.016736\n",
      "batch 5737: loss 0.015165\n",
      "batch 5738: loss 0.009469\n",
      "batch 5739: loss 0.125137\n",
      "batch 5740: loss 0.006011\n",
      "batch 5741: loss 0.043026\n",
      "batch 5742: loss 0.014218\n",
      "batch 5743: loss 0.081074\n",
      "batch 5744: loss 0.041899\n",
      "batch 5745: loss 0.136304\n",
      "batch 5746: loss 0.050498\n",
      "batch 5747: loss 0.051257\n",
      "batch 5748: loss 0.034382\n",
      "batch 5749: loss 0.010967\n",
      "batch 5750: loss 0.036295\n",
      "batch 5751: loss 0.135059\n",
      "batch 5752: loss 0.030383\n",
      "batch 5753: loss 0.017425\n",
      "batch 5754: loss 0.025335\n",
      "batch 5755: loss 0.097497\n",
      "batch 5756: loss 0.139470\n",
      "batch 5757: loss 0.050160\n",
      "batch 5758: loss 0.022631\n",
      "batch 5759: loss 0.033909\n",
      "batch 5760: loss 0.021862\n",
      "batch 5761: loss 0.048538\n",
      "batch 5762: loss 0.030531\n",
      "batch 5763: loss 0.135206\n",
      "batch 5764: loss 0.027458\n",
      "batch 5765: loss 0.030129\n",
      "batch 5766: loss 0.102356\n",
      "batch 5767: loss 0.134453\n",
      "batch 5768: loss 0.011702\n",
      "batch 5769: loss 0.023042\n",
      "batch 5770: loss 0.029860\n",
      "batch 5771: loss 0.088296\n",
      "batch 5772: loss 0.031956\n",
      "batch 5773: loss 0.021335\n",
      "batch 5774: loss 0.013800\n",
      "batch 5775: loss 0.040668\n",
      "batch 5776: loss 0.091231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5777: loss 0.104267\n",
      "batch 5778: loss 0.119306\n",
      "batch 5779: loss 0.129082\n",
      "batch 5780: loss 0.028040\n",
      "batch 5781: loss 0.145208\n",
      "batch 5782: loss 0.037131\n",
      "batch 5783: loss 0.073354\n",
      "batch 5784: loss 0.078038\n",
      "batch 5785: loss 0.065277\n",
      "batch 5786: loss 0.071798\n",
      "batch 5787: loss 0.083576\n",
      "batch 5788: loss 0.069607\n",
      "batch 5789: loss 0.031996\n",
      "batch 5790: loss 0.032126\n",
      "batch 5791: loss 0.084191\n",
      "batch 5792: loss 0.016723\n",
      "batch 5793: loss 0.038692\n",
      "batch 5794: loss 0.026809\n",
      "batch 5795: loss 0.070674\n",
      "batch 5796: loss 0.010927\n",
      "batch 5797: loss 0.033127\n",
      "batch 5798: loss 0.011371\n",
      "batch 5799: loss 0.013495\n",
      "batch 5800: loss 0.027510\n",
      "batch 5801: loss 0.064980\n",
      "batch 5802: loss 0.028854\n",
      "batch 5803: loss 0.031969\n",
      "batch 5804: loss 0.029882\n",
      "batch 5805: loss 0.009789\n",
      "batch 5806: loss 0.047843\n",
      "batch 5807: loss 0.071522\n",
      "batch 5808: loss 0.025624\n",
      "batch 5809: loss 0.035372\n",
      "batch 5810: loss 0.016835\n",
      "batch 5811: loss 0.049969\n",
      "batch 5812: loss 0.017545\n",
      "batch 5813: loss 0.033383\n",
      "batch 5814: loss 0.209501\n",
      "batch 5815: loss 0.032177\n",
      "batch 5816: loss 0.084891\n",
      "batch 5817: loss 0.017185\n",
      "batch 5818: loss 0.066531\n",
      "batch 5819: loss 0.041121\n",
      "batch 5820: loss 0.011832\n",
      "batch 5821: loss 0.053503\n",
      "batch 5822: loss 0.053958\n",
      "batch 5823: loss 0.007897\n",
      "batch 5824: loss 0.069307\n",
      "batch 5825: loss 0.034334\n",
      "batch 5826: loss 0.013949\n",
      "batch 5827: loss 0.031798\n",
      "batch 5828: loss 0.029750\n",
      "batch 5829: loss 0.042269\n",
      "batch 5830: loss 0.006097\n",
      "batch 5831: loss 0.072813\n",
      "batch 5832: loss 0.067032\n",
      "batch 5833: loss 0.042312\n",
      "batch 5834: loss 0.016861\n",
      "batch 5835: loss 0.041228\n",
      "batch 5836: loss 0.325086\n",
      "batch 5837: loss 0.051885\n",
      "batch 5838: loss 0.006480\n",
      "batch 5839: loss 0.002388\n",
      "batch 5840: loss 0.096476\n",
      "batch 5841: loss 0.029651\n",
      "batch 5842: loss 0.090278\n",
      "batch 5843: loss 0.018295\n",
      "batch 5844: loss 0.016001\n",
      "batch 5845: loss 0.023846\n",
      "batch 5846: loss 0.031253\n",
      "batch 5847: loss 0.030183\n",
      "batch 5848: loss 0.034523\n",
      "batch 5849: loss 0.111710\n",
      "batch 5850: loss 0.130675\n",
      "batch 5851: loss 0.003311\n",
      "batch 5852: loss 0.055261\n",
      "batch 5853: loss 0.015876\n",
      "batch 5854: loss 0.036093\n",
      "batch 5855: loss 0.006008\n",
      "batch 5856: loss 0.011545\n",
      "batch 5857: loss 0.003986\n",
      "batch 5858: loss 0.027954\n",
      "batch 5859: loss 0.054942\n",
      "batch 5860: loss 0.024651\n",
      "batch 5861: loss 0.033160\n",
      "batch 5862: loss 0.160352\n",
      "batch 5863: loss 0.082523\n",
      "batch 5864: loss 0.045956\n",
      "batch 5865: loss 0.014795\n",
      "batch 5866: loss 0.026628\n",
      "batch 5867: loss 0.129820\n",
      "batch 5868: loss 0.053823\n",
      "batch 5869: loss 0.055491\n",
      "batch 5870: loss 0.021246\n",
      "batch 5871: loss 0.025549\n",
      "batch 5872: loss 0.098679\n",
      "batch 5873: loss 0.161890\n",
      "batch 5874: loss 0.026496\n",
      "batch 5875: loss 0.198603\n",
      "batch 5876: loss 0.022765\n",
      "batch 5877: loss 0.077319\n",
      "batch 5878: loss 0.029982\n",
      "batch 5879: loss 0.035036\n",
      "batch 5880: loss 0.012781\n",
      "batch 5881: loss 0.024423\n",
      "batch 5882: loss 0.100255\n",
      "batch 5883: loss 0.038094\n",
      "batch 5884: loss 0.125369\n",
      "batch 5885: loss 0.020786\n",
      "batch 5886: loss 0.025445\n",
      "batch 5887: loss 0.036806\n",
      "batch 5888: loss 0.072845\n",
      "batch 5889: loss 0.013740\n",
      "batch 5890: loss 0.009755\n",
      "batch 5891: loss 0.022244\n",
      "batch 5892: loss 0.040071\n",
      "batch 5893: loss 0.067621\n",
      "batch 5894: loss 0.016720\n",
      "batch 5895: loss 0.074857\n",
      "batch 5896: loss 0.060262\n",
      "batch 5897: loss 0.030528\n",
      "batch 5898: loss 0.052703\n",
      "batch 5899: loss 0.010866\n",
      "batch 5900: loss 0.018398\n",
      "batch 5901: loss 0.049168\n",
      "batch 5902: loss 0.057150\n",
      "batch 5903: loss 0.035200\n",
      "batch 5904: loss 0.022892\n",
      "batch 5905: loss 0.026875\n",
      "batch 5906: loss 0.013704\n",
      "batch 5907: loss 0.072464\n",
      "batch 5908: loss 0.028752\n",
      "batch 5909: loss 0.052799\n",
      "batch 5910: loss 0.019009\n",
      "batch 5911: loss 0.012539\n",
      "batch 5912: loss 0.029621\n",
      "batch 5913: loss 0.112438\n",
      "batch 5914: loss 0.211546\n",
      "batch 5915: loss 0.057980\n",
      "batch 5916: loss 0.049926\n",
      "batch 5917: loss 0.023009\n",
      "batch 5918: loss 0.016927\n",
      "batch 5919: loss 0.016040\n",
      "batch 5920: loss 0.007091\n",
      "batch 5921: loss 0.068683\n",
      "batch 5922: loss 0.004011\n",
      "batch 5923: loss 0.027995\n",
      "batch 5924: loss 0.080094\n",
      "batch 5925: loss 0.008226\n",
      "batch 5926: loss 0.094384\n",
      "batch 5927: loss 0.014932\n",
      "batch 5928: loss 0.030764\n",
      "batch 5929: loss 0.019127\n",
      "batch 5930: loss 0.075165\n",
      "batch 5931: loss 0.030992\n",
      "batch 5932: loss 0.142513\n",
      "batch 5933: loss 0.068991\n",
      "batch 5934: loss 0.012148\n",
      "batch 5935: loss 0.075386\n",
      "batch 5936: loss 0.009635\n",
      "batch 5937: loss 0.111574\n",
      "batch 5938: loss 0.105889\n",
      "batch 5939: loss 0.092912\n",
      "batch 5940: loss 0.024704\n",
      "batch 5941: loss 0.181045\n",
      "batch 5942: loss 0.025712\n",
      "batch 5943: loss 0.084548\n",
      "batch 5944: loss 0.122203\n",
      "batch 5945: loss 0.010300\n",
      "batch 5946: loss 0.063238\n",
      "batch 5947: loss 0.028999\n",
      "batch 5948: loss 0.082935\n",
      "batch 5949: loss 0.023578\n",
      "batch 5950: loss 0.025014\n",
      "batch 5951: loss 0.039594\n",
      "batch 5952: loss 0.131440\n",
      "batch 5953: loss 0.026170\n",
      "batch 5954: loss 0.074196\n",
      "batch 5955: loss 0.032907\n",
      "batch 5956: loss 0.026549\n",
      "batch 5957: loss 0.039334\n",
      "batch 5958: loss 0.116926\n",
      "batch 5959: loss 0.027771\n",
      "batch 5960: loss 0.007349\n",
      "batch 5961: loss 0.040462\n",
      "batch 5962: loss 0.022648\n",
      "batch 5963: loss 0.052029\n",
      "batch 5964: loss 0.069582\n",
      "batch 5965: loss 0.016692\n",
      "batch 5966: loss 0.077419\n",
      "batch 5967: loss 0.013488\n",
      "batch 5968: loss 0.044979\n",
      "batch 5969: loss 0.012979\n",
      "batch 5970: loss 0.084278\n",
      "batch 5971: loss 0.009005\n",
      "batch 5972: loss 0.073622\n",
      "batch 5973: loss 0.055542\n",
      "batch 5974: loss 0.088729\n",
      "batch 5975: loss 0.020893\n",
      "batch 5976: loss 0.012050\n",
      "batch 5977: loss 0.079281\n",
      "batch 5978: loss 0.109306\n",
      "batch 5979: loss 0.024978\n",
      "batch 5980: loss 0.083228\n",
      "batch 5981: loss 0.113672\n",
      "batch 5982: loss 0.122051\n",
      "batch 5983: loss 0.126540\n",
      "batch 5984: loss 0.011215\n",
      "batch 5985: loss 0.017730\n",
      "batch 5986: loss 0.079757\n",
      "batch 5987: loss 0.060469\n",
      "batch 5988: loss 0.107954\n",
      "batch 5989: loss 0.013342\n",
      "batch 5990: loss 0.036553\n",
      "batch 5991: loss 0.105831\n",
      "batch 5992: loss 0.032174\n",
      "batch 5993: loss 0.063141\n",
      "batch 5994: loss 0.017080\n",
      "batch 5995: loss 0.045057\n",
      "batch 5996: loss 0.053986\n",
      "batch 5997: loss 0.015046\n",
      "batch 5998: loss 0.017398\n",
      "batch 5999: loss 0.036781\n",
      "test accuracy: 0.973000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "#        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L2(0.01))\n",
    "\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上正則向(keras版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 1s - loss: 12.2978 - accuracy: 0.7207 - val_loss: 8.7443 - val_accuracy: 0.8365\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 7.6594 - accuracy: 0.8703 - val_loss: 6.6998 - val_accuracy: 0.8843\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 5.8720 - accuracy: 0.9081 - val_loss: 5.2062 - val_accuracy: 0.9046\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 4.5586 - accuracy: 0.9265 - val_loss: 4.0508 - val_accuracy: 0.9223\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 3.5584 - accuracy: 0.9383 - val_loss: 3.1832 - val_accuracy: 0.9312\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 2.8015 - accuracy: 0.9464 - val_loss: 2.5239 - val_accuracy: 0.9357\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 2.2219 - accuracy: 0.9496 - val_loss: 2.0048 - val_accuracy: 0.9428\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 1.7787 - accuracy: 0.9540 - val_loss: 1.6279 - val_accuracy: 0.9443\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 1.4565 - accuracy: 0.9545 - val_loss: 1.3506 - val_accuracy: 0.9443\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 1.2055 - accuracy: 0.9572 - val_loss: 1.1298 - val_accuracy: 0.9477\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 1.0117 - accuracy: 0.9599 - val_loss: 0.9549 - val_accuracy: 0.9518\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 0.8632 - accuracy: 0.9606 - val_loss: 0.8330 - val_accuracy: 0.9501\n",
      "Epoch 13/100\n",
      "165/165 - 0s - loss: 0.7491 - accuracy: 0.9620 - val_loss: 0.7312 - val_accuracy: 0.9506\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.6549 - accuracy: 0.9630 - val_loss: 0.6464 - val_accuracy: 0.9526\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.5811 - accuracy: 0.9644 - val_loss: 0.5861 - val_accuracy: 0.9514\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.5270 - accuracy: 0.9646 - val_loss: 0.5364 - val_accuracy: 0.9549\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.4844 - accuracy: 0.9670 - val_loss: 0.4952 - val_accuracy: 0.9551\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.4452 - accuracy: 0.9675 - val_loss: 0.4638 - val_accuracy: 0.9558\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.4170 - accuracy: 0.9676 - val_loss: 0.4460 - val_accuracy: 0.9542\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.3939 - accuracy: 0.9691 - val_loss: 0.4160 - val_accuracy: 0.9576\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.3739 - accuracy: 0.9695 - val_loss: 0.3992 - val_accuracy: 0.9582\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.3562 - accuracy: 0.9717 - val_loss: 0.3864 - val_accuracy: 0.9589\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.3399 - accuracy: 0.9727 - val_loss: 0.3767 - val_accuracy: 0.9583\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.3284 - accuracy: 0.9726 - val_loss: 0.3626 - val_accuracy: 0.9589\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.3158 - accuracy: 0.9740 - val_loss: 0.3645 - val_accuracy: 0.9560\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.3073 - accuracy: 0.9735 - val_loss: 0.3428 - val_accuracy: 0.9618\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.2948 - accuracy: 0.9760 - val_loss: 0.3377 - val_accuracy: 0.9608\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.2855 - accuracy: 0.9761 - val_loss: 0.3230 - val_accuracy: 0.9628\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.2786 - accuracy: 0.9760 - val_loss: 0.3200 - val_accuracy: 0.9614\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.2732 - accuracy: 0.9766 - val_loss: 0.3294 - val_accuracy: 0.9557\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.2654 - accuracy: 0.9776 - val_loss: 0.3270 - val_accuracy: 0.9564\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.2587 - accuracy: 0.9777 - val_loss: 0.3062 - val_accuracy: 0.9615\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.2516 - accuracy: 0.9787 - val_loss: 0.3104 - val_accuracy: 0.9573\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.2467 - accuracy: 0.9789 - val_loss: 0.3067 - val_accuracy: 0.9587\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.2407 - accuracy: 0.9796 - val_loss: 0.2972 - val_accuracy: 0.9603\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.2361 - accuracy: 0.9800 - val_loss: 0.2927 - val_accuracy: 0.9626\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.2320 - accuracy: 0.9800 - val_loss: 0.2918 - val_accuracy: 0.9602\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.2281 - accuracy: 0.9804 - val_loss: 0.2853 - val_accuracy: 0.9614\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.2249 - accuracy: 0.9805 - val_loss: 0.2826 - val_accuracy: 0.9623\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.2144 - accuracy: 0.9822 - val_loss: 0.2748 - val_accuracy: 0.9630\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.2103 - accuracy: 0.9823 - val_loss: 0.2719 - val_accuracy: 0.9629\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.2075 - accuracy: 0.9823 - val_loss: 0.2755 - val_accuracy: 0.9626\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.2043 - accuracy: 0.9830 - val_loss: 0.2736 - val_accuracy: 0.9612\n",
      "Epoch 44/100\n",
      "165/165 - 0s - loss: 0.2003 - accuracy: 0.9840 - val_loss: 0.2719 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.1965 - accuracy: 0.9853 - val_loss: 0.2680 - val_accuracy: 0.9618\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.1951 - accuracy: 0.9847 - val_loss: 0.2702 - val_accuracy: 0.9630\n",
      "Epoch 47/100\n",
      "165/165 - 0s - loss: 0.1895 - accuracy: 0.9854 - val_loss: 0.2689 - val_accuracy: 0.9603\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.1927 - accuracy: 0.9839 - val_loss: 0.2596 - val_accuracy: 0.9638\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.1853 - accuracy: 0.9860 - val_loss: 0.2653 - val_accuracy: 0.9622\n",
      "Epoch 50/100\n",
      "165/165 - 0s - loss: 0.1830 - accuracy: 0.9867 - val_loss: 0.2622 - val_accuracy: 0.9636\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.1816 - accuracy: 0.9868 - val_loss: 0.2593 - val_accuracy: 0.9637\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.1789 - accuracy: 0.9878 - val_loss: 0.2638 - val_accuracy: 0.9631\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.1762 - accuracy: 0.9880 - val_loss: 0.2626 - val_accuracy: 0.9614\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.1762 - accuracy: 0.9878 - val_loss: 0.2611 - val_accuracy: 0.9632\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.1754 - accuracy: 0.9884 - val_loss: 0.2574 - val_accuracy: 0.9636\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.1712 - accuracy: 0.9893 - val_loss: 0.2594 - val_accuracy: 0.9624\n",
      "Epoch 57/100\n",
      "165/165 - 0s - loss: 0.1710 - accuracy: 0.9890 - val_loss: 0.2608 - val_accuracy: 0.9623\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.1697 - accuracy: 0.9883 - val_loss: 0.2566 - val_accuracy: 0.9623\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.1668 - accuracy: 0.9890 - val_loss: 0.2534 - val_accuracy: 0.9632\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.1666 - accuracy: 0.9889 - val_loss: 0.2610 - val_accuracy: 0.9618\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.1656 - accuracy: 0.9892 - val_loss: 0.2524 - val_accuracy: 0.9644\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.1605 - accuracy: 0.9915 - val_loss: 0.2540 - val_accuracy: 0.9635\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.1599 - accuracy: 0.9908 - val_loss: 0.2545 - val_accuracy: 0.9631\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.1593 - accuracy: 0.9914 - val_loss: 0.2546 - val_accuracy: 0.9632\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.1587 - accuracy: 0.9911 - val_loss: 0.2613 - val_accuracy: 0.9619\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.1578 - accuracy: 0.9913 - val_loss: 0.2594 - val_accuracy: 0.9627\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.1579 - accuracy: 0.9913 - val_loss: 0.2614 - val_accuracy: 0.9619\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.1553 - accuracy: 0.9919 - val_loss: 0.2547 - val_accuracy: 0.9646\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.1548 - accuracy: 0.9921 - val_loss: 0.2542 - val_accuracy: 0.9634\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.1563 - accuracy: 0.9915 - val_loss: 0.2683 - val_accuracy: 0.9612\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.1533 - accuracy: 0.9923 - val_loss: 0.2516 - val_accuracy: 0.9635\n",
      "Epoch 72/100\n",
      "165/165 - 0s - loss: 0.1522 - accuracy: 0.9925 - val_loss: 0.2588 - val_accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.1534 - accuracy: 0.9919 - val_loss: 0.2581 - val_accuracy: 0.9629\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.1551 - accuracy: 0.9912 - val_loss: 0.2589 - val_accuracy: 0.9622\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.1487 - accuracy: 0.9934 - val_loss: 0.2579 - val_accuracy: 0.9615\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.1489 - accuracy: 0.9935 - val_loss: 0.2527 - val_accuracy: 0.9641\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.1484 - accuracy: 0.9932 - val_loss: 0.2556 - val_accuracy: 0.9634\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.1466 - accuracy: 0.9936 - val_loss: 0.2589 - val_accuracy: 0.9614\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.1480 - accuracy: 0.9930 - val_loss: 0.2570 - val_accuracy: 0.9638\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.1487 - accuracy: 0.9932 - val_loss: 0.2500 - val_accuracy: 0.9647\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.1444 - accuracy: 0.9940 - val_loss: 0.2581 - val_accuracy: 0.9612\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.1447 - accuracy: 0.9945 - val_loss: 0.2508 - val_accuracy: 0.9647\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.1425 - accuracy: 0.9943 - val_loss: 0.2539 - val_accuracy: 0.9631\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.1444 - accuracy: 0.9938 - val_loss: 0.2545 - val_accuracy: 0.9628\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.1420 - accuracy: 0.9947 - val_loss: 0.2766 - val_accuracy: 0.9559\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.1483 - accuracy: 0.9925 - val_loss: 0.2507 - val_accuracy: 0.9637\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.1403 - accuracy: 0.9951 - val_loss: 0.2467 - val_accuracy: 0.9644\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.1397 - accuracy: 0.9950 - val_loss: 0.2577 - val_accuracy: 0.9609\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.1410 - accuracy: 0.9946 - val_loss: 0.2484 - val_accuracy: 0.9637\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.1394 - accuracy: 0.9947 - val_loss: 0.2577 - val_accuracy: 0.9617\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.1402 - accuracy: 0.9948 - val_loss: 0.2540 - val_accuracy: 0.9636\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.1382 - accuracy: 0.9953 - val_loss: 0.2552 - val_accuracy: 0.9629\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.1364 - accuracy: 0.9956 - val_loss: 0.2486 - val_accuracy: 0.9638\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.1400 - accuracy: 0.9945 - val_loss: 0.2512 - val_accuracy: 0.9642\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.1362 - accuracy: 0.9956 - val_loss: 0.2583 - val_accuracy: 0.9612\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.1362 - accuracy: 0.9955 - val_loss: 0.2532 - val_accuracy: 0.9636\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.1352 - accuracy: 0.9957 - val_loss: 0.2545 - val_accuracy: 0.9630\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.1402 - accuracy: 0.9940 - val_loss: 0.2546 - val_accuracy: 0.9629\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.1342 - accuracy: 0.9957 - val_loss: 0.2592 - val_accuracy: 0.9619\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.1358 - accuracy: 0.9952 - val_loss: 0.2518 - val_accuracy: 0.9630\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 623us/step - loss: 0.2380 - accuracy: 0.9659\n",
      "[0.23797090351581573, 0.9659000039100647]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.498419\n",
      "batch 1: loss 2.635604\n",
      "batch 2: loss 2.407203\n",
      "batch 3: loss 2.214640\n",
      "batch 4: loss 2.261443\n",
      "batch 5: loss 2.198328\n",
      "batch 6: loss 1.999668\n",
      "batch 7: loss 1.995966\n",
      "batch 8: loss 1.993903\n",
      "batch 9: loss 1.906288\n",
      "batch 10: loss 1.853575\n",
      "batch 11: loss 1.915547\n",
      "batch 12: loss 1.831369\n",
      "batch 13: loss 1.763740\n",
      "batch 14: loss 1.800135\n",
      "batch 15: loss 1.722186\n",
      "batch 16: loss 1.822513\n",
      "batch 17: loss 1.602395\n",
      "batch 18: loss 1.722422\n",
      "batch 19: loss 1.676798\n",
      "batch 20: loss 1.475105\n",
      "batch 21: loss 1.377925\n",
      "batch 22: loss 1.504978\n",
      "batch 23: loss 1.517743\n",
      "batch 24: loss 1.373024\n",
      "batch 25: loss 1.239873\n",
      "batch 26: loss 1.572929\n",
      "batch 27: loss 1.432106\n",
      "batch 28: loss 1.619348\n",
      "batch 29: loss 1.174587\n",
      "batch 30: loss 1.073669\n",
      "batch 31: loss 1.465997\n",
      "batch 32: loss 1.255353\n",
      "batch 33: loss 1.155795\n",
      "batch 34: loss 1.296609\n",
      "batch 35: loss 1.073558\n",
      "batch 36: loss 1.336342\n",
      "batch 37: loss 1.267272\n",
      "batch 38: loss 1.178755\n",
      "batch 39: loss 1.001333\n",
      "batch 40: loss 0.997519\n",
      "batch 41: loss 1.056225\n",
      "batch 42: loss 1.094310\n",
      "batch 43: loss 1.070584\n",
      "batch 44: loss 1.089481\n",
      "batch 45: loss 1.095041\n",
      "batch 46: loss 1.231807\n",
      "batch 47: loss 1.220832\n",
      "batch 48: loss 1.272230\n",
      "batch 49: loss 0.757778\n",
      "batch 50: loss 0.950426\n",
      "batch 51: loss 1.149092\n",
      "batch 52: loss 0.946435\n",
      "batch 53: loss 1.085209\n",
      "batch 54: loss 0.980582\n",
      "batch 55: loss 1.155615\n",
      "batch 56: loss 1.162506\n",
      "batch 57: loss 0.962146\n",
      "batch 58: loss 1.021104\n",
      "batch 59: loss 1.157901\n",
      "batch 60: loss 0.977819\n",
      "batch 61: loss 1.105391\n",
      "batch 62: loss 0.988278\n",
      "batch 63: loss 1.104766\n",
      "batch 64: loss 0.896299\n",
      "batch 65: loss 1.021345\n",
      "batch 66: loss 1.015307\n",
      "batch 67: loss 0.883446\n",
      "batch 68: loss 0.789484\n",
      "batch 69: loss 0.938781\n",
      "batch 70: loss 0.849245\n",
      "batch 71: loss 0.732529\n",
      "batch 72: loss 0.833068\n",
      "batch 73: loss 0.987685\n",
      "batch 74: loss 0.944482\n",
      "batch 75: loss 1.109011\n",
      "batch 76: loss 0.947106\n",
      "batch 77: loss 0.885960\n",
      "batch 78: loss 1.049027\n",
      "batch 79: loss 0.873432\n",
      "batch 80: loss 0.700232\n",
      "batch 81: loss 1.031090\n",
      "batch 82: loss 0.789199\n",
      "batch 83: loss 0.693124\n",
      "batch 84: loss 0.860373\n",
      "batch 85: loss 0.885633\n",
      "batch 86: loss 0.797125\n",
      "batch 87: loss 0.969703\n",
      "batch 88: loss 0.950941\n",
      "batch 89: loss 0.836948\n",
      "batch 90: loss 1.101284\n",
      "batch 91: loss 1.146153\n",
      "batch 92: loss 0.817575\n",
      "batch 93: loss 0.795825\n",
      "batch 94: loss 0.825262\n",
      "batch 95: loss 0.858705\n",
      "batch 96: loss 0.644997\n",
      "batch 97: loss 0.908389\n",
      "batch 98: loss 0.950777\n",
      "batch 99: loss 0.962736\n",
      "batch 100: loss 0.840596\n",
      "batch 101: loss 0.902391\n",
      "batch 102: loss 0.738191\n",
      "batch 103: loss 0.738853\n",
      "batch 104: loss 0.713630\n",
      "batch 105: loss 0.970407\n",
      "batch 106: loss 0.904341\n",
      "batch 107: loss 0.937435\n",
      "batch 108: loss 0.511614\n",
      "batch 109: loss 0.823012\n",
      "batch 110: loss 0.877192\n",
      "batch 111: loss 0.733028\n",
      "batch 112: loss 1.021824\n",
      "batch 113: loss 0.923847\n",
      "batch 114: loss 0.766925\n",
      "batch 115: loss 0.787597\n",
      "batch 116: loss 0.881824\n",
      "batch 117: loss 0.765741\n",
      "batch 118: loss 0.788185\n",
      "batch 119: loss 0.762670\n",
      "batch 120: loss 0.887794\n",
      "batch 121: loss 0.859409\n",
      "batch 122: loss 0.913353\n",
      "batch 123: loss 0.710236\n",
      "batch 124: loss 0.505656\n",
      "batch 125: loss 0.739202\n",
      "batch 126: loss 0.820261\n",
      "batch 127: loss 0.973416\n",
      "batch 128: loss 0.623730\n",
      "batch 129: loss 0.625249\n",
      "batch 130: loss 0.891576\n",
      "batch 131: loss 0.620312\n",
      "batch 132: loss 0.759417\n",
      "batch 133: loss 0.800765\n",
      "batch 134: loss 0.704001\n",
      "batch 135: loss 0.812755\n",
      "batch 136: loss 1.002800\n",
      "batch 137: loss 0.837942\n",
      "batch 138: loss 0.969279\n",
      "batch 139: loss 0.788535\n",
      "batch 140: loss 0.796679\n",
      "batch 141: loss 0.525917\n",
      "batch 142: loss 0.722916\n",
      "batch 143: loss 0.603956\n",
      "batch 144: loss 0.697324\n",
      "batch 145: loss 0.566427\n",
      "batch 146: loss 0.618500\n",
      "batch 147: loss 0.826066\n",
      "batch 148: loss 0.676971\n",
      "batch 149: loss 0.775630\n",
      "batch 150: loss 0.740920\n",
      "batch 151: loss 0.652246\n",
      "batch 152: loss 0.741714\n",
      "batch 153: loss 0.764409\n",
      "batch 154: loss 0.683964\n",
      "batch 155: loss 0.606776\n",
      "batch 156: loss 0.656121\n",
      "batch 157: loss 0.890270\n",
      "batch 158: loss 0.687162\n",
      "batch 159: loss 0.594384\n",
      "batch 160: loss 0.822436\n",
      "batch 161: loss 0.633569\n",
      "batch 162: loss 0.545920\n",
      "batch 163: loss 0.671549\n",
      "batch 164: loss 0.714352\n",
      "batch 165: loss 1.029496\n",
      "batch 166: loss 0.772111\n",
      "batch 167: loss 0.651543\n",
      "batch 168: loss 0.446822\n",
      "batch 169: loss 0.637093\n",
      "batch 170: loss 0.675674\n",
      "batch 171: loss 0.614988\n",
      "batch 172: loss 0.867630\n",
      "batch 173: loss 0.704041\n",
      "batch 174: loss 0.908629\n",
      "batch 175: loss 0.672983\n",
      "batch 176: loss 0.883223\n",
      "batch 177: loss 0.665349\n",
      "batch 178: loss 0.706059\n",
      "batch 179: loss 0.696251\n",
      "batch 180: loss 0.828327\n",
      "batch 181: loss 0.799580\n",
      "batch 182: loss 0.896541\n",
      "batch 183: loss 0.542207\n",
      "batch 184: loss 0.609546\n",
      "batch 185: loss 0.789991\n",
      "batch 186: loss 0.863222\n",
      "batch 187: loss 0.555999\n",
      "batch 188: loss 0.689653\n",
      "batch 189: loss 0.710416\n",
      "batch 190: loss 0.728017\n",
      "batch 191: loss 0.781821\n",
      "batch 192: loss 0.607261\n",
      "batch 193: loss 0.871038\n",
      "batch 194: loss 0.711508\n",
      "batch 195: loss 0.616485\n",
      "batch 196: loss 0.923727\n",
      "batch 197: loss 0.817322\n",
      "batch 198: loss 0.687053\n",
      "batch 199: loss 0.542625\n",
      "batch 200: loss 0.487352\n",
      "batch 201: loss 1.090608\n",
      "batch 202: loss 0.760382\n",
      "batch 203: loss 0.586646\n",
      "batch 204: loss 0.658669\n",
      "batch 205: loss 0.598851\n",
      "batch 206: loss 0.459041\n",
      "batch 207: loss 0.624590\n",
      "batch 208: loss 0.627505\n",
      "batch 209: loss 0.607465\n",
      "batch 210: loss 0.655627\n",
      "batch 211: loss 0.587647\n",
      "batch 212: loss 0.724367\n",
      "batch 213: loss 0.593456\n",
      "batch 214: loss 0.795762\n",
      "batch 215: loss 0.581981\n",
      "batch 216: loss 0.604143\n",
      "batch 217: loss 0.736132\n",
      "batch 218: loss 0.793248\n",
      "batch 219: loss 0.825791\n",
      "batch 220: loss 0.491822\n",
      "batch 221: loss 0.851109\n",
      "batch 222: loss 0.852966\n",
      "batch 223: loss 0.706516\n",
      "batch 224: loss 0.747983\n",
      "batch 225: loss 0.581801\n",
      "batch 226: loss 0.862487\n",
      "batch 227: loss 0.949470\n",
      "batch 228: loss 0.676859\n",
      "batch 229: loss 0.653919\n",
      "batch 230: loss 0.662954\n",
      "batch 231: loss 0.460101\n",
      "batch 232: loss 0.491606\n",
      "batch 233: loss 0.688479\n",
      "batch 234: loss 0.608700\n",
      "batch 235: loss 0.651392\n",
      "batch 236: loss 0.513343\n",
      "batch 237: loss 0.579766\n",
      "batch 238: loss 0.507127\n",
      "batch 239: loss 0.530843\n",
      "batch 240: loss 0.595397\n",
      "batch 241: loss 0.546346\n",
      "batch 242: loss 0.692417\n",
      "batch 243: loss 0.763736\n",
      "batch 244: loss 0.563879\n",
      "batch 245: loss 0.376164\n",
      "batch 246: loss 0.511518\n",
      "batch 247: loss 0.814133\n",
      "batch 248: loss 0.706500\n",
      "batch 249: loss 0.688195\n",
      "batch 250: loss 0.838774\n",
      "batch 251: loss 0.713640\n",
      "batch 252: loss 0.793802\n",
      "batch 253: loss 0.771545\n",
      "batch 254: loss 0.733642\n",
      "batch 255: loss 0.685985\n",
      "batch 256: loss 0.702442\n",
      "batch 257: loss 0.727667\n",
      "batch 258: loss 0.737567\n",
      "batch 259: loss 0.750195\n",
      "batch 260: loss 0.838896\n",
      "batch 261: loss 0.627122\n",
      "batch 262: loss 0.807464\n",
      "batch 263: loss 0.455188\n",
      "batch 264: loss 0.691314\n",
      "batch 265: loss 0.781948\n",
      "batch 266: loss 0.490338\n",
      "batch 267: loss 0.519612\n",
      "batch 268: loss 0.642248\n",
      "batch 269: loss 0.281658\n",
      "batch 270: loss 0.428852\n",
      "batch 271: loss 0.643322\n",
      "batch 272: loss 0.518977\n",
      "batch 273: loss 0.482804\n",
      "batch 274: loss 0.534293\n",
      "batch 275: loss 0.455390\n",
      "batch 276: loss 0.845859\n",
      "batch 277: loss 0.583507\n",
      "batch 278: loss 0.671844\n",
      "batch 279: loss 1.026058\n",
      "batch 280: loss 0.567968\n",
      "batch 281: loss 0.415816\n",
      "batch 282: loss 0.892803\n",
      "batch 283: loss 0.788367\n",
      "batch 284: loss 0.550403\n",
      "batch 285: loss 0.773157\n",
      "batch 286: loss 0.524650\n",
      "batch 287: loss 0.700640\n",
      "batch 288: loss 0.797991\n",
      "batch 289: loss 0.735762\n",
      "batch 290: loss 0.345550\n",
      "batch 291: loss 0.525423\n",
      "batch 292: loss 0.507285\n",
      "batch 293: loss 0.422331\n",
      "batch 294: loss 0.496785\n",
      "batch 295: loss 0.639594\n",
      "batch 296: loss 0.555203\n",
      "batch 297: loss 0.972449\n",
      "batch 298: loss 0.636403\n",
      "batch 299: loss 0.622730\n",
      "batch 300: loss 0.525221\n",
      "batch 301: loss 0.526370\n",
      "batch 302: loss 0.567609\n",
      "batch 303: loss 0.529419\n",
      "batch 304: loss 0.522382\n",
      "batch 305: loss 0.721795\n",
      "batch 306: loss 0.442671\n",
      "batch 307: loss 0.749155\n",
      "batch 308: loss 0.426025\n",
      "batch 309: loss 0.790452\n",
      "batch 310: loss 0.656210\n",
      "batch 311: loss 0.624726\n",
      "batch 312: loss 0.592729\n",
      "batch 313: loss 0.637649\n",
      "batch 314: loss 1.229714\n",
      "batch 315: loss 0.453816\n",
      "batch 316: loss 0.482495\n",
      "batch 317: loss 0.751177\n",
      "batch 318: loss 0.508638\n",
      "batch 319: loss 0.792634\n",
      "batch 320: loss 0.520066\n",
      "batch 321: loss 0.678206\n",
      "batch 322: loss 0.569671\n",
      "batch 323: loss 0.666993\n",
      "batch 324: loss 0.764666\n",
      "batch 325: loss 0.825220\n",
      "batch 326: loss 0.600399\n",
      "batch 327: loss 0.599790\n",
      "batch 328: loss 0.557272\n",
      "batch 329: loss 0.875688\n",
      "batch 330: loss 0.515081\n",
      "batch 331: loss 0.692462\n",
      "batch 332: loss 0.974715\n",
      "batch 333: loss 0.626075\n",
      "batch 334: loss 0.756048\n",
      "batch 335: loss 0.381535\n",
      "batch 336: loss 0.385239\n",
      "batch 337: loss 0.706976\n",
      "batch 338: loss 0.583490\n",
      "batch 339: loss 0.519588\n",
      "batch 340: loss 0.614396\n",
      "batch 341: loss 0.380142\n",
      "batch 342: loss 0.682216\n",
      "batch 343: loss 0.472996\n",
      "batch 344: loss 0.470029\n",
      "batch 345: loss 0.529312\n",
      "batch 346: loss 0.657154\n",
      "batch 347: loss 0.704508\n",
      "batch 348: loss 0.481738\n",
      "batch 349: loss 0.689290\n",
      "batch 350: loss 0.602501\n",
      "batch 351: loss 0.510490\n",
      "batch 352: loss 0.551908\n",
      "batch 353: loss 0.442643\n",
      "batch 354: loss 0.618174\n",
      "batch 355: loss 0.442847\n",
      "batch 356: loss 0.671808\n",
      "batch 357: loss 0.599182\n",
      "batch 358: loss 0.480427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 359: loss 0.406115\n",
      "batch 360: loss 0.566716\n",
      "batch 361: loss 0.572099\n",
      "batch 362: loss 0.742430\n",
      "batch 363: loss 0.569983\n",
      "batch 364: loss 0.572757\n",
      "batch 365: loss 0.513316\n",
      "batch 366: loss 0.641713\n",
      "batch 367: loss 0.361921\n",
      "batch 368: loss 0.698060\n",
      "batch 369: loss 0.719322\n",
      "batch 370: loss 0.525427\n",
      "batch 371: loss 0.580642\n",
      "batch 372: loss 0.542786\n",
      "batch 373: loss 0.649420\n",
      "batch 374: loss 0.587308\n",
      "batch 375: loss 0.479136\n",
      "batch 376: loss 0.478934\n",
      "batch 377: loss 0.497809\n",
      "batch 378: loss 0.309522\n",
      "batch 379: loss 0.764512\n",
      "batch 380: loss 0.612215\n",
      "batch 381: loss 0.620390\n",
      "batch 382: loss 0.545985\n",
      "batch 383: loss 0.534761\n",
      "batch 384: loss 0.742777\n",
      "batch 385: loss 0.560495\n",
      "batch 386: loss 0.526278\n",
      "batch 387: loss 0.428880\n",
      "batch 388: loss 0.693090\n",
      "batch 389: loss 0.465271\n",
      "batch 390: loss 0.373078\n",
      "batch 391: loss 0.583335\n",
      "batch 392: loss 0.513403\n",
      "batch 393: loss 0.507378\n",
      "batch 394: loss 0.461447\n",
      "batch 395: loss 0.686804\n",
      "batch 396: loss 0.498673\n",
      "batch 397: loss 0.413858\n",
      "batch 398: loss 0.678915\n",
      "batch 399: loss 0.272286\n",
      "batch 400: loss 0.671525\n",
      "batch 401: loss 0.569706\n",
      "batch 402: loss 0.791501\n",
      "batch 403: loss 0.303074\n",
      "batch 404: loss 0.584662\n",
      "batch 405: loss 0.667629\n",
      "batch 406: loss 0.574150\n",
      "batch 407: loss 0.876126\n",
      "batch 408: loss 0.539455\n",
      "batch 409: loss 0.804955\n",
      "batch 410: loss 0.585975\n",
      "batch 411: loss 0.271438\n",
      "batch 412: loss 0.501149\n",
      "batch 413: loss 0.781223\n",
      "batch 414: loss 0.758961\n",
      "batch 415: loss 0.683941\n",
      "batch 416: loss 0.620368\n",
      "batch 417: loss 0.760399\n",
      "batch 418: loss 0.608351\n",
      "batch 419: loss 0.534518\n",
      "batch 420: loss 0.525424\n",
      "batch 421: loss 0.374538\n",
      "batch 422: loss 0.443934\n",
      "batch 423: loss 0.570107\n",
      "batch 424: loss 0.806731\n",
      "batch 425: loss 0.417533\n",
      "batch 426: loss 0.566703\n",
      "batch 427: loss 0.566490\n",
      "batch 428: loss 0.712521\n",
      "batch 429: loss 0.613534\n",
      "batch 430: loss 0.935867\n",
      "batch 431: loss 0.354117\n",
      "batch 432: loss 0.561814\n",
      "batch 433: loss 0.671831\n",
      "batch 434: loss 0.649711\n",
      "batch 435: loss 0.425636\n",
      "batch 436: loss 0.716892\n",
      "batch 437: loss 0.499356\n",
      "batch 438: loss 0.648685\n",
      "batch 439: loss 0.770507\n",
      "batch 440: loss 0.509976\n",
      "batch 441: loss 0.504785\n",
      "batch 442: loss 0.578438\n",
      "batch 443: loss 0.517588\n",
      "batch 444: loss 0.818270\n",
      "batch 445: loss 0.313542\n",
      "batch 446: loss 0.629781\n",
      "batch 447: loss 0.715666\n",
      "batch 448: loss 0.743777\n",
      "batch 449: loss 0.390805\n",
      "batch 450: loss 0.648731\n",
      "batch 451: loss 0.835699\n",
      "batch 452: loss 0.719341\n",
      "batch 453: loss 0.546558\n",
      "batch 454: loss 0.693905\n",
      "batch 455: loss 0.797009\n",
      "batch 456: loss 0.753742\n",
      "batch 457: loss 0.523381\n",
      "batch 458: loss 0.644074\n",
      "batch 459: loss 0.568025\n",
      "batch 460: loss 0.580554\n",
      "batch 461: loss 0.473352\n",
      "batch 462: loss 0.471123\n",
      "batch 463: loss 0.511576\n",
      "batch 464: loss 0.727182\n",
      "batch 465: loss 0.408897\n",
      "batch 466: loss 0.608799\n",
      "batch 467: loss 0.485134\n",
      "batch 468: loss 0.605942\n",
      "batch 469: loss 0.423211\n",
      "batch 470: loss 0.433421\n",
      "batch 471: loss 0.611324\n",
      "batch 472: loss 0.339864\n",
      "batch 473: loss 0.402209\n",
      "batch 474: loss 0.506629\n",
      "batch 475: loss 0.794778\n",
      "batch 476: loss 0.418370\n",
      "batch 477: loss 0.481555\n",
      "batch 478: loss 0.412770\n",
      "batch 479: loss 0.491476\n",
      "batch 480: loss 0.521690\n",
      "batch 481: loss 0.635019\n",
      "batch 482: loss 0.525724\n",
      "batch 483: loss 0.375562\n",
      "batch 484: loss 0.662696\n",
      "batch 485: loss 0.467991\n",
      "batch 486: loss 0.480841\n",
      "batch 487: loss 0.658552\n",
      "batch 488: loss 0.559221\n",
      "batch 489: loss 0.646287\n",
      "batch 490: loss 0.501436\n",
      "batch 491: loss 0.574885\n",
      "batch 492: loss 0.736178\n",
      "batch 493: loss 0.397849\n",
      "batch 494: loss 0.740351\n",
      "batch 495: loss 0.352216\n",
      "batch 496: loss 0.687329\n",
      "batch 497: loss 0.573962\n",
      "batch 498: loss 0.592102\n",
      "batch 499: loss 0.845595\n",
      "batch 500: loss 0.704264\n",
      "batch 501: loss 0.586535\n",
      "batch 502: loss 0.547235\n",
      "batch 503: loss 0.791466\n",
      "batch 504: loss 0.381151\n",
      "batch 505: loss 0.372555\n",
      "batch 506: loss 0.489192\n",
      "batch 507: loss 0.464136\n",
      "batch 508: loss 0.527142\n",
      "batch 509: loss 0.511576\n",
      "batch 510: loss 0.615576\n",
      "batch 511: loss 0.609174\n",
      "batch 512: loss 0.624272\n",
      "batch 513: loss 0.401555\n",
      "batch 514: loss 0.477912\n",
      "batch 515: loss 0.660083\n",
      "batch 516: loss 0.667855\n",
      "batch 517: loss 0.457364\n",
      "batch 518: loss 0.675960\n",
      "batch 519: loss 0.599604\n",
      "batch 520: loss 0.390547\n",
      "batch 521: loss 0.617930\n",
      "batch 522: loss 0.489461\n",
      "batch 523: loss 0.457302\n",
      "batch 524: loss 0.589331\n",
      "batch 525: loss 0.738410\n",
      "batch 526: loss 0.914649\n",
      "batch 527: loss 0.518623\n",
      "batch 528: loss 0.359642\n",
      "batch 529: loss 0.542796\n",
      "batch 530: loss 0.364321\n",
      "batch 531: loss 0.575255\n",
      "batch 532: loss 0.555690\n",
      "batch 533: loss 0.629280\n",
      "batch 534: loss 0.704244\n",
      "batch 535: loss 0.509854\n",
      "batch 536: loss 0.478232\n",
      "batch 537: loss 0.554796\n",
      "batch 538: loss 0.405293\n",
      "batch 539: loss 0.990865\n",
      "batch 540: loss 0.519657\n",
      "batch 541: loss 0.326317\n",
      "batch 542: loss 0.396081\n",
      "batch 543: loss 0.752775\n",
      "batch 544: loss 0.509237\n",
      "batch 545: loss 0.549821\n",
      "batch 546: loss 0.475252\n",
      "batch 547: loss 0.455437\n",
      "batch 548: loss 0.520995\n",
      "batch 549: loss 0.497677\n",
      "batch 550: loss 0.366137\n",
      "batch 551: loss 0.398888\n",
      "batch 552: loss 0.644732\n",
      "batch 553: loss 0.732146\n",
      "batch 554: loss 0.608794\n",
      "batch 555: loss 0.430311\n",
      "batch 556: loss 0.597345\n",
      "batch 557: loss 0.586574\n",
      "batch 558: loss 0.626383\n",
      "batch 559: loss 0.417687\n",
      "batch 560: loss 0.599530\n",
      "batch 561: loss 0.426873\n",
      "batch 562: loss 0.437313\n",
      "batch 563: loss 0.362871\n",
      "batch 564: loss 0.849080\n",
      "batch 565: loss 0.502500\n",
      "batch 566: loss 0.542848\n",
      "batch 567: loss 0.626048\n",
      "batch 568: loss 0.586164\n",
      "batch 569: loss 0.390868\n",
      "batch 570: loss 0.424421\n",
      "batch 571: loss 0.742271\n",
      "batch 572: loss 0.647530\n",
      "batch 573: loss 0.674584\n",
      "batch 574: loss 0.576518\n",
      "batch 575: loss 0.488300\n",
      "batch 576: loss 0.416547\n",
      "batch 577: loss 0.601521\n",
      "batch 578: loss 0.742827\n",
      "batch 579: loss 0.599152\n",
      "batch 580: loss 0.439792\n",
      "batch 581: loss 0.526059\n",
      "batch 582: loss 0.544301\n",
      "batch 583: loss 0.586521\n",
      "batch 584: loss 0.540702\n",
      "batch 585: loss 0.378392\n",
      "batch 586: loss 0.673900\n",
      "batch 587: loss 0.781877\n",
      "batch 588: loss 0.413823\n",
      "batch 589: loss 0.629191\n",
      "batch 590: loss 0.766303\n",
      "batch 591: loss 0.446013\n",
      "batch 592: loss 0.365924\n",
      "batch 593: loss 0.433479\n",
      "batch 594: loss 0.645523\n",
      "batch 595: loss 0.552134\n",
      "batch 596: loss 0.466637\n",
      "batch 597: loss 0.420312\n",
      "batch 598: loss 0.419531\n",
      "batch 599: loss 0.736694\n",
      "batch 600: loss 0.449065\n",
      "batch 601: loss 0.445142\n",
      "batch 602: loss 0.551631\n",
      "batch 603: loss 0.488327\n",
      "batch 604: loss 0.612907\n",
      "batch 605: loss 0.515056\n",
      "batch 606: loss 0.352636\n",
      "batch 607: loss 0.404745\n",
      "batch 608: loss 0.346541\n",
      "batch 609: loss 0.448923\n",
      "batch 610: loss 0.432494\n",
      "batch 611: loss 0.471635\n",
      "batch 612: loss 0.677311\n",
      "batch 613: loss 0.457326\n",
      "batch 614: loss 0.269517\n",
      "batch 615: loss 0.510186\n",
      "batch 616: loss 0.614176\n",
      "batch 617: loss 0.497628\n",
      "batch 618: loss 0.529329\n",
      "batch 619: loss 0.500346\n",
      "batch 620: loss 0.367815\n",
      "batch 621: loss 0.665532\n",
      "batch 622: loss 0.540739\n",
      "batch 623: loss 0.322739\n",
      "batch 624: loss 0.440697\n",
      "batch 625: loss 0.497972\n",
      "batch 626: loss 0.592096\n",
      "batch 627: loss 0.466892\n",
      "batch 628: loss 0.399116\n",
      "batch 629: loss 0.406458\n",
      "batch 630: loss 0.376699\n",
      "batch 631: loss 0.497329\n",
      "batch 632: loss 0.366223\n",
      "batch 633: loss 0.548736\n",
      "batch 634: loss 0.540624\n",
      "batch 635: loss 0.406926\n",
      "batch 636: loss 0.349657\n",
      "batch 637: loss 0.548427\n",
      "batch 638: loss 0.441167\n",
      "batch 639: loss 0.414072\n",
      "batch 640: loss 0.582282\n",
      "batch 641: loss 0.394096\n",
      "batch 642: loss 0.580949\n",
      "batch 643: loss 0.676846\n",
      "batch 644: loss 0.510827\n",
      "batch 645: loss 0.640317\n",
      "batch 646: loss 0.562635\n",
      "batch 647: loss 0.600178\n",
      "batch 648: loss 0.399170\n",
      "batch 649: loss 0.476355\n",
      "batch 650: loss 0.542607\n",
      "batch 651: loss 0.604087\n",
      "batch 652: loss 0.316208\n",
      "batch 653: loss 0.338078\n",
      "batch 654: loss 0.605886\n",
      "batch 655: loss 0.520474\n",
      "batch 656: loss 0.630322\n",
      "batch 657: loss 0.668662\n",
      "batch 658: loss 0.615487\n",
      "batch 659: loss 0.319008\n",
      "batch 660: loss 0.367661\n",
      "batch 661: loss 0.630882\n",
      "batch 662: loss 0.487129\n",
      "batch 663: loss 0.591029\n",
      "batch 664: loss 0.477105\n",
      "batch 665: loss 0.954591\n",
      "batch 666: loss 0.393916\n",
      "batch 667: loss 0.646947\n",
      "batch 668: loss 0.563672\n",
      "batch 669: loss 0.374459\n",
      "batch 670: loss 0.441379\n",
      "batch 671: loss 0.507805\n",
      "batch 672: loss 0.469265\n",
      "batch 673: loss 0.617509\n",
      "batch 674: loss 0.402374\n",
      "batch 675: loss 0.628617\n",
      "batch 676: loss 0.505691\n",
      "batch 677: loss 0.431900\n",
      "batch 678: loss 0.484411\n",
      "batch 679: loss 0.455789\n",
      "batch 680: loss 0.662476\n",
      "batch 681: loss 0.861489\n",
      "batch 682: loss 0.441371\n",
      "batch 683: loss 0.412150\n",
      "batch 684: loss 0.474757\n",
      "batch 685: loss 0.304654\n",
      "batch 686: loss 0.446507\n",
      "batch 687: loss 0.429720\n",
      "batch 688: loss 0.397419\n",
      "batch 689: loss 0.822730\n",
      "batch 690: loss 0.578256\n",
      "batch 691: loss 0.571055\n",
      "batch 692: loss 0.425554\n",
      "batch 693: loss 0.618896\n",
      "batch 694: loss 0.581622\n",
      "batch 695: loss 0.697580\n",
      "batch 696: loss 0.507017\n",
      "batch 697: loss 0.370108\n",
      "batch 698: loss 0.448909\n",
      "batch 699: loss 0.348555\n",
      "batch 700: loss 0.548321\n",
      "batch 701: loss 0.492427\n",
      "batch 702: loss 0.340538\n",
      "batch 703: loss 0.461639\n",
      "batch 704: loss 0.429301\n",
      "batch 705: loss 0.450972\n",
      "batch 706: loss 0.426424\n",
      "batch 707: loss 0.484771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 708: loss 0.528945\n",
      "batch 709: loss 0.430981\n",
      "batch 710: loss 0.439774\n",
      "batch 711: loss 0.402829\n",
      "batch 712: loss 0.497018\n",
      "batch 713: loss 0.657941\n",
      "batch 714: loss 0.399706\n",
      "batch 715: loss 0.345701\n",
      "batch 716: loss 0.284940\n",
      "batch 717: loss 0.516705\n",
      "batch 718: loss 0.443770\n",
      "batch 719: loss 0.463060\n",
      "batch 720: loss 0.765259\n",
      "batch 721: loss 0.496383\n",
      "batch 722: loss 0.529534\n",
      "batch 723: loss 0.526521\n",
      "batch 724: loss 0.852631\n",
      "batch 725: loss 0.486209\n",
      "batch 726: loss 0.442862\n",
      "batch 727: loss 0.495100\n",
      "batch 728: loss 0.572288\n",
      "batch 729: loss 0.607890\n",
      "batch 730: loss 0.253955\n",
      "batch 731: loss 0.555694\n",
      "batch 732: loss 0.562818\n",
      "batch 733: loss 0.336219\n",
      "batch 734: loss 0.623113\n",
      "batch 735: loss 0.506338\n",
      "batch 736: loss 0.417890\n",
      "batch 737: loss 0.605502\n",
      "batch 738: loss 0.519231\n",
      "batch 739: loss 0.619505\n",
      "batch 740: loss 0.563615\n",
      "batch 741: loss 0.739717\n",
      "batch 742: loss 0.479301\n",
      "batch 743: loss 0.380317\n",
      "batch 744: loss 0.731291\n",
      "batch 745: loss 0.393382\n",
      "batch 746: loss 0.564103\n",
      "batch 747: loss 0.476700\n",
      "batch 748: loss 0.816982\n",
      "batch 749: loss 0.293347\n",
      "batch 750: loss 0.432463\n",
      "batch 751: loss 0.625520\n",
      "batch 752: loss 0.372511\n",
      "batch 753: loss 0.648733\n",
      "batch 754: loss 0.545742\n",
      "batch 755: loss 0.792787\n",
      "batch 756: loss 0.390322\n",
      "batch 757: loss 0.428647\n",
      "batch 758: loss 0.538307\n",
      "batch 759: loss 0.480369\n",
      "batch 760: loss 0.477032\n",
      "batch 761: loss 0.674157\n",
      "batch 762: loss 0.618345\n",
      "batch 763: loss 0.362399\n",
      "batch 764: loss 0.637894\n",
      "batch 765: loss 0.458934\n",
      "batch 766: loss 0.385291\n",
      "batch 767: loss 0.585708\n",
      "batch 768: loss 0.408226\n",
      "batch 769: loss 0.555898\n",
      "batch 770: loss 0.486461\n",
      "batch 771: loss 0.610533\n",
      "batch 772: loss 0.473342\n",
      "batch 773: loss 0.282109\n",
      "batch 774: loss 0.459128\n",
      "batch 775: loss 0.551628\n",
      "batch 776: loss 0.425064\n",
      "batch 777: loss 0.517326\n",
      "batch 778: loss 0.468467\n",
      "batch 779: loss 0.528336\n",
      "batch 780: loss 0.491863\n",
      "batch 781: loss 0.646004\n",
      "batch 782: loss 0.458849\n",
      "batch 783: loss 0.596785\n",
      "batch 784: loss 0.514712\n",
      "batch 785: loss 0.484592\n",
      "batch 786: loss 0.514359\n",
      "batch 787: loss 0.609187\n",
      "batch 788: loss 0.513945\n",
      "batch 789: loss 0.493180\n",
      "batch 790: loss 0.255368\n",
      "batch 791: loss 0.396447\n",
      "batch 792: loss 0.475250\n",
      "batch 793: loss 0.678552\n",
      "batch 794: loss 0.479602\n",
      "batch 795: loss 0.455272\n",
      "batch 796: loss 0.482635\n",
      "batch 797: loss 0.487441\n",
      "batch 798: loss 0.439526\n",
      "batch 799: loss 0.582562\n",
      "batch 800: loss 0.324757\n",
      "batch 801: loss 0.612029\n",
      "batch 802: loss 0.452714\n",
      "batch 803: loss 0.500780\n",
      "batch 804: loss 0.640607\n",
      "batch 805: loss 0.606874\n",
      "batch 806: loss 0.555605\n",
      "batch 807: loss 0.397682\n",
      "batch 808: loss 0.646209\n",
      "batch 809: loss 0.350562\n",
      "batch 810: loss 0.681107\n",
      "batch 811: loss 0.667653\n",
      "batch 812: loss 0.571909\n",
      "batch 813: loss 0.288979\n",
      "batch 814: loss 0.385910\n",
      "batch 815: loss 0.448638\n",
      "batch 816: loss 0.612760\n",
      "batch 817: loss 0.503696\n",
      "batch 818: loss 0.392833\n",
      "batch 819: loss 0.541213\n",
      "batch 820: loss 0.321955\n",
      "batch 821: loss 0.586274\n",
      "batch 822: loss 0.284162\n",
      "batch 823: loss 0.405756\n",
      "batch 824: loss 0.432564\n",
      "batch 825: loss 0.443247\n",
      "batch 826: loss 0.397093\n",
      "batch 827: loss 0.607619\n",
      "batch 828: loss 0.545771\n",
      "batch 829: loss 0.388610\n",
      "batch 830: loss 0.370539\n",
      "batch 831: loss 0.498289\n",
      "batch 832: loss 0.325427\n",
      "batch 833: loss 0.322480\n",
      "batch 834: loss 0.450203\n",
      "batch 835: loss 0.640148\n",
      "batch 836: loss 0.398198\n",
      "batch 837: loss 0.332798\n",
      "batch 838: loss 0.486520\n",
      "batch 839: loss 0.689378\n",
      "batch 840: loss 0.613938\n",
      "batch 841: loss 0.605442\n",
      "batch 842: loss 0.404841\n",
      "batch 843: loss 0.223269\n",
      "batch 844: loss 0.647661\n",
      "batch 845: loss 0.588897\n",
      "batch 846: loss 0.473443\n",
      "batch 847: loss 0.614060\n",
      "batch 848: loss 0.600353\n",
      "batch 849: loss 0.435586\n",
      "batch 850: loss 0.512065\n",
      "batch 851: loss 0.446934\n",
      "batch 852: loss 0.417692\n",
      "batch 853: loss 0.710563\n",
      "batch 854: loss 0.576423\n",
      "batch 855: loss 0.602260\n",
      "batch 856: loss 0.330450\n",
      "batch 857: loss 0.401249\n",
      "batch 858: loss 0.372868\n",
      "batch 859: loss 0.391786\n",
      "batch 860: loss 0.518266\n",
      "batch 861: loss 0.514886\n",
      "batch 862: loss 0.567818\n",
      "batch 863: loss 0.596312\n",
      "batch 864: loss 0.569187\n",
      "batch 865: loss 0.338066\n",
      "batch 866: loss 0.438245\n",
      "batch 867: loss 0.390413\n",
      "batch 868: loss 0.308216\n",
      "batch 869: loss 0.378328\n",
      "batch 870: loss 0.604601\n",
      "batch 871: loss 0.371022\n",
      "batch 872: loss 0.406763\n",
      "batch 873: loss 0.429494\n",
      "batch 874: loss 0.480924\n",
      "batch 875: loss 0.432169\n",
      "batch 876: loss 0.694925\n",
      "batch 877: loss 0.563775\n",
      "batch 878: loss 0.376833\n",
      "batch 879: loss 0.340285\n",
      "batch 880: loss 0.536500\n",
      "batch 881: loss 0.376928\n",
      "batch 882: loss 0.636297\n",
      "batch 883: loss 0.513902\n",
      "batch 884: loss 0.540319\n",
      "batch 885: loss 0.425414\n",
      "batch 886: loss 0.515980\n",
      "batch 887: loss 0.363609\n",
      "batch 888: loss 0.732696\n",
      "batch 889: loss 0.483954\n",
      "batch 890: loss 0.460565\n",
      "batch 891: loss 0.357761\n",
      "batch 892: loss 0.535541\n",
      "batch 893: loss 0.722986\n",
      "batch 894: loss 0.459540\n",
      "batch 895: loss 0.316971\n",
      "batch 896: loss 0.423649\n",
      "batch 897: loss 0.709674\n",
      "batch 898: loss 0.473377\n",
      "batch 899: loss 0.275335\n",
      "batch 900: loss 0.512089\n",
      "batch 901: loss 0.320754\n",
      "batch 902: loss 0.567669\n",
      "batch 903: loss 0.440868\n",
      "batch 904: loss 0.645683\n",
      "batch 905: loss 0.487362\n",
      "batch 906: loss 0.647152\n",
      "batch 907: loss 0.466958\n",
      "batch 908: loss 0.420987\n",
      "batch 909: loss 0.395834\n",
      "batch 910: loss 0.545134\n",
      "batch 911: loss 0.435952\n",
      "batch 912: loss 0.613439\n",
      "batch 913: loss 0.605375\n",
      "batch 914: loss 0.513794\n",
      "batch 915: loss 0.376934\n",
      "batch 916: loss 0.331268\n",
      "batch 917: loss 0.437795\n",
      "batch 918: loss 0.620325\n",
      "batch 919: loss 0.484139\n",
      "batch 920: loss 0.339378\n",
      "batch 921: loss 0.396191\n",
      "batch 922: loss 0.385840\n",
      "batch 923: loss 0.596361\n",
      "batch 924: loss 0.541116\n",
      "batch 925: loss 0.495434\n",
      "batch 926: loss 0.605952\n",
      "batch 927: loss 0.419036\n",
      "batch 928: loss 0.382217\n",
      "batch 929: loss 0.333052\n",
      "batch 930: loss 0.292995\n",
      "batch 931: loss 0.591519\n",
      "batch 932: loss 0.497941\n",
      "batch 933: loss 0.532587\n",
      "batch 934: loss 0.717456\n",
      "batch 935: loss 0.658999\n",
      "batch 936: loss 0.433681\n",
      "batch 937: loss 0.409990\n",
      "batch 938: loss 0.605539\n",
      "batch 939: loss 0.413853\n",
      "batch 940: loss 0.629342\n",
      "batch 941: loss 0.360390\n",
      "batch 942: loss 0.447453\n",
      "batch 943: loss 0.430807\n",
      "batch 944: loss 0.293744\n",
      "batch 945: loss 0.409976\n",
      "batch 946: loss 0.275816\n",
      "batch 947: loss 0.636068\n",
      "batch 948: loss 0.654130\n",
      "batch 949: loss 0.446427\n",
      "batch 950: loss 0.422909\n",
      "batch 951: loss 0.386004\n",
      "batch 952: loss 0.722992\n",
      "batch 953: loss 0.238825\n",
      "batch 954: loss 0.386379\n",
      "batch 955: loss 0.576521\n",
      "batch 956: loss 0.592418\n",
      "batch 957: loss 0.384720\n",
      "batch 958: loss 0.578831\n",
      "batch 959: loss 0.291764\n",
      "batch 960: loss 0.519503\n",
      "batch 961: loss 0.342980\n",
      "batch 962: loss 0.395318\n",
      "batch 963: loss 0.556617\n",
      "batch 964: loss 0.412324\n",
      "batch 965: loss 0.433118\n",
      "batch 966: loss 0.481865\n",
      "batch 967: loss 0.502655\n",
      "batch 968: loss 0.589186\n",
      "batch 969: loss 0.617172\n",
      "batch 970: loss 0.411931\n",
      "batch 971: loss 0.353460\n",
      "batch 972: loss 0.369537\n",
      "batch 973: loss 0.451254\n",
      "batch 974: loss 0.458247\n",
      "batch 975: loss 0.600566\n",
      "batch 976: loss 0.527969\n",
      "batch 977: loss 0.505523\n",
      "batch 978: loss 0.567750\n",
      "batch 979: loss 0.437927\n",
      "batch 980: loss 0.403136\n",
      "batch 981: loss 0.406381\n",
      "batch 982: loss 0.543034\n",
      "batch 983: loss 0.471046\n",
      "batch 984: loss 0.354192\n",
      "batch 985: loss 0.537635\n",
      "batch 986: loss 0.293038\n",
      "batch 987: loss 0.326879\n",
      "batch 988: loss 0.381288\n",
      "batch 989: loss 0.442323\n",
      "batch 990: loss 0.414890\n",
      "batch 991: loss 0.374715\n",
      "batch 992: loss 0.523700\n",
      "batch 993: loss 0.394889\n",
      "batch 994: loss 0.528104\n",
      "batch 995: loss 0.376287\n",
      "batch 996: loss 0.506860\n",
      "batch 997: loss 0.618019\n",
      "batch 998: loss 0.390605\n",
      "batch 999: loss 0.502156\n",
      "batch 1000: loss 0.561188\n",
      "batch 1001: loss 0.392356\n",
      "batch 1002: loss 0.556071\n",
      "batch 1003: loss 0.429951\n",
      "batch 1004: loss 0.373574\n",
      "batch 1005: loss 0.375775\n",
      "batch 1006: loss 0.439967\n",
      "batch 1007: loss 0.507335\n",
      "batch 1008: loss 0.338010\n",
      "batch 1009: loss 0.587254\n",
      "batch 1010: loss 0.411395\n",
      "batch 1011: loss 0.366720\n",
      "batch 1012: loss 0.427787\n",
      "batch 1013: loss 0.393780\n",
      "batch 1014: loss 0.540699\n",
      "batch 1015: loss 0.438409\n",
      "batch 1016: loss 0.839859\n",
      "batch 1017: loss 0.582849\n",
      "batch 1018: loss 0.314778\n",
      "batch 1019: loss 0.287093\n",
      "batch 1020: loss 0.332422\n",
      "batch 1021: loss 0.457118\n",
      "batch 1022: loss 0.404860\n",
      "batch 1023: loss 0.449689\n",
      "batch 1024: loss 0.369224\n",
      "batch 1025: loss 0.602901\n",
      "batch 1026: loss 0.649299\n",
      "batch 1027: loss 0.704875\n",
      "batch 1028: loss 0.447293\n",
      "batch 1029: loss 0.674906\n",
      "batch 1030: loss 0.488755\n",
      "batch 1031: loss 0.488892\n",
      "batch 1032: loss 0.725232\n",
      "batch 1033: loss 0.481194\n",
      "batch 1034: loss 0.515961\n",
      "batch 1035: loss 0.273932\n",
      "batch 1036: loss 0.417575\n",
      "batch 1037: loss 0.484518\n",
      "batch 1038: loss 0.396889\n",
      "batch 1039: loss 0.264899\n",
      "batch 1040: loss 0.452078\n",
      "batch 1041: loss 0.399704\n",
      "batch 1042: loss 0.292060\n",
      "batch 1043: loss 0.632003\n",
      "batch 1044: loss 0.310237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1045: loss 0.414347\n",
      "batch 1046: loss 0.648935\n",
      "batch 1047: loss 0.386628\n",
      "batch 1048: loss 0.419186\n",
      "batch 1049: loss 0.438937\n",
      "batch 1050: loss 0.552920\n",
      "batch 1051: loss 0.396171\n",
      "batch 1052: loss 0.610090\n",
      "batch 1053: loss 0.369565\n",
      "batch 1054: loss 0.432931\n",
      "batch 1055: loss 0.491226\n",
      "batch 1056: loss 0.239867\n",
      "batch 1057: loss 0.611416\n",
      "batch 1058: loss 0.571295\n",
      "batch 1059: loss 0.374221\n",
      "batch 1060: loss 0.388975\n",
      "batch 1061: loss 0.526509\n",
      "batch 1062: loss 0.422878\n",
      "batch 1063: loss 0.392288\n",
      "batch 1064: loss 0.295494\n",
      "batch 1065: loss 0.606242\n",
      "batch 1066: loss 0.299090\n",
      "batch 1067: loss 0.673192\n",
      "batch 1068: loss 0.517196\n",
      "batch 1069: loss 0.691572\n",
      "batch 1070: loss 0.767054\n",
      "batch 1071: loss 0.426581\n",
      "batch 1072: loss 0.564476\n",
      "batch 1073: loss 0.370820\n",
      "batch 1074: loss 0.510492\n",
      "batch 1075: loss 0.463874\n",
      "batch 1076: loss 0.439214\n",
      "batch 1077: loss 0.688979\n",
      "batch 1078: loss 0.608888\n",
      "batch 1079: loss 0.461749\n",
      "batch 1080: loss 0.506123\n",
      "batch 1081: loss 0.579458\n",
      "batch 1082: loss 0.537320\n",
      "batch 1083: loss 0.683306\n",
      "batch 1084: loss 0.491544\n",
      "batch 1085: loss 0.473189\n",
      "batch 1086: loss 0.345885\n",
      "batch 1087: loss 0.413897\n",
      "batch 1088: loss 0.434948\n",
      "batch 1089: loss 0.541643\n",
      "batch 1090: loss 0.405176\n",
      "batch 1091: loss 0.617172\n",
      "batch 1092: loss 0.591098\n",
      "batch 1093: loss 0.515413\n",
      "batch 1094: loss 0.503505\n",
      "batch 1095: loss 0.302751\n",
      "batch 1096: loss 0.460217\n",
      "batch 1097: loss 0.541813\n",
      "batch 1098: loss 0.495193\n",
      "batch 1099: loss 0.341085\n",
      "batch 1100: loss 0.456435\n",
      "batch 1101: loss 0.627783\n",
      "batch 1102: loss 0.350737\n",
      "batch 1103: loss 0.467527\n",
      "batch 1104: loss 0.436685\n",
      "batch 1105: loss 0.297365\n",
      "batch 1106: loss 0.510025\n",
      "batch 1107: loss 0.361402\n",
      "batch 1108: loss 0.412502\n",
      "batch 1109: loss 0.415348\n",
      "batch 1110: loss 0.444964\n",
      "batch 1111: loss 0.326662\n",
      "batch 1112: loss 0.409219\n",
      "batch 1113: loss 0.449725\n",
      "batch 1114: loss 0.401432\n",
      "batch 1115: loss 0.450085\n",
      "batch 1116: loss 0.491939\n",
      "batch 1117: loss 0.525286\n",
      "batch 1118: loss 0.290343\n",
      "batch 1119: loss 0.521561\n",
      "batch 1120: loss 0.528871\n",
      "batch 1121: loss 0.482225\n",
      "batch 1122: loss 0.280790\n",
      "batch 1123: loss 0.476715\n",
      "batch 1124: loss 0.517622\n",
      "batch 1125: loss 0.496195\n",
      "batch 1126: loss 0.333647\n",
      "batch 1127: loss 0.360730\n",
      "batch 1128: loss 0.204125\n",
      "batch 1129: loss 0.375191\n",
      "batch 1130: loss 0.568343\n",
      "batch 1131: loss 0.342955\n",
      "batch 1132: loss 0.302618\n",
      "batch 1133: loss 0.574821\n",
      "batch 1134: loss 0.381047\n",
      "batch 1135: loss 0.606676\n",
      "batch 1136: loss 0.643745\n",
      "batch 1137: loss 0.461692\n",
      "batch 1138: loss 0.356848\n",
      "batch 1139: loss 0.533329\n",
      "batch 1140: loss 0.615633\n",
      "batch 1141: loss 0.561590\n",
      "batch 1142: loss 0.483944\n",
      "batch 1143: loss 0.370109\n",
      "batch 1144: loss 0.347626\n",
      "batch 1145: loss 0.666229\n",
      "batch 1146: loss 0.311211\n",
      "batch 1147: loss 0.376659\n",
      "batch 1148: loss 0.512210\n",
      "batch 1149: loss 0.381154\n",
      "batch 1150: loss 0.406624\n",
      "batch 1151: loss 0.548922\n",
      "batch 1152: loss 0.302110\n",
      "batch 1153: loss 0.612730\n",
      "batch 1154: loss 0.313658\n",
      "batch 1155: loss 0.736952\n",
      "batch 1156: loss 0.308177\n",
      "batch 1157: loss 0.410181\n",
      "batch 1158: loss 0.831160\n",
      "batch 1159: loss 0.604786\n",
      "batch 1160: loss 0.370823\n",
      "batch 1161: loss 0.361173\n",
      "batch 1162: loss 0.429276\n",
      "batch 1163: loss 0.737712\n",
      "batch 1164: loss 0.372653\n",
      "batch 1165: loss 0.282412\n",
      "batch 1166: loss 0.387454\n",
      "batch 1167: loss 0.518475\n",
      "batch 1168: loss 0.470859\n",
      "batch 1169: loss 0.454101\n",
      "batch 1170: loss 0.632451\n",
      "batch 1171: loss 0.387916\n",
      "batch 1172: loss 0.388527\n",
      "batch 1173: loss 0.321338\n",
      "batch 1174: loss 0.445021\n",
      "batch 1175: loss 0.436335\n",
      "batch 1176: loss 0.304193\n",
      "batch 1177: loss 0.385786\n",
      "batch 1178: loss 0.804024\n",
      "batch 1179: loss 0.396238\n",
      "batch 1180: loss 0.292826\n",
      "batch 1181: loss 0.413573\n",
      "batch 1182: loss 0.566190\n",
      "batch 1183: loss 0.305745\n",
      "batch 1184: loss 0.672252\n",
      "batch 1185: loss 0.546611\n",
      "batch 1186: loss 0.513376\n",
      "batch 1187: loss 0.517934\n",
      "batch 1188: loss 0.562010\n",
      "batch 1189: loss 0.531831\n",
      "batch 1190: loss 0.388137\n",
      "batch 1191: loss 0.394074\n",
      "batch 1192: loss 0.365570\n",
      "batch 1193: loss 0.390241\n",
      "batch 1194: loss 0.336287\n",
      "batch 1195: loss 0.395971\n",
      "batch 1196: loss 0.560910\n",
      "batch 1197: loss 0.505508\n",
      "batch 1198: loss 0.331824\n",
      "batch 1199: loss 0.578318\n",
      "batch 1200: loss 0.286077\n",
      "batch 1201: loss 0.437559\n",
      "batch 1202: loss 0.393822\n",
      "batch 1203: loss 0.351274\n",
      "batch 1204: loss 0.464944\n",
      "batch 1205: loss 0.267016\n",
      "batch 1206: loss 0.500577\n",
      "batch 1207: loss 0.490041\n",
      "batch 1208: loss 0.335905\n",
      "batch 1209: loss 0.288298\n",
      "batch 1210: loss 0.506796\n",
      "batch 1211: loss 0.369549\n",
      "batch 1212: loss 0.213823\n",
      "batch 1213: loss 0.499365\n",
      "batch 1214: loss 0.560907\n",
      "batch 1215: loss 0.380515\n",
      "batch 1216: loss 0.499317\n",
      "batch 1217: loss 0.367465\n",
      "batch 1218: loss 0.602168\n",
      "batch 1219: loss 0.362114\n",
      "batch 1220: loss 0.472255\n",
      "batch 1221: loss 0.719137\n",
      "batch 1222: loss 0.543897\n",
      "batch 1223: loss 0.498650\n",
      "batch 1224: loss 0.267142\n",
      "batch 1225: loss 0.421877\n",
      "batch 1226: loss 0.311527\n",
      "batch 1227: loss 0.574672\n",
      "batch 1228: loss 0.323559\n",
      "batch 1229: loss 0.626362\n",
      "batch 1230: loss 0.424285\n",
      "batch 1231: loss 0.455142\n",
      "batch 1232: loss 0.561822\n",
      "batch 1233: loss 0.398603\n",
      "batch 1234: loss 0.543822\n",
      "batch 1235: loss 0.524366\n",
      "batch 1236: loss 0.641348\n",
      "batch 1237: loss 0.407900\n",
      "batch 1238: loss 0.442534\n",
      "batch 1239: loss 0.324388\n",
      "batch 1240: loss 0.321751\n",
      "batch 1241: loss 0.511558\n",
      "batch 1242: loss 0.430576\n",
      "batch 1243: loss 0.529921\n",
      "batch 1244: loss 0.333638\n",
      "batch 1245: loss 0.413304\n",
      "batch 1246: loss 0.490268\n",
      "batch 1247: loss 0.376121\n",
      "batch 1248: loss 0.454504\n",
      "batch 1249: loss 0.332350\n",
      "batch 1250: loss 0.314422\n",
      "batch 1251: loss 0.390270\n",
      "batch 1252: loss 0.435816\n",
      "batch 1253: loss 0.539462\n",
      "batch 1254: loss 0.415842\n",
      "batch 1255: loss 0.431879\n",
      "batch 1256: loss 0.303461\n",
      "batch 1257: loss 0.490778\n",
      "batch 1258: loss 0.353150\n",
      "batch 1259: loss 0.432193\n",
      "batch 1260: loss 0.624830\n",
      "batch 1261: loss 0.296545\n",
      "batch 1262: loss 0.285258\n",
      "batch 1263: loss 0.315485\n",
      "batch 1264: loss 0.334050\n",
      "batch 1265: loss 0.723988\n",
      "batch 1266: loss 0.386699\n",
      "batch 1267: loss 0.493603\n",
      "batch 1268: loss 0.490361\n",
      "batch 1269: loss 0.344039\n",
      "batch 1270: loss 0.526249\n",
      "batch 1271: loss 0.369157\n",
      "batch 1272: loss 0.618279\n",
      "batch 1273: loss 0.379761\n",
      "batch 1274: loss 0.407505\n",
      "batch 1275: loss 0.319559\n",
      "batch 1276: loss 0.407064\n",
      "batch 1277: loss 0.480644\n",
      "batch 1278: loss 0.424139\n",
      "batch 1279: loss 0.367336\n",
      "batch 1280: loss 0.387728\n",
      "batch 1281: loss 0.399981\n",
      "batch 1282: loss 0.348133\n",
      "batch 1283: loss 0.381165\n",
      "batch 1284: loss 0.525158\n",
      "batch 1285: loss 0.390627\n",
      "batch 1286: loss 0.312307\n",
      "batch 1287: loss 0.661790\n",
      "batch 1288: loss 0.529601\n",
      "batch 1289: loss 0.398225\n",
      "batch 1290: loss 0.320377\n",
      "batch 1291: loss 0.588428\n",
      "batch 1292: loss 0.432569\n",
      "batch 1293: loss 0.337866\n",
      "batch 1294: loss 0.441407\n",
      "batch 1295: loss 0.691705\n",
      "batch 1296: loss 0.550091\n",
      "batch 1297: loss 0.359894\n",
      "batch 1298: loss 0.392342\n",
      "batch 1299: loss 0.673120\n",
      "batch 1300: loss 0.401472\n",
      "batch 1301: loss 0.627908\n",
      "batch 1302: loss 0.310921\n",
      "batch 1303: loss 0.474705\n",
      "batch 1304: loss 0.534374\n",
      "batch 1305: loss 0.403577\n",
      "batch 1306: loss 0.323453\n",
      "batch 1307: loss 0.606763\n",
      "batch 1308: loss 0.574553\n",
      "batch 1309: loss 0.451015\n",
      "batch 1310: loss 0.192083\n",
      "batch 1311: loss 0.635147\n",
      "batch 1312: loss 0.558176\n",
      "batch 1313: loss 0.547531\n",
      "batch 1314: loss 0.488003\n",
      "batch 1315: loss 0.432414\n",
      "batch 1316: loss 0.385252\n",
      "batch 1317: loss 0.485584\n",
      "batch 1318: loss 0.389775\n",
      "batch 1319: loss 0.506899\n",
      "batch 1320: loss 0.314082\n",
      "batch 1321: loss 0.280658\n",
      "batch 1322: loss 0.505412\n",
      "batch 1323: loss 0.451059\n",
      "batch 1324: loss 0.262602\n",
      "batch 1325: loss 0.341425\n",
      "batch 1326: loss 0.253412\n",
      "batch 1327: loss 0.475803\n",
      "batch 1328: loss 0.588831\n",
      "batch 1329: loss 0.215943\n",
      "batch 1330: loss 0.380693\n",
      "batch 1331: loss 0.505309\n",
      "batch 1332: loss 0.273250\n",
      "batch 1333: loss 0.335857\n",
      "batch 1334: loss 0.239395\n",
      "batch 1335: loss 0.313946\n",
      "batch 1336: loss 0.438155\n",
      "batch 1337: loss 0.595224\n",
      "batch 1338: loss 0.422388\n",
      "batch 1339: loss 0.449592\n",
      "batch 1340: loss 0.509120\n",
      "batch 1341: loss 0.432048\n",
      "batch 1342: loss 0.395858\n",
      "batch 1343: loss 0.481952\n",
      "batch 1344: loss 0.707371\n",
      "batch 1345: loss 0.512659\n",
      "batch 1346: loss 0.399973\n",
      "batch 1347: loss 0.783278\n",
      "batch 1348: loss 0.417342\n",
      "batch 1349: loss 0.605728\n",
      "batch 1350: loss 0.583915\n",
      "batch 1351: loss 0.413199\n",
      "batch 1352: loss 0.641750\n",
      "batch 1353: loss 0.680709\n",
      "batch 1354: loss 0.295570\n",
      "batch 1355: loss 0.331377\n",
      "batch 1356: loss 0.551701\n",
      "batch 1357: loss 0.440343\n",
      "batch 1358: loss 0.381740\n",
      "batch 1359: loss 0.467718\n",
      "batch 1360: loss 0.674367\n",
      "batch 1361: loss 0.457097\n",
      "batch 1362: loss 0.443760\n",
      "batch 1363: loss 0.490746\n",
      "batch 1364: loss 0.230933\n",
      "batch 1365: loss 0.414622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1366: loss 0.475459\n",
      "batch 1367: loss 0.509099\n",
      "batch 1368: loss 0.426686\n",
      "batch 1369: loss 0.451097\n",
      "batch 1370: loss 0.519520\n",
      "batch 1371: loss 0.419101\n",
      "batch 1372: loss 0.576564\n",
      "batch 1373: loss 0.441977\n",
      "batch 1374: loss 0.303445\n",
      "batch 1375: loss 0.570960\n",
      "batch 1376: loss 0.442122\n",
      "batch 1377: loss 0.423163\n",
      "batch 1378: loss 0.483543\n",
      "batch 1379: loss 0.302292\n",
      "batch 1380: loss 0.360143\n",
      "batch 1381: loss 0.467584\n",
      "batch 1382: loss 0.456898\n",
      "batch 1383: loss 0.578029\n",
      "batch 1384: loss 0.478360\n",
      "batch 1385: loss 0.638917\n",
      "batch 1386: loss 0.316761\n",
      "batch 1387: loss 0.541755\n",
      "batch 1388: loss 0.406918\n",
      "batch 1389: loss 0.385403\n",
      "batch 1390: loss 0.283261\n",
      "batch 1391: loss 0.313505\n",
      "batch 1392: loss 0.247930\n",
      "batch 1393: loss 0.369552\n",
      "batch 1394: loss 0.510989\n",
      "batch 1395: loss 0.375022\n",
      "batch 1396: loss 0.283947\n",
      "batch 1397: loss 0.245324\n",
      "batch 1398: loss 0.416421\n",
      "batch 1399: loss 0.429556\n",
      "batch 1400: loss 0.294875\n",
      "batch 1401: loss 0.437991\n",
      "batch 1402: loss 0.315348\n",
      "batch 1403: loss 0.553218\n",
      "batch 1404: loss 0.413622\n",
      "batch 1405: loss 0.214982\n",
      "batch 1406: loss 0.668389\n",
      "batch 1407: loss 0.431848\n",
      "batch 1408: loss 0.283456\n",
      "batch 1409: loss 0.389457\n",
      "batch 1410: loss 0.312104\n",
      "batch 1411: loss 0.470447\n",
      "batch 1412: loss 0.324813\n",
      "batch 1413: loss 0.555534\n",
      "batch 1414: loss 0.695666\n",
      "batch 1415: loss 0.328772\n",
      "batch 1416: loss 0.337879\n",
      "batch 1417: loss 0.248176\n",
      "batch 1418: loss 0.354217\n",
      "batch 1419: loss 0.622532\n",
      "batch 1420: loss 0.418613\n",
      "batch 1421: loss 0.472030\n",
      "batch 1422: loss 0.355434\n",
      "batch 1423: loss 0.562182\n",
      "batch 1424: loss 0.330250\n",
      "batch 1425: loss 0.428700\n",
      "batch 1426: loss 0.325940\n",
      "batch 1427: loss 0.289468\n",
      "batch 1428: loss 0.527592\n",
      "batch 1429: loss 0.670383\n",
      "batch 1430: loss 0.364150\n",
      "batch 1431: loss 0.471448\n",
      "batch 1432: loss 0.531635\n",
      "batch 1433: loss 0.553994\n",
      "batch 1434: loss 0.606111\n",
      "batch 1435: loss 0.537923\n",
      "batch 1436: loss 0.454012\n",
      "batch 1437: loss 0.412447\n",
      "batch 1438: loss 0.287126\n",
      "batch 1439: loss 0.321455\n",
      "batch 1440: loss 0.313818\n",
      "batch 1441: loss 0.320672\n",
      "batch 1442: loss 0.366687\n",
      "batch 1443: loss 0.324870\n",
      "batch 1444: loss 0.370872\n",
      "batch 1445: loss 0.409065\n",
      "batch 1446: loss 0.476712\n",
      "batch 1447: loss 0.294911\n",
      "batch 1448: loss 0.345980\n",
      "batch 1449: loss 0.422111\n",
      "batch 1450: loss 0.434097\n",
      "batch 1451: loss 0.380687\n",
      "batch 1452: loss 0.501586\n",
      "batch 1453: loss 0.549800\n",
      "batch 1454: loss 0.360415\n",
      "batch 1455: loss 0.849907\n",
      "batch 1456: loss 0.368582\n",
      "batch 1457: loss 0.478976\n",
      "batch 1458: loss 0.255270\n",
      "batch 1459: loss 0.396747\n",
      "batch 1460: loss 0.440957\n",
      "batch 1461: loss 0.457781\n",
      "batch 1462: loss 0.361393\n",
      "batch 1463: loss 0.389439\n",
      "batch 1464: loss 0.397739\n",
      "batch 1465: loss 0.433428\n",
      "batch 1466: loss 0.380191\n",
      "batch 1467: loss 0.561594\n",
      "batch 1468: loss 0.311850\n",
      "batch 1469: loss 0.316496\n",
      "batch 1470: loss 0.485792\n",
      "batch 1471: loss 0.488412\n",
      "batch 1472: loss 0.331355\n",
      "batch 1473: loss 0.494754\n",
      "batch 1474: loss 0.426926\n",
      "batch 1475: loss 0.624908\n",
      "batch 1476: loss 0.267900\n",
      "batch 1477: loss 0.492200\n",
      "batch 1478: loss 0.528670\n",
      "batch 1479: loss 0.324487\n",
      "batch 1480: loss 0.558819\n",
      "batch 1481: loss 0.486575\n",
      "batch 1482: loss 0.548364\n",
      "batch 1483: loss 0.574411\n",
      "batch 1484: loss 0.467101\n",
      "batch 1485: loss 0.225659\n",
      "batch 1486: loss 0.268063\n",
      "batch 1487: loss 0.259539\n",
      "batch 1488: loss 0.371320\n",
      "batch 1489: loss 0.619811\n",
      "batch 1490: loss 0.871585\n",
      "batch 1491: loss 0.288731\n",
      "batch 1492: loss 0.428393\n",
      "batch 1493: loss 0.435327\n",
      "batch 1494: loss 0.513161\n",
      "batch 1495: loss 0.524748\n",
      "batch 1496: loss 0.271327\n",
      "batch 1497: loss 0.332726\n",
      "batch 1498: loss 0.217690\n",
      "batch 1499: loss 0.470478\n",
      "batch 1500: loss 0.716010\n",
      "batch 1501: loss 0.346659\n",
      "batch 1502: loss 0.613968\n",
      "batch 1503: loss 0.450287\n",
      "batch 1504: loss 0.271429\n",
      "batch 1505: loss 0.449195\n",
      "batch 1506: loss 0.438583\n",
      "batch 1507: loss 0.369886\n",
      "batch 1508: loss 0.382352\n",
      "batch 1509: loss 0.229312\n",
      "batch 1510: loss 0.278959\n",
      "batch 1511: loss 0.399933\n",
      "batch 1512: loss 0.391419\n",
      "batch 1513: loss 0.435752\n",
      "batch 1514: loss 0.353225\n",
      "batch 1515: loss 0.464897\n",
      "batch 1516: loss 0.282963\n",
      "batch 1517: loss 0.395505\n",
      "batch 1518: loss 0.335224\n",
      "batch 1519: loss 0.504746\n",
      "batch 1520: loss 0.373999\n",
      "batch 1521: loss 0.556769\n",
      "batch 1522: loss 0.432900\n",
      "batch 1523: loss 0.425741\n",
      "batch 1524: loss 0.447203\n",
      "batch 1525: loss 0.626803\n",
      "batch 1526: loss 0.361300\n",
      "batch 1527: loss 0.502848\n",
      "batch 1528: loss 0.556475\n",
      "batch 1529: loss 0.375306\n",
      "batch 1530: loss 0.569233\n",
      "batch 1531: loss 0.585210\n",
      "batch 1532: loss 0.200076\n",
      "batch 1533: loss 0.338507\n",
      "batch 1534: loss 0.506034\n",
      "batch 1535: loss 0.598014\n",
      "batch 1536: loss 0.367087\n",
      "batch 1537: loss 0.477548\n",
      "batch 1538: loss 0.414260\n",
      "batch 1539: loss 0.162507\n",
      "batch 1540: loss 0.334132\n",
      "batch 1541: loss 0.462358\n",
      "batch 1542: loss 0.391306\n",
      "batch 1543: loss 0.341734\n",
      "batch 1544: loss 0.440682\n",
      "batch 1545: loss 0.222306\n",
      "batch 1546: loss 0.302925\n",
      "batch 1547: loss 0.369901\n",
      "batch 1548: loss 0.357780\n",
      "batch 1549: loss 0.397889\n",
      "batch 1550: loss 0.530525\n",
      "batch 1551: loss 0.232828\n",
      "batch 1552: loss 0.227134\n",
      "batch 1553: loss 0.344088\n",
      "batch 1554: loss 0.408847\n",
      "batch 1555: loss 0.258072\n",
      "batch 1556: loss 0.451503\n",
      "batch 1557: loss 0.155482\n",
      "batch 1558: loss 0.457987\n",
      "batch 1559: loss 0.463595\n",
      "batch 1560: loss 0.381633\n",
      "batch 1561: loss 0.346628\n",
      "batch 1562: loss 0.100568\n",
      "batch 1563: loss 0.400095\n",
      "batch 1564: loss 0.508532\n",
      "batch 1565: loss 0.326764\n",
      "batch 1566: loss 0.349418\n",
      "batch 1567: loss 0.435482\n",
      "batch 1568: loss 0.444497\n",
      "batch 1569: loss 0.360189\n",
      "batch 1570: loss 0.420281\n",
      "batch 1571: loss 0.395109\n",
      "batch 1572: loss 0.445653\n",
      "batch 1573: loss 0.440687\n",
      "batch 1574: loss 0.426695\n",
      "batch 1575: loss 0.417800\n",
      "batch 1576: loss 0.290468\n",
      "batch 1577: loss 0.381711\n",
      "batch 1578: loss 0.579788\n",
      "batch 1579: loss 0.477194\n",
      "batch 1580: loss 0.373044\n",
      "batch 1581: loss 0.551122\n",
      "batch 1582: loss 0.252710\n",
      "batch 1583: loss 0.636446\n",
      "batch 1584: loss 0.459169\n",
      "batch 1585: loss 0.232120\n",
      "batch 1586: loss 0.246700\n",
      "batch 1587: loss 0.287274\n",
      "batch 1588: loss 0.301160\n",
      "batch 1589: loss 0.520047\n",
      "batch 1590: loss 0.373402\n",
      "batch 1591: loss 0.383045\n",
      "batch 1592: loss 0.218802\n",
      "batch 1593: loss 0.302265\n",
      "batch 1594: loss 0.370019\n",
      "batch 1595: loss 0.338267\n",
      "batch 1596: loss 0.268689\n",
      "batch 1597: loss 0.391011\n",
      "batch 1598: loss 0.395061\n",
      "batch 1599: loss 0.411133\n",
      "batch 1600: loss 0.208509\n",
      "batch 1601: loss 0.652363\n",
      "batch 1602: loss 0.288859\n",
      "batch 1603: loss 0.630257\n",
      "batch 1604: loss 0.368365\n",
      "batch 1605: loss 0.732315\n",
      "batch 1606: loss 0.375527\n",
      "batch 1607: loss 0.470008\n",
      "batch 1608: loss 0.423781\n",
      "batch 1609: loss 0.305977\n",
      "batch 1610: loss 0.279649\n",
      "batch 1611: loss 0.324705\n",
      "batch 1612: loss 0.394027\n",
      "batch 1613: loss 0.378241\n",
      "batch 1614: loss 0.313529\n",
      "batch 1615: loss 0.429606\n",
      "batch 1616: loss 0.376219\n",
      "batch 1617: loss 0.362324\n",
      "batch 1618: loss 0.380500\n",
      "batch 1619: loss 0.841888\n",
      "batch 1620: loss 0.272319\n",
      "batch 1621: loss 0.298401\n",
      "batch 1622: loss 0.346640\n",
      "batch 1623: loss 0.368594\n",
      "batch 1624: loss 0.537508\n",
      "batch 1625: loss 0.340881\n",
      "batch 1626: loss 0.364338\n",
      "batch 1627: loss 0.354875\n",
      "batch 1628: loss 0.304420\n",
      "batch 1629: loss 0.476034\n",
      "batch 1630: loss 0.350043\n",
      "batch 1631: loss 0.417797\n",
      "batch 1632: loss 0.464511\n",
      "batch 1633: loss 0.156825\n",
      "batch 1634: loss 0.250541\n",
      "batch 1635: loss 0.465339\n",
      "batch 1636: loss 0.329329\n",
      "batch 1637: loss 0.371369\n",
      "batch 1638: loss 0.417497\n",
      "batch 1639: loss 0.512883\n",
      "batch 1640: loss 0.447768\n",
      "batch 1641: loss 0.291627\n",
      "batch 1642: loss 0.379026\n",
      "batch 1643: loss 0.220393\n",
      "batch 1644: loss 0.540221\n",
      "batch 1645: loss 0.334076\n",
      "batch 1646: loss 0.431276\n",
      "batch 1647: loss 0.327386\n",
      "batch 1648: loss 0.498744\n",
      "batch 1649: loss 0.474028\n",
      "batch 1650: loss 0.482104\n",
      "batch 1651: loss 0.476658\n",
      "batch 1652: loss 0.183514\n",
      "batch 1653: loss 0.658687\n",
      "batch 1654: loss 0.219883\n",
      "batch 1655: loss 0.520422\n",
      "batch 1656: loss 0.262071\n",
      "batch 1657: loss 0.475118\n",
      "batch 1658: loss 0.421448\n",
      "batch 1659: loss 0.476114\n",
      "batch 1660: loss 0.285147\n",
      "batch 1661: loss 0.332245\n",
      "batch 1662: loss 0.683089\n",
      "batch 1663: loss 0.520092\n",
      "batch 1664: loss 0.202456\n",
      "batch 1665: loss 0.518859\n",
      "batch 1666: loss 0.500035\n",
      "batch 1667: loss 0.302271\n",
      "batch 1668: loss 0.572955\n",
      "batch 1669: loss 0.514383\n",
      "batch 1670: loss 0.419878\n",
      "batch 1671: loss 0.349913\n",
      "batch 1672: loss 0.485384\n",
      "batch 1673: loss 0.301811\n",
      "batch 1674: loss 0.449480\n",
      "batch 1675: loss 0.326385\n",
      "batch 1676: loss 0.246106\n",
      "batch 1677: loss 0.361165\n",
      "batch 1678: loss 0.641124\n",
      "batch 1679: loss 0.267817\n",
      "batch 1680: loss 0.318642\n",
      "batch 1681: loss 0.384443\n",
      "batch 1682: loss 0.552871\n",
      "batch 1683: loss 0.373671\n",
      "batch 1684: loss 0.359757\n",
      "batch 1685: loss 0.647282\n",
      "batch 1686: loss 0.619483\n",
      "batch 1687: loss 0.275728\n",
      "batch 1688: loss 0.434985\n",
      "batch 1689: loss 0.420254\n",
      "batch 1690: loss 0.318535\n",
      "batch 1691: loss 0.571523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1692: loss 0.411267\n",
      "batch 1693: loss 0.575654\n",
      "batch 1694: loss 0.422794\n",
      "batch 1695: loss 0.466459\n",
      "batch 1696: loss 0.324839\n",
      "batch 1697: loss 0.414568\n",
      "batch 1698: loss 0.402268\n",
      "batch 1699: loss 0.443381\n",
      "batch 1700: loss 0.603955\n",
      "batch 1701: loss 0.296736\n",
      "batch 1702: loss 0.346278\n",
      "batch 1703: loss 0.462573\n",
      "batch 1704: loss 0.296320\n",
      "batch 1705: loss 0.225401\n",
      "batch 1706: loss 0.423946\n",
      "batch 1707: loss 0.311364\n",
      "batch 1708: loss 0.310355\n",
      "batch 1709: loss 0.221539\n",
      "batch 1710: loss 0.661283\n",
      "batch 1711: loss 0.331966\n",
      "batch 1712: loss 0.333495\n",
      "batch 1713: loss 0.356812\n",
      "batch 1714: loss 0.283094\n",
      "batch 1715: loss 0.289634\n",
      "batch 1716: loss 0.451053\n",
      "batch 1717: loss 0.497461\n",
      "batch 1718: loss 0.630391\n",
      "batch 1719: loss 0.377442\n",
      "batch 1720: loss 0.300124\n",
      "batch 1721: loss 0.338912\n",
      "batch 1722: loss 0.655747\n",
      "batch 1723: loss 0.300997\n",
      "batch 1724: loss 0.257950\n",
      "batch 1725: loss 0.254861\n",
      "batch 1726: loss 0.312760\n",
      "batch 1727: loss 0.283360\n",
      "batch 1728: loss 0.671471\n",
      "batch 1729: loss 0.499702\n",
      "batch 1730: loss 0.719139\n",
      "batch 1731: loss 0.580759\n",
      "batch 1732: loss 0.483092\n",
      "batch 1733: loss 0.390062\n",
      "batch 1734: loss 0.431741\n",
      "batch 1735: loss 0.228920\n",
      "batch 1736: loss 0.409493\n",
      "batch 1737: loss 0.533258\n",
      "batch 1738: loss 0.558667\n",
      "batch 1739: loss 0.445908\n",
      "batch 1740: loss 0.308445\n",
      "batch 1741: loss 0.324503\n",
      "batch 1742: loss 0.307225\n",
      "batch 1743: loss 0.508789\n",
      "batch 1744: loss 0.484834\n",
      "batch 1745: loss 0.608127\n",
      "batch 1746: loss 0.319034\n",
      "batch 1747: loss 0.472029\n",
      "batch 1748: loss 0.430444\n",
      "batch 1749: loss 0.506978\n",
      "batch 1750: loss 0.451645\n",
      "batch 1751: loss 0.346569\n",
      "batch 1752: loss 0.345178\n",
      "batch 1753: loss 0.365579\n",
      "batch 1754: loss 0.527909\n",
      "batch 1755: loss 0.484403\n",
      "batch 1756: loss 0.262138\n",
      "batch 1757: loss 0.302718\n",
      "batch 1758: loss 0.370090\n",
      "batch 1759: loss 0.471557\n",
      "batch 1760: loss 0.423925\n",
      "batch 1761: loss 0.499557\n",
      "batch 1762: loss 0.326799\n",
      "batch 1763: loss 0.344139\n",
      "batch 1764: loss 0.227786\n",
      "batch 1765: loss 0.346916\n",
      "batch 1766: loss 0.244489\n",
      "batch 1767: loss 0.495275\n",
      "batch 1768: loss 0.666132\n",
      "batch 1769: loss 0.251676\n",
      "batch 1770: loss 0.281885\n",
      "batch 1771: loss 0.235416\n",
      "batch 1772: loss 0.441951\n",
      "batch 1773: loss 0.341022\n",
      "batch 1774: loss 0.582605\n",
      "batch 1775: loss 0.499821\n",
      "batch 1776: loss 0.314612\n",
      "batch 1777: loss 0.503825\n",
      "batch 1778: loss 0.527344\n",
      "batch 1779: loss 0.321451\n",
      "batch 1780: loss 0.451645\n",
      "batch 1781: loss 0.455419\n",
      "batch 1782: loss 0.482561\n",
      "batch 1783: loss 0.342791\n",
      "batch 1784: loss 0.111263\n",
      "batch 1785: loss 0.431407\n",
      "batch 1786: loss 0.356356\n",
      "batch 1787: loss 0.613503\n",
      "batch 1788: loss 0.366321\n",
      "batch 1789: loss 0.363925\n",
      "batch 1790: loss 0.492281\n",
      "batch 1791: loss 0.425402\n",
      "batch 1792: loss 0.388882\n",
      "batch 1793: loss 0.419499\n",
      "batch 1794: loss 0.616210\n",
      "batch 1795: loss 0.422531\n",
      "batch 1796: loss 0.381372\n",
      "batch 1797: loss 0.333204\n",
      "batch 1798: loss 0.318460\n",
      "batch 1799: loss 0.341258\n",
      "batch 1800: loss 0.325084\n",
      "batch 1801: loss 0.468689\n",
      "batch 1802: loss 0.579290\n",
      "batch 1803: loss 0.408915\n",
      "batch 1804: loss 0.593714\n",
      "batch 1805: loss 0.399983\n",
      "batch 1806: loss 0.359109\n",
      "batch 1807: loss 0.416958\n",
      "batch 1808: loss 0.417219\n",
      "batch 1809: loss 0.476733\n",
      "batch 1810: loss 0.170892\n",
      "batch 1811: loss 0.323198\n",
      "batch 1812: loss 0.414599\n",
      "batch 1813: loss 0.420963\n",
      "batch 1814: loss 0.444147\n",
      "batch 1815: loss 0.409933\n",
      "batch 1816: loss 0.542268\n",
      "batch 1817: loss 0.317551\n",
      "batch 1818: loss 0.447701\n",
      "batch 1819: loss 0.312201\n",
      "batch 1820: loss 0.481509\n",
      "batch 1821: loss 0.437392\n",
      "batch 1822: loss 0.367612\n",
      "batch 1823: loss 0.202379\n",
      "batch 1824: loss 0.255391\n",
      "batch 1825: loss 0.298567\n",
      "batch 1826: loss 0.424279\n",
      "batch 1827: loss 0.272490\n",
      "batch 1828: loss 0.485747\n",
      "batch 1829: loss 0.327623\n",
      "batch 1830: loss 0.354191\n",
      "batch 1831: loss 0.489954\n",
      "batch 1832: loss 0.518736\n",
      "batch 1833: loss 0.433276\n",
      "batch 1834: loss 0.318728\n",
      "batch 1835: loss 0.319692\n",
      "batch 1836: loss 0.439064\n",
      "batch 1837: loss 0.553637\n",
      "batch 1838: loss 0.193900\n",
      "batch 1839: loss 0.425992\n",
      "batch 1840: loss 0.149186\n",
      "batch 1841: loss 0.512666\n",
      "batch 1842: loss 0.401202\n",
      "batch 1843: loss 0.290929\n",
      "batch 1844: loss 0.383146\n",
      "batch 1845: loss 0.505255\n",
      "batch 1846: loss 0.366152\n",
      "batch 1847: loss 0.347077\n",
      "batch 1848: loss 0.189319\n",
      "batch 1849: loss 0.304302\n",
      "batch 1850: loss 0.685708\n",
      "batch 1851: loss 0.628260\n",
      "batch 1852: loss 0.358314\n",
      "batch 1853: loss 0.365618\n",
      "batch 1854: loss 0.408596\n",
      "batch 1855: loss 0.449814\n",
      "batch 1856: loss 0.280419\n",
      "batch 1857: loss 0.491959\n",
      "batch 1858: loss 0.363220\n",
      "batch 1859: loss 0.341044\n",
      "batch 1860: loss 0.444022\n",
      "batch 1861: loss 0.377273\n",
      "batch 1862: loss 0.469427\n",
      "batch 1863: loss 0.377557\n",
      "batch 1864: loss 0.451155\n",
      "batch 1865: loss 0.505219\n",
      "batch 1866: loss 0.448813\n",
      "batch 1867: loss 0.506689\n",
      "batch 1868: loss 0.315137\n",
      "batch 1869: loss 0.312024\n",
      "batch 1870: loss 0.379499\n",
      "batch 1871: loss 0.420327\n",
      "batch 1872: loss 0.389082\n",
      "batch 1873: loss 0.352892\n",
      "batch 1874: loss 0.481117\n",
      "batch 1875: loss 0.617137\n",
      "batch 1876: loss 0.410853\n",
      "batch 1877: loss 0.445964\n",
      "batch 1878: loss 0.362228\n",
      "batch 1879: loss 0.331166\n",
      "batch 1880: loss 0.528305\n",
      "batch 1881: loss 0.231919\n",
      "batch 1882: loss 0.375747\n",
      "batch 1883: loss 0.316700\n",
      "batch 1884: loss 0.389804\n",
      "batch 1885: loss 0.256472\n",
      "batch 1886: loss 0.223889\n",
      "batch 1887: loss 0.364559\n",
      "batch 1888: loss 0.389740\n",
      "batch 1889: loss 0.643226\n",
      "batch 1890: loss 0.277515\n",
      "batch 1891: loss 0.308113\n",
      "batch 1892: loss 0.420451\n",
      "batch 1893: loss 0.313692\n",
      "batch 1894: loss 0.638653\n",
      "batch 1895: loss 0.335002\n",
      "batch 1896: loss 0.221192\n",
      "batch 1897: loss 0.521143\n",
      "batch 1898: loss 0.324657\n",
      "batch 1899: loss 0.562192\n",
      "batch 1900: loss 0.316458\n",
      "batch 1901: loss 0.446511\n",
      "batch 1902: loss 0.519737\n",
      "batch 1903: loss 0.541207\n",
      "batch 1904: loss 0.287712\n",
      "batch 1905: loss 0.244937\n",
      "batch 1906: loss 0.383897\n",
      "batch 1907: loss 0.368903\n",
      "batch 1908: loss 0.375869\n",
      "batch 1909: loss 0.467094\n",
      "batch 1910: loss 0.245979\n",
      "batch 1911: loss 0.349584\n",
      "batch 1912: loss 0.440514\n",
      "batch 1913: loss 0.718531\n",
      "batch 1914: loss 0.574202\n",
      "batch 1915: loss 0.366466\n",
      "batch 1916: loss 0.253986\n",
      "batch 1917: loss 0.268442\n",
      "batch 1918: loss 0.400574\n",
      "batch 1919: loss 0.288143\n",
      "batch 1920: loss 0.399391\n",
      "batch 1921: loss 0.364309\n",
      "batch 1922: loss 0.245485\n",
      "batch 1923: loss 0.406953\n",
      "batch 1924: loss 0.580045\n",
      "batch 1925: loss 0.367316\n",
      "batch 1926: loss 0.357588\n",
      "batch 1927: loss 0.444536\n",
      "batch 1928: loss 0.515922\n",
      "batch 1929: loss 0.226577\n",
      "batch 1930: loss 0.285060\n",
      "batch 1931: loss 0.362468\n",
      "batch 1932: loss 0.428372\n",
      "batch 1933: loss 0.563609\n",
      "batch 1934: loss 0.447215\n",
      "batch 1935: loss 0.335863\n",
      "batch 1936: loss 0.263869\n",
      "batch 1937: loss 0.377421\n",
      "batch 1938: loss 0.346994\n",
      "batch 1939: loss 0.307919\n",
      "batch 1940: loss 0.333913\n",
      "batch 1941: loss 0.359750\n",
      "batch 1942: loss 0.462815\n",
      "batch 1943: loss 0.424724\n",
      "batch 1944: loss 0.378769\n",
      "batch 1945: loss 0.363048\n",
      "batch 1946: loss 0.217793\n",
      "batch 1947: loss 0.312066\n",
      "batch 1948: loss 0.433788\n",
      "batch 1949: loss 0.421091\n",
      "batch 1950: loss 0.215678\n",
      "batch 1951: loss 0.515364\n",
      "batch 1952: loss 0.641595\n",
      "batch 1953: loss 0.598372\n",
      "batch 1954: loss 0.372773\n",
      "batch 1955: loss 0.396741\n",
      "batch 1956: loss 0.395708\n",
      "batch 1957: loss 0.347650\n",
      "batch 1958: loss 0.402542\n",
      "batch 1959: loss 0.237807\n",
      "batch 1960: loss 0.262788\n",
      "batch 1961: loss 0.323786\n",
      "batch 1962: loss 0.265753\n",
      "batch 1963: loss 0.385989\n",
      "batch 1964: loss 0.318783\n",
      "batch 1965: loss 0.461404\n",
      "batch 1966: loss 0.429150\n",
      "batch 1967: loss 0.485751\n",
      "batch 1968: loss 0.403556\n",
      "batch 1969: loss 0.710960\n",
      "batch 1970: loss 0.229002\n",
      "batch 1971: loss 0.369146\n",
      "batch 1972: loss 0.465546\n",
      "batch 1973: loss 0.205859\n",
      "batch 1974: loss 0.549304\n",
      "batch 1975: loss 0.262033\n",
      "batch 1976: loss 0.166392\n",
      "batch 1977: loss 0.361489\n",
      "batch 1978: loss 0.326911\n",
      "batch 1979: loss 0.600417\n",
      "batch 1980: loss 0.735984\n",
      "batch 1981: loss 0.301893\n",
      "batch 1982: loss 0.391852\n",
      "batch 1983: loss 0.408970\n",
      "batch 1984: loss 0.312220\n",
      "batch 1985: loss 0.258961\n",
      "batch 1986: loss 0.331546\n",
      "batch 1987: loss 0.426294\n",
      "batch 1988: loss 0.341644\n",
      "batch 1989: loss 0.296202\n",
      "batch 1990: loss 0.507998\n",
      "batch 1991: loss 0.425818\n",
      "batch 1992: loss 0.427274\n",
      "batch 1993: loss 0.277036\n",
      "batch 1994: loss 0.266392\n",
      "batch 1995: loss 0.488314\n",
      "batch 1996: loss 0.362617\n",
      "batch 1997: loss 0.489267\n",
      "batch 1998: loss 0.508356\n",
      "batch 1999: loss 0.205696\n",
      "batch 2000: loss 0.367509\n",
      "batch 2001: loss 0.451072\n",
      "batch 2002: loss 0.236537\n",
      "batch 2003: loss 0.278773\n",
      "batch 2004: loss 0.581735\n",
      "batch 2005: loss 0.458743\n",
      "batch 2006: loss 0.374002\n",
      "batch 2007: loss 0.325910\n",
      "batch 2008: loss 0.503228\n",
      "batch 2009: loss 0.411824\n",
      "batch 2010: loss 0.433196\n",
      "batch 2011: loss 0.475113\n",
      "batch 2012: loss 0.261810\n",
      "batch 2013: loss 0.523838\n",
      "batch 2014: loss 0.340179\n",
      "batch 2015: loss 0.352759\n",
      "batch 2016: loss 0.326756\n",
      "batch 2017: loss 0.382759\n",
      "batch 2018: loss 0.506337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2019: loss 0.375340\n",
      "batch 2020: loss 0.309619\n",
      "batch 2021: loss 0.353147\n",
      "batch 2022: loss 0.313131\n",
      "batch 2023: loss 0.395520\n",
      "batch 2024: loss 0.401397\n",
      "batch 2025: loss 0.426099\n",
      "batch 2026: loss 0.230515\n",
      "batch 2027: loss 0.489360\n",
      "batch 2028: loss 0.336970\n",
      "batch 2029: loss 0.377183\n",
      "batch 2030: loss 0.430129\n",
      "batch 2031: loss 0.348536\n",
      "batch 2032: loss 0.205603\n",
      "batch 2033: loss 0.512012\n",
      "batch 2034: loss 0.402339\n",
      "batch 2035: loss 0.401757\n",
      "batch 2036: loss 0.586729\n",
      "batch 2037: loss 0.497157\n",
      "batch 2038: loss 0.351865\n",
      "batch 2039: loss 0.268479\n",
      "batch 2040: loss 0.465948\n",
      "batch 2041: loss 0.447223\n",
      "batch 2042: loss 0.313272\n",
      "batch 2043: loss 0.397131\n",
      "batch 2044: loss 0.452703\n",
      "batch 2045: loss 0.510491\n",
      "batch 2046: loss 0.305883\n",
      "batch 2047: loss 0.367272\n",
      "batch 2048: loss 0.366562\n",
      "batch 2049: loss 0.371648\n",
      "batch 2050: loss 0.306750\n",
      "batch 2051: loss 0.318003\n",
      "batch 2052: loss 0.315848\n",
      "batch 2053: loss 0.501045\n",
      "batch 2054: loss 0.350377\n",
      "batch 2055: loss 0.486145\n",
      "batch 2056: loss 0.543847\n",
      "batch 2057: loss 0.239187\n",
      "batch 2058: loss 0.280582\n",
      "batch 2059: loss 0.368276\n",
      "batch 2060: loss 0.372132\n",
      "batch 2061: loss 0.532489\n",
      "batch 2062: loss 0.418374\n",
      "batch 2063: loss 0.275853\n",
      "batch 2064: loss 0.250984\n",
      "batch 2065: loss 0.341274\n",
      "batch 2066: loss 0.343763\n",
      "batch 2067: loss 0.532615\n",
      "batch 2068: loss 0.384943\n",
      "batch 2069: loss 0.367908\n",
      "batch 2070: loss 0.452058\n",
      "batch 2071: loss 0.370272\n",
      "batch 2072: loss 0.489529\n",
      "batch 2073: loss 0.455344\n",
      "batch 2074: loss 0.496297\n",
      "batch 2075: loss 0.463726\n",
      "batch 2076: loss 0.346731\n",
      "batch 2077: loss 0.370047\n",
      "batch 2078: loss 0.261026\n",
      "batch 2079: loss 0.438001\n",
      "batch 2080: loss 0.291823\n",
      "batch 2081: loss 0.453621\n",
      "batch 2082: loss 0.320880\n",
      "batch 2083: loss 0.409948\n",
      "batch 2084: loss 0.657145\n",
      "batch 2085: loss 0.234562\n",
      "batch 2086: loss 0.331085\n",
      "batch 2087: loss 0.299615\n",
      "batch 2088: loss 0.336394\n",
      "batch 2089: loss 0.124672\n",
      "batch 2090: loss 0.487360\n",
      "batch 2091: loss 0.307639\n",
      "batch 2092: loss 0.262376\n",
      "batch 2093: loss 0.571503\n",
      "batch 2094: loss 0.551583\n",
      "batch 2095: loss 0.421746\n",
      "batch 2096: loss 0.303008\n",
      "batch 2097: loss 0.412331\n",
      "batch 2098: loss 0.431991\n",
      "batch 2099: loss 0.370271\n",
      "batch 2100: loss 0.422730\n",
      "batch 2101: loss 0.202778\n",
      "batch 2102: loss 0.434968\n",
      "batch 2103: loss 0.380792\n",
      "batch 2104: loss 0.323831\n",
      "batch 2105: loss 0.379034\n",
      "batch 2106: loss 0.430111\n",
      "batch 2107: loss 0.229355\n",
      "batch 2108: loss 0.298009\n",
      "batch 2109: loss 0.388991\n",
      "batch 2110: loss 0.252652\n",
      "batch 2111: loss 0.218442\n",
      "batch 2112: loss 0.313392\n",
      "batch 2113: loss 0.529093\n",
      "batch 2114: loss 0.411034\n",
      "batch 2115: loss 0.359282\n",
      "batch 2116: loss 0.420632\n",
      "batch 2117: loss 0.250519\n",
      "batch 2118: loss 0.365854\n",
      "batch 2119: loss 0.296392\n",
      "batch 2120: loss 0.329104\n",
      "batch 2121: loss 0.461828\n",
      "batch 2122: loss 0.392833\n",
      "batch 2123: loss 0.349956\n",
      "batch 2124: loss 0.316159\n",
      "batch 2125: loss 0.510818\n",
      "batch 2126: loss 0.334851\n",
      "batch 2127: loss 0.502749\n",
      "batch 2128: loss 0.254495\n",
      "batch 2129: loss 0.414664\n",
      "batch 2130: loss 0.322628\n",
      "batch 2131: loss 0.476833\n",
      "batch 2132: loss 0.238843\n",
      "batch 2133: loss 0.421409\n",
      "batch 2134: loss 0.411798\n",
      "batch 2135: loss 0.415163\n",
      "batch 2136: loss 0.510437\n",
      "batch 2137: loss 0.564041\n",
      "batch 2138: loss 0.479110\n",
      "batch 2139: loss 0.303450\n",
      "batch 2140: loss 0.316137\n",
      "batch 2141: loss 0.462052\n",
      "batch 2142: loss 0.331018\n",
      "batch 2143: loss 0.321322\n",
      "batch 2144: loss 0.487793\n",
      "batch 2145: loss 0.394523\n",
      "batch 2146: loss 0.215896\n",
      "batch 2147: loss 0.549708\n",
      "batch 2148: loss 0.358921\n",
      "batch 2149: loss 0.261954\n",
      "batch 2150: loss 0.503984\n",
      "batch 2151: loss 0.530986\n",
      "batch 2152: loss 0.347882\n",
      "batch 2153: loss 0.424916\n",
      "batch 2154: loss 0.236946\n",
      "batch 2155: loss 0.379267\n",
      "batch 2156: loss 0.327242\n",
      "batch 2157: loss 0.391177\n",
      "batch 2158: loss 0.346367\n",
      "batch 2159: loss 0.267441\n",
      "batch 2160: loss 0.386232\n",
      "batch 2161: loss 0.329232\n",
      "batch 2162: loss 0.381389\n",
      "batch 2163: loss 0.440764\n",
      "batch 2164: loss 0.267550\n",
      "batch 2165: loss 0.243131\n",
      "batch 2166: loss 0.409417\n",
      "batch 2167: loss 0.296320\n",
      "batch 2168: loss 0.310640\n",
      "batch 2169: loss 0.297714\n",
      "batch 2170: loss 0.277038\n",
      "batch 2171: loss 0.524302\n",
      "batch 2172: loss 0.367934\n",
      "batch 2173: loss 0.450076\n",
      "batch 2174: loss 0.381366\n",
      "batch 2175: loss 0.340229\n",
      "batch 2176: loss 0.293677\n",
      "batch 2177: loss 0.519932\n",
      "batch 2178: loss 0.378482\n",
      "batch 2179: loss 0.651161\n",
      "batch 2180: loss 0.328618\n",
      "batch 2181: loss 0.585552\n",
      "batch 2182: loss 0.379450\n",
      "batch 2183: loss 0.166986\n",
      "batch 2184: loss 0.437815\n",
      "batch 2185: loss 0.229212\n",
      "batch 2186: loss 0.303544\n",
      "batch 2187: loss 0.341349\n",
      "batch 2188: loss 0.480580\n",
      "batch 2189: loss 0.314650\n",
      "batch 2190: loss 0.454072\n",
      "batch 2191: loss 0.570528\n",
      "batch 2192: loss 0.406708\n",
      "batch 2193: loss 0.208749\n",
      "batch 2194: loss 0.556741\n",
      "batch 2195: loss 0.419565\n",
      "batch 2196: loss 0.476912\n",
      "batch 2197: loss 0.341364\n",
      "batch 2198: loss 0.591042\n",
      "batch 2199: loss 0.279547\n",
      "batch 2200: loss 0.418188\n",
      "batch 2201: loss 0.170174\n",
      "batch 2202: loss 0.691030\n",
      "batch 2203: loss 0.417450\n",
      "batch 2204: loss 0.796812\n",
      "batch 2205: loss 0.364736\n",
      "batch 2206: loss 0.399549\n",
      "batch 2207: loss 0.523755\n",
      "batch 2208: loss 0.359472\n",
      "batch 2209: loss 0.295177\n",
      "batch 2210: loss 0.471113\n",
      "batch 2211: loss 0.311030\n",
      "batch 2212: loss 0.421847\n",
      "batch 2213: loss 0.252021\n",
      "batch 2214: loss 0.308455\n",
      "batch 2215: loss 0.257217\n",
      "batch 2216: loss 0.359024\n",
      "batch 2217: loss 0.338334\n",
      "batch 2218: loss 0.301172\n",
      "batch 2219: loss 0.168565\n",
      "batch 2220: loss 0.398862\n",
      "batch 2221: loss 0.442603\n",
      "batch 2222: loss 0.449356\n",
      "batch 2223: loss 0.398587\n",
      "batch 2224: loss 0.538753\n",
      "batch 2225: loss 0.349670\n",
      "batch 2226: loss 0.306908\n",
      "batch 2227: loss 0.361997\n",
      "batch 2228: loss 0.319193\n",
      "batch 2229: loss 0.290177\n",
      "batch 2230: loss 0.331497\n",
      "batch 2231: loss 0.365471\n",
      "batch 2232: loss 0.368911\n",
      "batch 2233: loss 0.332823\n",
      "batch 2234: loss 0.353283\n",
      "batch 2235: loss 0.473957\n",
      "batch 2236: loss 0.375042\n",
      "batch 2237: loss 0.233012\n",
      "batch 2238: loss 0.304688\n",
      "batch 2239: loss 0.486497\n",
      "batch 2240: loss 0.333230\n",
      "batch 2241: loss 0.294662\n",
      "batch 2242: loss 0.386759\n",
      "batch 2243: loss 0.460663\n",
      "batch 2244: loss 0.415217\n",
      "batch 2245: loss 0.355417\n",
      "batch 2246: loss 0.289565\n",
      "batch 2247: loss 0.184652\n",
      "batch 2248: loss 0.214023\n",
      "batch 2249: loss 0.329208\n",
      "batch 2250: loss 0.244414\n",
      "batch 2251: loss 0.354403\n",
      "batch 2252: loss 0.288536\n",
      "batch 2253: loss 0.290077\n",
      "batch 2254: loss 0.358057\n",
      "batch 2255: loss 0.298090\n",
      "batch 2256: loss 0.474754\n",
      "batch 2257: loss 0.197447\n",
      "batch 2258: loss 0.497358\n",
      "batch 2259: loss 0.138949\n",
      "batch 2260: loss 0.424297\n",
      "batch 2261: loss 0.676688\n",
      "batch 2262: loss 0.600915\n",
      "batch 2263: loss 0.526658\n",
      "batch 2264: loss 0.443621\n",
      "batch 2265: loss 0.354614\n",
      "batch 2266: loss 0.188987\n",
      "batch 2267: loss 0.670859\n",
      "batch 2268: loss 0.249695\n",
      "batch 2269: loss 0.440161\n",
      "batch 2270: loss 0.468277\n",
      "batch 2271: loss 0.208786\n",
      "batch 2272: loss 0.289768\n",
      "batch 2273: loss 0.469882\n",
      "batch 2274: loss 0.417235\n",
      "batch 2275: loss 0.505408\n",
      "batch 2276: loss 0.250175\n",
      "batch 2277: loss 0.364558\n",
      "batch 2278: loss 0.245257\n",
      "batch 2279: loss 0.534905\n",
      "batch 2280: loss 0.282519\n",
      "batch 2281: loss 0.402286\n",
      "batch 2282: loss 0.509417\n",
      "batch 2283: loss 0.319931\n",
      "batch 2284: loss 0.273021\n",
      "batch 2285: loss 0.354423\n",
      "batch 2286: loss 0.370611\n",
      "batch 2287: loss 0.463557\n",
      "batch 2288: loss 0.273227\n",
      "batch 2289: loss 0.332373\n",
      "batch 2290: loss 0.338030\n",
      "batch 2291: loss 0.221241\n",
      "batch 2292: loss 0.404646\n",
      "batch 2293: loss 0.359888\n",
      "batch 2294: loss 0.225489\n",
      "batch 2295: loss 0.189467\n",
      "batch 2296: loss 0.277104\n",
      "batch 2297: loss 0.451681\n",
      "batch 2298: loss 0.322968\n",
      "batch 2299: loss 0.351347\n",
      "batch 2300: loss 0.339479\n",
      "batch 2301: loss 0.555306\n",
      "batch 2302: loss 0.233534\n",
      "batch 2303: loss 0.405834\n",
      "batch 2304: loss 0.373058\n",
      "batch 2305: loss 0.349976\n",
      "batch 2306: loss 0.349723\n",
      "batch 2307: loss 0.473903\n",
      "batch 2308: loss 0.270637\n",
      "batch 2309: loss 0.396504\n",
      "batch 2310: loss 0.256367\n",
      "batch 2311: loss 0.379224\n",
      "batch 2312: loss 0.334455\n",
      "batch 2313: loss 0.395659\n",
      "batch 2314: loss 0.364962\n",
      "batch 2315: loss 0.217080\n",
      "batch 2316: loss 0.527729\n",
      "batch 2317: loss 0.499970\n",
      "batch 2318: loss 0.244455\n",
      "batch 2319: loss 0.385314\n",
      "batch 2320: loss 0.467395\n",
      "batch 2321: loss 0.178477\n",
      "batch 2322: loss 0.403456\n",
      "batch 2323: loss 0.369559\n",
      "batch 2324: loss 0.331417\n",
      "batch 2325: loss 0.256147\n",
      "batch 2326: loss 0.444462\n",
      "batch 2327: loss 0.480062\n",
      "batch 2328: loss 0.261787\n",
      "batch 2329: loss 0.292666\n",
      "batch 2330: loss 0.352992\n",
      "batch 2331: loss 0.191097\n",
      "batch 2332: loss 0.391511\n",
      "batch 2333: loss 0.440154\n",
      "batch 2334: loss 0.260329\n",
      "batch 2335: loss 0.257370\n",
      "batch 2336: loss 0.344453\n",
      "batch 2337: loss 0.483452\n",
      "batch 2338: loss 0.396808\n",
      "batch 2339: loss 0.448179\n",
      "batch 2340: loss 0.225658\n",
      "batch 2341: loss 0.335656\n",
      "batch 2342: loss 0.381211\n",
      "batch 2343: loss 0.340420\n",
      "batch 2344: loss 0.308395\n",
      "batch 2345: loss 0.369793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2346: loss 0.264437\n",
      "batch 2347: loss 0.238391\n",
      "batch 2348: loss 0.238617\n",
      "batch 2349: loss 0.579527\n",
      "batch 2350: loss 0.369271\n",
      "batch 2351: loss 0.272259\n",
      "batch 2352: loss 0.497437\n",
      "batch 2353: loss 0.320397\n",
      "batch 2354: loss 0.293907\n",
      "batch 2355: loss 0.355812\n",
      "batch 2356: loss 0.398644\n",
      "batch 2357: loss 0.436400\n",
      "batch 2358: loss 0.550273\n",
      "batch 2359: loss 0.287784\n",
      "batch 2360: loss 0.319919\n",
      "batch 2361: loss 0.398564\n",
      "batch 2362: loss 0.455481\n",
      "batch 2363: loss 0.491780\n",
      "batch 2364: loss 0.309706\n",
      "batch 2365: loss 0.247118\n",
      "batch 2366: loss 0.335454\n",
      "batch 2367: loss 0.380809\n",
      "batch 2368: loss 0.299922\n",
      "batch 2369: loss 0.371437\n",
      "batch 2370: loss 0.445149\n",
      "batch 2371: loss 0.335687\n",
      "batch 2372: loss 0.333688\n",
      "batch 2373: loss 0.384285\n",
      "batch 2374: loss 0.456100\n",
      "batch 2375: loss 0.391714\n",
      "batch 2376: loss 0.393422\n",
      "batch 2377: loss 0.469480\n",
      "batch 2378: loss 0.687497\n",
      "batch 2379: loss 0.342347\n",
      "batch 2380: loss 0.373468\n",
      "batch 2381: loss 0.393066\n",
      "batch 2382: loss 0.379462\n",
      "batch 2383: loss 0.187236\n",
      "batch 2384: loss 0.571780\n",
      "batch 2385: loss 0.306340\n",
      "batch 2386: loss 0.352254\n",
      "batch 2387: loss 0.286499\n",
      "batch 2388: loss 0.459912\n",
      "batch 2389: loss 0.229602\n",
      "batch 2390: loss 0.522708\n",
      "batch 2391: loss 0.328417\n",
      "batch 2392: loss 0.397787\n",
      "batch 2393: loss 0.357342\n",
      "batch 2394: loss 0.385324\n",
      "batch 2395: loss 0.355718\n",
      "batch 2396: loss 0.355356\n",
      "batch 2397: loss 0.404566\n",
      "batch 2398: loss 0.367141\n",
      "batch 2399: loss 0.212114\n",
      "batch 2400: loss 0.412731\n",
      "batch 2401: loss 0.368548\n",
      "batch 2402: loss 0.310460\n",
      "batch 2403: loss 0.440392\n",
      "batch 2404: loss 0.276095\n",
      "batch 2405: loss 0.440193\n",
      "batch 2406: loss 0.379938\n",
      "batch 2407: loss 0.389392\n",
      "batch 2408: loss 0.218538\n",
      "batch 2409: loss 0.246601\n",
      "batch 2410: loss 0.488013\n",
      "batch 2411: loss 0.237742\n",
      "batch 2412: loss 0.285791\n",
      "batch 2413: loss 0.443922\n",
      "batch 2414: loss 0.383013\n",
      "batch 2415: loss 0.173750\n",
      "batch 2416: loss 0.399205\n",
      "batch 2417: loss 0.339646\n",
      "batch 2418: loss 0.544694\n",
      "batch 2419: loss 0.306667\n",
      "batch 2420: loss 0.228359\n",
      "batch 2421: loss 0.397011\n",
      "batch 2422: loss 0.274216\n",
      "batch 2423: loss 0.337671\n",
      "batch 2424: loss 0.405620\n",
      "batch 2425: loss 0.359964\n",
      "batch 2426: loss 0.463653\n",
      "batch 2427: loss 0.482940\n",
      "batch 2428: loss 0.337491\n",
      "batch 2429: loss 0.363299\n",
      "batch 2430: loss 0.363864\n",
      "batch 2431: loss 0.256094\n",
      "batch 2432: loss 0.330984\n",
      "batch 2433: loss 0.523650\n",
      "batch 2434: loss 0.240439\n",
      "batch 2435: loss 0.306467\n",
      "batch 2436: loss 0.416415\n",
      "batch 2437: loss 0.650399\n",
      "batch 2438: loss 0.280933\n",
      "batch 2439: loss 0.288524\n",
      "batch 2440: loss 0.348661\n",
      "batch 2441: loss 0.495689\n",
      "batch 2442: loss 0.343576\n",
      "batch 2443: loss 0.570851\n",
      "batch 2444: loss 0.517009\n",
      "batch 2445: loss 0.213318\n",
      "batch 2446: loss 0.306665\n",
      "batch 2447: loss 0.508677\n",
      "batch 2448: loss 0.318481\n",
      "batch 2449: loss 0.127831\n",
      "batch 2450: loss 0.330669\n",
      "batch 2451: loss 0.501231\n",
      "batch 2452: loss 0.529816\n",
      "batch 2453: loss 0.373341\n",
      "batch 2454: loss 0.403051\n",
      "batch 2455: loss 0.653137\n",
      "batch 2456: loss 0.363001\n",
      "batch 2457: loss 0.425337\n",
      "batch 2458: loss 0.437033\n",
      "batch 2459: loss 0.583635\n",
      "batch 2460: loss 0.495444\n",
      "batch 2461: loss 0.443497\n",
      "batch 2462: loss 0.500135\n",
      "batch 2463: loss 0.235748\n",
      "batch 2464: loss 0.298525\n",
      "batch 2465: loss 0.532982\n",
      "batch 2466: loss 0.352826\n",
      "batch 2467: loss 0.234795\n",
      "batch 2468: loss 0.465466\n",
      "batch 2469: loss 0.519285\n",
      "batch 2470: loss 0.263089\n",
      "batch 2471: loss 0.336993\n",
      "batch 2472: loss 0.340305\n",
      "batch 2473: loss 0.425724\n",
      "batch 2474: loss 0.443872\n",
      "batch 2475: loss 0.392535\n",
      "batch 2476: loss 0.277909\n",
      "batch 2477: loss 0.489170\n",
      "batch 2478: loss 0.497077\n",
      "batch 2479: loss 0.429942\n",
      "batch 2480: loss 0.530352\n",
      "batch 2481: loss 0.466253\n",
      "batch 2482: loss 0.452520\n",
      "batch 2483: loss 0.273149\n",
      "batch 2484: loss 0.322923\n",
      "batch 2485: loss 0.286179\n",
      "batch 2486: loss 0.394276\n",
      "batch 2487: loss 0.328891\n",
      "batch 2488: loss 0.441729\n",
      "batch 2489: loss 0.407201\n",
      "batch 2490: loss 0.245714\n",
      "batch 2491: loss 0.292455\n",
      "batch 2492: loss 0.573093\n",
      "batch 2493: loss 0.555846\n",
      "batch 2494: loss 0.453239\n",
      "batch 2495: loss 0.335826\n",
      "batch 2496: loss 0.374757\n",
      "batch 2497: loss 0.526115\n",
      "batch 2498: loss 0.495797\n",
      "batch 2499: loss 0.285159\n",
      "batch 2500: loss 0.258731\n",
      "batch 2501: loss 0.577608\n",
      "batch 2502: loss 0.406132\n",
      "batch 2503: loss 0.293767\n",
      "batch 2504: loss 0.439866\n",
      "batch 2505: loss 0.423865\n",
      "batch 2506: loss 0.377693\n",
      "batch 2507: loss 0.236402\n",
      "batch 2508: loss 0.452395\n",
      "batch 2509: loss 0.341193\n",
      "batch 2510: loss 0.420078\n",
      "batch 2511: loss 0.383477\n",
      "batch 2512: loss 0.250300\n",
      "batch 2513: loss 0.232976\n",
      "batch 2514: loss 0.476034\n",
      "batch 2515: loss 0.299070\n",
      "batch 2516: loss 0.404242\n",
      "batch 2517: loss 0.312616\n",
      "batch 2518: loss 0.626026\n",
      "batch 2519: loss 0.324708\n",
      "batch 2520: loss 0.540597\n",
      "batch 2521: loss 0.422753\n",
      "batch 2522: loss 0.292000\n",
      "batch 2523: loss 0.309320\n",
      "batch 2524: loss 0.287762\n",
      "batch 2525: loss 0.507568\n",
      "batch 2526: loss 0.349129\n",
      "batch 2527: loss 0.250509\n",
      "batch 2528: loss 0.331200\n",
      "batch 2529: loss 0.421845\n",
      "batch 2530: loss 0.341488\n",
      "batch 2531: loss 0.405340\n",
      "batch 2532: loss 0.527034\n",
      "batch 2533: loss 0.495891\n",
      "batch 2534: loss 0.335972\n",
      "batch 2535: loss 0.411464\n",
      "batch 2536: loss 0.305351\n",
      "batch 2537: loss 0.467126\n",
      "batch 2538: loss 0.259125\n",
      "batch 2539: loss 0.320336\n",
      "batch 2540: loss 0.336916\n",
      "batch 2541: loss 0.245853\n",
      "batch 2542: loss 0.349734\n",
      "batch 2543: loss 0.347375\n",
      "batch 2544: loss 0.238241\n",
      "batch 2545: loss 0.559949\n",
      "batch 2546: loss 0.462719\n",
      "batch 2547: loss 0.282141\n",
      "batch 2548: loss 0.414438\n",
      "batch 2549: loss 0.351376\n",
      "batch 2550: loss 0.445264\n",
      "batch 2551: loss 0.344728\n",
      "batch 2552: loss 0.499236\n",
      "batch 2553: loss 0.157011\n",
      "batch 2554: loss 0.418459\n",
      "batch 2555: loss 0.797677\n",
      "batch 2556: loss 0.199486\n",
      "batch 2557: loss 0.397461\n",
      "batch 2558: loss 0.244989\n",
      "batch 2559: loss 0.430900\n",
      "batch 2560: loss 0.373675\n",
      "batch 2561: loss 0.586485\n",
      "batch 2562: loss 0.204271\n",
      "batch 2563: loss 0.293796\n",
      "batch 2564: loss 0.393875\n",
      "batch 2565: loss 0.333610\n",
      "batch 2566: loss 0.334932\n",
      "batch 2567: loss 0.379997\n",
      "batch 2568: loss 0.344566\n",
      "batch 2569: loss 0.558261\n",
      "batch 2570: loss 0.329971\n",
      "batch 2571: loss 0.301229\n",
      "batch 2572: loss 0.533849\n",
      "batch 2573: loss 0.540988\n",
      "batch 2574: loss 0.369311\n",
      "batch 2575: loss 0.201009\n",
      "batch 2576: loss 0.231226\n",
      "batch 2577: loss 0.396236\n",
      "batch 2578: loss 0.262434\n",
      "batch 2579: loss 0.424368\n",
      "batch 2580: loss 0.326981\n",
      "batch 2581: loss 0.438489\n",
      "batch 2582: loss 0.300497\n",
      "batch 2583: loss 0.593683\n",
      "batch 2584: loss 0.239141\n",
      "batch 2585: loss 0.293702\n",
      "batch 2586: loss 0.330452\n",
      "batch 2587: loss 0.332743\n",
      "batch 2588: loss 0.443353\n",
      "batch 2589: loss 0.429178\n",
      "batch 2590: loss 0.349265\n",
      "batch 2591: loss 0.502166\n",
      "batch 2592: loss 0.291623\n",
      "batch 2593: loss 0.386136\n",
      "batch 2594: loss 0.349607\n",
      "batch 2595: loss 0.832208\n",
      "batch 2596: loss 0.328501\n",
      "batch 2597: loss 0.259276\n",
      "batch 2598: loss 0.409569\n",
      "batch 2599: loss 0.457991\n",
      "batch 2600: loss 0.353854\n",
      "batch 2601: loss 0.538842\n",
      "batch 2602: loss 0.259411\n",
      "batch 2603: loss 0.327233\n",
      "batch 2604: loss 0.441532\n",
      "batch 2605: loss 0.265876\n",
      "batch 2606: loss 0.425659\n",
      "batch 2607: loss 0.581426\n",
      "batch 2608: loss 0.292562\n",
      "batch 2609: loss 0.471160\n",
      "batch 2610: loss 0.281983\n",
      "batch 2611: loss 0.306298\n",
      "batch 2612: loss 0.559447\n",
      "batch 2613: loss 0.604594\n",
      "batch 2614: loss 0.381752\n",
      "batch 2615: loss 0.376933\n",
      "batch 2616: loss 0.521585\n",
      "batch 2617: loss 0.463748\n",
      "batch 2618: loss 0.448373\n",
      "batch 2619: loss 0.369892\n",
      "batch 2620: loss 0.245149\n",
      "batch 2621: loss 0.353437\n",
      "batch 2622: loss 0.363516\n",
      "batch 2623: loss 0.423138\n",
      "batch 2624: loss 0.244540\n",
      "batch 2625: loss 0.379806\n",
      "batch 2626: loss 0.368166\n",
      "batch 2627: loss 0.460393\n",
      "batch 2628: loss 0.498813\n",
      "batch 2629: loss 0.466541\n",
      "batch 2630: loss 0.455529\n",
      "batch 2631: loss 0.263985\n",
      "batch 2632: loss 0.534737\n",
      "batch 2633: loss 0.260437\n",
      "batch 2634: loss 0.322131\n",
      "batch 2635: loss 0.196070\n",
      "batch 2636: loss 0.334641\n",
      "batch 2637: loss 0.681721\n",
      "batch 2638: loss 0.402585\n",
      "batch 2639: loss 0.189335\n",
      "batch 2640: loss 0.327915\n",
      "batch 2641: loss 0.268526\n",
      "batch 2642: loss 0.615230\n",
      "batch 2643: loss 0.435489\n",
      "batch 2644: loss 0.240655\n",
      "batch 2645: loss 0.296681\n",
      "batch 2646: loss 0.459124\n",
      "batch 2647: loss 0.255243\n",
      "batch 2648: loss 0.530791\n",
      "batch 2649: loss 0.443741\n",
      "batch 2650: loss 0.448151\n",
      "batch 2651: loss 0.252092\n",
      "batch 2652: loss 0.371724\n",
      "batch 2653: loss 0.167825\n",
      "batch 2654: loss 0.557718\n",
      "batch 2655: loss 0.439322\n",
      "batch 2656: loss 0.577811\n",
      "batch 2657: loss 0.440135\n",
      "batch 2658: loss 0.302139\n",
      "batch 2659: loss 0.272747\n",
      "batch 2660: loss 0.505490\n",
      "batch 2661: loss 0.365593\n",
      "batch 2662: loss 0.402613\n",
      "batch 2663: loss 0.222665\n",
      "batch 2664: loss 0.209377\n",
      "batch 2665: loss 0.453638\n",
      "batch 2666: loss 0.311624\n",
      "batch 2667: loss 0.162318\n",
      "batch 2668: loss 0.375741\n",
      "batch 2669: loss 0.209887\n",
      "batch 2670: loss 0.346367\n",
      "batch 2671: loss 0.338296\n",
      "batch 2672: loss 0.362162\n",
      "batch 2673: loss 0.344173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2674: loss 0.556553\n",
      "batch 2675: loss 0.253723\n",
      "batch 2676: loss 0.376422\n",
      "batch 2677: loss 0.392447\n",
      "batch 2678: loss 0.490783\n",
      "batch 2679: loss 0.419489\n",
      "batch 2680: loss 0.375210\n",
      "batch 2681: loss 0.344524\n",
      "batch 2682: loss 0.259367\n",
      "batch 2683: loss 0.207390\n",
      "batch 2684: loss 0.431722\n",
      "batch 2685: loss 0.267513\n",
      "batch 2686: loss 0.508803\n",
      "batch 2687: loss 0.449236\n",
      "batch 2688: loss 0.346929\n",
      "batch 2689: loss 0.575158\n",
      "batch 2690: loss 0.320318\n",
      "batch 2691: loss 0.560883\n",
      "batch 2692: loss 0.391922\n",
      "batch 2693: loss 0.376321\n",
      "batch 2694: loss 0.501416\n",
      "batch 2695: loss 0.400498\n",
      "batch 2696: loss 0.315333\n",
      "batch 2697: loss 0.403015\n",
      "batch 2698: loss 0.354772\n",
      "batch 2699: loss 0.335109\n",
      "batch 2700: loss 0.262013\n",
      "batch 2701: loss 0.243106\n",
      "batch 2702: loss 0.303485\n",
      "batch 2703: loss 0.381383\n",
      "batch 2704: loss 0.373259\n",
      "batch 2705: loss 0.394926\n",
      "batch 2706: loss 0.612809\n",
      "batch 2707: loss 0.244523\n",
      "batch 2708: loss 0.263420\n",
      "batch 2709: loss 0.418025\n",
      "batch 2710: loss 0.259016\n",
      "batch 2711: loss 0.558447\n",
      "batch 2712: loss 0.330985\n",
      "batch 2713: loss 0.191647\n",
      "batch 2714: loss 0.302763\n",
      "batch 2715: loss 0.404719\n",
      "batch 2716: loss 0.308935\n",
      "batch 2717: loss 0.498473\n",
      "batch 2718: loss 0.498032\n",
      "batch 2719: loss 0.359772\n",
      "batch 2720: loss 0.411204\n",
      "batch 2721: loss 0.367118\n",
      "batch 2722: loss 0.431353\n",
      "batch 2723: loss 0.194145\n",
      "batch 2724: loss 0.361013\n",
      "batch 2725: loss 0.391977\n",
      "batch 2726: loss 0.317723\n",
      "batch 2727: loss 0.532844\n",
      "batch 2728: loss 0.288520\n",
      "batch 2729: loss 0.436689\n",
      "batch 2730: loss 0.337560\n",
      "batch 2731: loss 0.326593\n",
      "batch 2732: loss 0.277504\n",
      "batch 2733: loss 0.421830\n",
      "batch 2734: loss 0.301514\n",
      "batch 2735: loss 0.395182\n",
      "batch 2736: loss 0.234229\n",
      "batch 2737: loss 0.433183\n",
      "batch 2738: loss 0.225297\n",
      "batch 2739: loss 0.206073\n",
      "batch 2740: loss 0.315083\n",
      "batch 2741: loss 0.265875\n",
      "batch 2742: loss 0.410234\n",
      "batch 2743: loss 0.484717\n",
      "batch 2744: loss 0.254841\n",
      "batch 2745: loss 0.448135\n",
      "batch 2746: loss 0.240421\n",
      "batch 2747: loss 0.259759\n",
      "batch 2748: loss 0.386223\n",
      "batch 2749: loss 0.372622\n",
      "batch 2750: loss 0.230407\n",
      "batch 2751: loss 0.443330\n",
      "batch 2752: loss 0.265083\n",
      "batch 2753: loss 0.352754\n",
      "batch 2754: loss 0.286529\n",
      "batch 2755: loss 0.267128\n",
      "batch 2756: loss 0.306646\n",
      "batch 2757: loss 0.262623\n",
      "batch 2758: loss 0.323090\n",
      "batch 2759: loss 0.354445\n",
      "batch 2760: loss 0.410239\n",
      "batch 2761: loss 0.361148\n",
      "batch 2762: loss 0.296364\n",
      "batch 2763: loss 0.455096\n",
      "batch 2764: loss 0.340015\n",
      "batch 2765: loss 0.342996\n",
      "batch 2766: loss 0.319771\n",
      "batch 2767: loss 0.360751\n",
      "batch 2768: loss 0.305291\n",
      "batch 2769: loss 0.367750\n",
      "batch 2770: loss 0.289198\n",
      "batch 2771: loss 0.360601\n",
      "batch 2772: loss 0.474039\n",
      "batch 2773: loss 0.413388\n",
      "batch 2774: loss 0.365846\n",
      "batch 2775: loss 0.398008\n",
      "batch 2776: loss 0.362539\n",
      "batch 2777: loss 0.358503\n",
      "batch 2778: loss 0.390098\n",
      "batch 2779: loss 0.264380\n",
      "batch 2780: loss 0.205110\n",
      "batch 2781: loss 0.451547\n",
      "batch 2782: loss 0.221276\n",
      "batch 2783: loss 0.296458\n",
      "batch 2784: loss 0.348367\n",
      "batch 2785: loss 0.389670\n",
      "batch 2786: loss 0.355430\n",
      "batch 2787: loss 0.523547\n",
      "batch 2788: loss 0.297698\n",
      "batch 2789: loss 0.231633\n",
      "batch 2790: loss 0.215325\n",
      "batch 2791: loss 0.480841\n",
      "batch 2792: loss 0.250626\n",
      "batch 2793: loss 0.333130\n",
      "batch 2794: loss 0.264177\n",
      "batch 2795: loss 0.450284\n",
      "batch 2796: loss 0.250852\n",
      "batch 2797: loss 0.322296\n",
      "batch 2798: loss 0.341205\n",
      "batch 2799: loss 0.293014\n",
      "batch 2800: loss 0.611606\n",
      "batch 2801: loss 0.325880\n",
      "batch 2802: loss 0.256782\n",
      "batch 2803: loss 0.209174\n",
      "batch 2804: loss 0.469394\n",
      "batch 2805: loss 0.174857\n",
      "batch 2806: loss 0.308902\n",
      "batch 2807: loss 0.373153\n",
      "batch 2808: loss 0.374799\n",
      "batch 2809: loss 0.354017\n",
      "batch 2810: loss 0.434711\n",
      "batch 2811: loss 0.374371\n",
      "batch 2812: loss 0.298483\n",
      "batch 2813: loss 0.420973\n",
      "batch 2814: loss 0.388484\n",
      "batch 2815: loss 0.343811\n",
      "batch 2816: loss 0.421545\n",
      "batch 2817: loss 0.441935\n",
      "batch 2818: loss 0.399537\n",
      "batch 2819: loss 0.623394\n",
      "batch 2820: loss 0.293102\n",
      "batch 2821: loss 0.524686\n",
      "batch 2822: loss 0.486180\n",
      "batch 2823: loss 0.292985\n",
      "batch 2824: loss 0.521931\n",
      "batch 2825: loss 0.426196\n",
      "batch 2826: loss 0.445219\n",
      "batch 2827: loss 0.418287\n",
      "batch 2828: loss 0.465407\n",
      "batch 2829: loss 0.441490\n",
      "batch 2830: loss 0.300918\n",
      "batch 2831: loss 0.447467\n",
      "batch 2832: loss 0.262312\n",
      "batch 2833: loss 0.222019\n",
      "batch 2834: loss 0.551345\n",
      "batch 2835: loss 0.290595\n",
      "batch 2836: loss 0.351810\n",
      "batch 2837: loss 0.704393\n",
      "batch 2838: loss 0.285449\n",
      "batch 2839: loss 0.486808\n",
      "batch 2840: loss 0.205687\n",
      "batch 2841: loss 0.295641\n",
      "batch 2842: loss 0.342119\n",
      "batch 2843: loss 0.340976\n",
      "batch 2844: loss 0.376496\n",
      "batch 2845: loss 0.391339\n",
      "batch 2846: loss 0.436695\n",
      "batch 2847: loss 0.254682\n",
      "batch 2848: loss 0.435135\n",
      "batch 2849: loss 0.448529\n",
      "batch 2850: loss 0.281205\n",
      "batch 2851: loss 0.351170\n",
      "batch 2852: loss 0.491025\n",
      "batch 2853: loss 0.583791\n",
      "batch 2854: loss 0.353448\n",
      "batch 2855: loss 0.470714\n",
      "batch 2856: loss 0.400069\n",
      "batch 2857: loss 0.309323\n",
      "batch 2858: loss 0.294123\n",
      "batch 2859: loss 0.351363\n",
      "batch 2860: loss 0.445720\n",
      "batch 2861: loss 0.487334\n",
      "batch 2862: loss 0.469113\n",
      "batch 2863: loss 0.415645\n",
      "batch 2864: loss 0.345584\n",
      "batch 2865: loss 0.358905\n",
      "batch 2866: loss 0.397147\n",
      "batch 2867: loss 0.490713\n",
      "batch 2868: loss 0.408944\n",
      "batch 2869: loss 0.569590\n",
      "batch 2870: loss 0.363638\n",
      "batch 2871: loss 0.226749\n",
      "batch 2872: loss 0.290898\n",
      "batch 2873: loss 0.634396\n",
      "batch 2874: loss 0.252360\n",
      "batch 2875: loss 0.337199\n",
      "batch 2876: loss 0.526874\n",
      "batch 2877: loss 0.409364\n",
      "batch 2878: loss 0.371485\n",
      "batch 2879: loss 0.429198\n",
      "batch 2880: loss 0.476288\n",
      "batch 2881: loss 0.207235\n",
      "batch 2882: loss 0.231408\n",
      "batch 2883: loss 0.155594\n",
      "batch 2884: loss 0.347890\n",
      "batch 2885: loss 0.266760\n",
      "batch 2886: loss 0.284541\n",
      "batch 2887: loss 0.284348\n",
      "batch 2888: loss 0.284757\n",
      "batch 2889: loss 0.212150\n",
      "batch 2890: loss 0.391022\n",
      "batch 2891: loss 0.424453\n",
      "batch 2892: loss 0.376054\n",
      "batch 2893: loss 0.242833\n",
      "batch 2894: loss 0.304332\n",
      "batch 2895: loss 0.315443\n",
      "batch 2896: loss 0.245429\n",
      "batch 2897: loss 0.363214\n",
      "batch 2898: loss 0.300265\n",
      "batch 2899: loss 0.350814\n",
      "batch 2900: loss 0.482049\n",
      "batch 2901: loss 0.440187\n",
      "batch 2902: loss 0.529498\n",
      "batch 2903: loss 0.209417\n",
      "batch 2904: loss 0.307846\n",
      "batch 2905: loss 0.358504\n",
      "batch 2906: loss 0.485752\n",
      "batch 2907: loss 0.493410\n",
      "batch 2908: loss 0.281585\n",
      "batch 2909: loss 0.339727\n",
      "batch 2910: loss 0.219816\n",
      "batch 2911: loss 0.429941\n",
      "batch 2912: loss 0.254026\n",
      "batch 2913: loss 0.345138\n",
      "batch 2914: loss 0.257980\n",
      "batch 2915: loss 0.241295\n",
      "batch 2916: loss 0.343040\n",
      "batch 2917: loss 0.355443\n",
      "batch 2918: loss 0.290768\n",
      "batch 2919: loss 0.295590\n",
      "batch 2920: loss 0.402368\n",
      "batch 2921: loss 0.252971\n",
      "batch 2922: loss 0.518839\n",
      "batch 2923: loss 0.318066\n",
      "batch 2924: loss 0.489730\n",
      "batch 2925: loss 0.714741\n",
      "batch 2926: loss 0.407796\n",
      "batch 2927: loss 0.372855\n",
      "batch 2928: loss 0.475213\n",
      "batch 2929: loss 0.413953\n",
      "batch 2930: loss 0.558602\n",
      "batch 2931: loss 0.464538\n",
      "batch 2932: loss 0.283580\n",
      "batch 2933: loss 0.452689\n",
      "batch 2934: loss 0.334833\n",
      "batch 2935: loss 0.375211\n",
      "batch 2936: loss 0.202934\n",
      "batch 2937: loss 0.318278\n",
      "batch 2938: loss 0.244083\n",
      "batch 2939: loss 0.221466\n",
      "batch 2940: loss 0.228567\n",
      "batch 2941: loss 0.371189\n",
      "batch 2942: loss 0.311443\n",
      "batch 2943: loss 0.333620\n",
      "batch 2944: loss 0.357149\n",
      "batch 2945: loss 0.429854\n",
      "batch 2946: loss 0.323186\n",
      "batch 2947: loss 0.181361\n",
      "batch 2948: loss 0.351686\n",
      "batch 2949: loss 0.328960\n",
      "batch 2950: loss 0.360822\n",
      "batch 2951: loss 0.513747\n",
      "batch 2952: loss 0.178506\n",
      "batch 2953: loss 0.295837\n",
      "batch 2954: loss 0.372788\n",
      "batch 2955: loss 0.523271\n",
      "batch 2956: loss 0.447555\n",
      "batch 2957: loss 0.424030\n",
      "batch 2958: loss 0.302303\n",
      "batch 2959: loss 0.368683\n",
      "batch 2960: loss 0.244864\n",
      "batch 2961: loss 0.456725\n",
      "batch 2962: loss 0.497469\n",
      "batch 2963: loss 0.387864\n",
      "batch 2964: loss 0.500397\n",
      "batch 2965: loss 0.281607\n",
      "batch 2966: loss 0.607684\n",
      "batch 2967: loss 0.460323\n",
      "batch 2968: loss 0.225196\n",
      "batch 2969: loss 0.424331\n",
      "batch 2970: loss 0.418336\n",
      "batch 2971: loss 0.364674\n",
      "batch 2972: loss 0.431840\n",
      "batch 2973: loss 0.520527\n",
      "batch 2974: loss 0.384733\n",
      "batch 2975: loss 0.214707\n",
      "batch 2976: loss 0.188088\n",
      "batch 2977: loss 0.385374\n",
      "batch 2978: loss 0.488439\n",
      "batch 2979: loss 0.312618\n",
      "batch 2980: loss 0.847117\n",
      "batch 2981: loss 0.352670\n",
      "batch 2982: loss 0.349163\n",
      "batch 2983: loss 0.438567\n",
      "batch 2984: loss 0.610976\n",
      "batch 2985: loss 0.318636\n",
      "batch 2986: loss 0.340891\n",
      "batch 2987: loss 0.384243\n",
      "batch 2988: loss 0.390221\n",
      "batch 2989: loss 0.453287\n",
      "batch 2990: loss 0.333004\n",
      "batch 2991: loss 0.318819\n",
      "batch 2992: loss 0.299063\n",
      "batch 2993: loss 0.640362\n",
      "batch 2994: loss 0.187302\n",
      "batch 2995: loss 0.256219\n",
      "batch 2996: loss 0.256028\n",
      "batch 2997: loss 0.307968\n",
      "batch 2998: loss 0.322861\n",
      "batch 2999: loss 0.274808\n",
      "batch 3000: loss 0.378771\n",
      "batch 3001: loss 0.457504\n",
      "batch 3002: loss 0.229532\n",
      "batch 3003: loss 0.317963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3004: loss 0.400091\n",
      "batch 3005: loss 0.484235\n",
      "batch 3006: loss 0.404848\n",
      "batch 3007: loss 0.286362\n",
      "batch 3008: loss 0.544197\n",
      "batch 3009: loss 0.490992\n",
      "batch 3010: loss 0.400302\n",
      "batch 3011: loss 0.362882\n",
      "batch 3012: loss 0.365509\n",
      "batch 3013: loss 0.363534\n",
      "batch 3014: loss 0.275295\n",
      "batch 3015: loss 0.287581\n",
      "batch 3016: loss 0.396903\n",
      "batch 3017: loss 0.232858\n",
      "batch 3018: loss 0.370770\n",
      "batch 3019: loss 0.264029\n",
      "batch 3020: loss 0.360515\n",
      "batch 3021: loss 0.430296\n",
      "batch 3022: loss 0.260256\n",
      "batch 3023: loss 0.443988\n",
      "batch 3024: loss 0.488968\n",
      "batch 3025: loss 0.507089\n",
      "batch 3026: loss 0.398496\n",
      "batch 3027: loss 0.394489\n",
      "batch 3028: loss 0.414014\n",
      "batch 3029: loss 0.431725\n",
      "batch 3030: loss 0.292123\n",
      "batch 3031: loss 0.434382\n",
      "batch 3032: loss 0.371169\n",
      "batch 3033: loss 0.321883\n",
      "batch 3034: loss 0.418642\n",
      "batch 3035: loss 0.242601\n",
      "batch 3036: loss 0.489504\n",
      "batch 3037: loss 0.231808\n",
      "batch 3038: loss 0.288003\n",
      "batch 3039: loss 0.307887\n",
      "batch 3040: loss 0.451149\n",
      "batch 3041: loss 0.630944\n",
      "batch 3042: loss 0.322231\n",
      "batch 3043: loss 0.231121\n",
      "batch 3044: loss 0.412367\n",
      "batch 3045: loss 0.327501\n",
      "batch 3046: loss 0.374106\n",
      "batch 3047: loss 0.598276\n",
      "batch 3048: loss 0.506188\n",
      "batch 3049: loss 0.524350\n",
      "batch 3050: loss 0.414918\n",
      "batch 3051: loss 0.519873\n",
      "batch 3052: loss 0.351006\n",
      "batch 3053: loss 0.461087\n",
      "batch 3054: loss 0.367886\n",
      "batch 3055: loss 0.232636\n",
      "batch 3056: loss 0.527796\n",
      "batch 3057: loss 0.290728\n",
      "batch 3058: loss 0.317053\n",
      "batch 3059: loss 0.228674\n",
      "batch 3060: loss 0.439571\n",
      "batch 3061: loss 0.286419\n",
      "batch 3062: loss 0.526534\n",
      "batch 3063: loss 0.207262\n",
      "batch 3064: loss 0.379992\n",
      "batch 3065: loss 0.316135\n",
      "batch 3066: loss 0.287199\n",
      "batch 3067: loss 0.305850\n",
      "batch 3068: loss 0.362130\n",
      "batch 3069: loss 0.482680\n",
      "batch 3070: loss 0.377135\n",
      "batch 3071: loss 0.588318\n",
      "batch 3072: loss 0.494670\n",
      "batch 3073: loss 0.251864\n",
      "batch 3074: loss 0.379006\n",
      "batch 3075: loss 0.305263\n",
      "batch 3076: loss 0.368260\n",
      "batch 3077: loss 0.524819\n",
      "batch 3078: loss 0.234694\n",
      "batch 3079: loss 0.386956\n",
      "batch 3080: loss 0.253068\n",
      "batch 3081: loss 0.382111\n",
      "batch 3082: loss 0.153182\n",
      "batch 3083: loss 0.390474\n",
      "batch 3084: loss 0.446912\n",
      "batch 3085: loss 0.266181\n",
      "batch 3086: loss 0.346713\n",
      "batch 3087: loss 0.255740\n",
      "batch 3088: loss 0.266543\n",
      "batch 3089: loss 0.284313\n",
      "batch 3090: loss 0.335497\n",
      "batch 3091: loss 0.261019\n",
      "batch 3092: loss 0.392914\n",
      "batch 3093: loss 0.176484\n",
      "batch 3094: loss 0.267411\n",
      "batch 3095: loss 0.372671\n",
      "batch 3096: loss 0.237959\n",
      "batch 3097: loss 0.398009\n",
      "batch 3098: loss 0.300480\n",
      "batch 3099: loss 0.314110\n",
      "batch 3100: loss 0.468444\n",
      "batch 3101: loss 0.537027\n",
      "batch 3102: loss 0.397625\n",
      "batch 3103: loss 0.378400\n",
      "batch 3104: loss 0.446152\n",
      "batch 3105: loss 0.291618\n",
      "batch 3106: loss 0.438085\n",
      "batch 3107: loss 0.464652\n",
      "batch 3108: loss 0.281515\n",
      "batch 3109: loss 0.317632\n",
      "batch 3110: loss 0.332386\n",
      "batch 3111: loss 0.417001\n",
      "batch 3112: loss 0.323514\n",
      "batch 3113: loss 0.290922\n",
      "batch 3114: loss 0.309548\n",
      "batch 3115: loss 0.408554\n",
      "batch 3116: loss 0.389319\n",
      "batch 3117: loss 0.345845\n",
      "batch 3118: loss 0.314845\n",
      "batch 3119: loss 0.296973\n",
      "batch 3120: loss 0.262381\n",
      "batch 3121: loss 0.520105\n",
      "batch 3122: loss 0.333381\n",
      "batch 3123: loss 0.292582\n",
      "batch 3124: loss 0.225139\n",
      "batch 3125: loss 0.341834\n",
      "batch 3126: loss 0.424334\n",
      "batch 3127: loss 0.319659\n",
      "batch 3128: loss 0.392161\n",
      "batch 3129: loss 0.167886\n",
      "batch 3130: loss 0.605157\n",
      "batch 3131: loss 0.270106\n",
      "batch 3132: loss 0.274591\n",
      "batch 3133: loss 0.312550\n",
      "batch 3134: loss 0.693876\n",
      "batch 3135: loss 0.400346\n",
      "batch 3136: loss 0.288606\n",
      "batch 3137: loss 0.247869\n",
      "batch 3138: loss 0.309313\n",
      "batch 3139: loss 0.406686\n",
      "batch 3140: loss 0.333182\n",
      "batch 3141: loss 0.200237\n",
      "batch 3142: loss 0.292593\n",
      "batch 3143: loss 0.319048\n",
      "batch 3144: loss 0.249649\n",
      "batch 3145: loss 0.357646\n",
      "batch 3146: loss 0.475056\n",
      "batch 3147: loss 0.445561\n",
      "batch 3148: loss 0.221419\n",
      "batch 3149: loss 0.316331\n",
      "batch 3150: loss 0.319248\n",
      "batch 3151: loss 0.315866\n",
      "batch 3152: loss 0.356294\n",
      "batch 3153: loss 0.248225\n",
      "batch 3154: loss 0.354033\n",
      "batch 3155: loss 0.289420\n",
      "batch 3156: loss 0.273762\n",
      "batch 3157: loss 0.415605\n",
      "batch 3158: loss 0.287456\n",
      "batch 3159: loss 0.398815\n",
      "batch 3160: loss 0.224378\n",
      "batch 3161: loss 0.482756\n",
      "batch 3162: loss 0.388309\n",
      "batch 3163: loss 0.290349\n",
      "batch 3164: loss 0.381646\n",
      "batch 3165: loss 0.238496\n",
      "batch 3166: loss 0.541987\n",
      "batch 3167: loss 0.401107\n",
      "batch 3168: loss 0.395241\n",
      "batch 3169: loss 0.386658\n",
      "batch 3170: loss 0.263690\n",
      "batch 3171: loss 0.360938\n",
      "batch 3172: loss 0.313423\n",
      "batch 3173: loss 0.348240\n",
      "batch 3174: loss 0.374245\n",
      "batch 3175: loss 0.409311\n",
      "batch 3176: loss 0.449144\n",
      "batch 3177: loss 0.343568\n",
      "batch 3178: loss 0.406442\n",
      "batch 3179: loss 0.592973\n",
      "batch 3180: loss 0.413187\n",
      "batch 3181: loss 0.273681\n",
      "batch 3182: loss 0.220497\n",
      "batch 3183: loss 0.699808\n",
      "batch 3184: loss 0.306864\n",
      "batch 3185: loss 0.379391\n",
      "batch 3186: loss 0.309989\n",
      "batch 3187: loss 0.297933\n",
      "batch 3188: loss 0.336016\n",
      "batch 3189: loss 0.218072\n",
      "batch 3190: loss 0.243170\n",
      "batch 3191: loss 0.325101\n",
      "batch 3192: loss 0.253572\n",
      "batch 3193: loss 0.226752\n",
      "batch 3194: loss 0.268525\n",
      "batch 3195: loss 0.282895\n",
      "batch 3196: loss 0.470678\n",
      "batch 3197: loss 0.208677\n",
      "batch 3198: loss 0.314021\n",
      "batch 3199: loss 0.575111\n",
      "batch 3200: loss 0.419245\n",
      "batch 3201: loss 0.275231\n",
      "batch 3202: loss 0.335382\n",
      "batch 3203: loss 0.337860\n",
      "batch 3204: loss 0.291129\n",
      "batch 3205: loss 0.402741\n",
      "batch 3206: loss 0.406809\n",
      "batch 3207: loss 0.259016\n",
      "batch 3208: loss 0.366701\n",
      "batch 3209: loss 0.217099\n",
      "batch 3210: loss 0.491117\n",
      "batch 3211: loss 0.482563\n",
      "batch 3212: loss 0.333458\n",
      "batch 3213: loss 0.381805\n",
      "batch 3214: loss 0.340044\n",
      "batch 3215: loss 0.222781\n",
      "batch 3216: loss 0.183359\n",
      "batch 3217: loss 0.705311\n",
      "batch 3218: loss 0.254372\n",
      "batch 3219: loss 0.644271\n",
      "batch 3220: loss 0.319601\n",
      "batch 3221: loss 0.495080\n",
      "batch 3222: loss 0.400561\n",
      "batch 3223: loss 0.426920\n",
      "batch 3224: loss 0.397762\n",
      "batch 3225: loss 0.459108\n",
      "batch 3226: loss 0.203534\n",
      "batch 3227: loss 0.353895\n",
      "batch 3228: loss 0.384879\n",
      "batch 3229: loss 0.317833\n",
      "batch 3230: loss 0.272033\n",
      "batch 3231: loss 0.221475\n",
      "batch 3232: loss 0.468369\n",
      "batch 3233: loss 0.343065\n",
      "batch 3234: loss 0.442110\n",
      "batch 3235: loss 0.556107\n",
      "batch 3236: loss 0.323105\n",
      "batch 3237: loss 0.338195\n",
      "batch 3238: loss 0.234621\n",
      "batch 3239: loss 0.421439\n",
      "batch 3240: loss 0.314797\n",
      "batch 3241: loss 0.368864\n",
      "batch 3242: loss 0.485043\n",
      "batch 3243: loss 0.443291\n",
      "batch 3244: loss 0.261979\n",
      "batch 3245: loss 0.401160\n",
      "batch 3246: loss 0.426518\n",
      "batch 3247: loss 0.292335\n",
      "batch 3248: loss 0.420174\n",
      "batch 3249: loss 0.278764\n",
      "batch 3250: loss 0.286016\n",
      "batch 3251: loss 0.366019\n",
      "batch 3252: loss 0.280050\n",
      "batch 3253: loss 0.209320\n",
      "batch 3254: loss 0.378410\n",
      "batch 3255: loss 0.440474\n",
      "batch 3256: loss 0.247054\n",
      "batch 3257: loss 0.109772\n",
      "batch 3258: loss 0.394623\n",
      "batch 3259: loss 0.287203\n",
      "batch 3260: loss 0.259515\n",
      "batch 3261: loss 0.287860\n",
      "batch 3262: loss 0.440401\n",
      "batch 3263: loss 0.515189\n",
      "batch 3264: loss 0.397712\n",
      "batch 3265: loss 0.362996\n",
      "batch 3266: loss 0.353833\n",
      "batch 3267: loss 0.420187\n",
      "batch 3268: loss 0.268487\n",
      "batch 3269: loss 0.557744\n",
      "batch 3270: loss 0.348511\n",
      "batch 3271: loss 0.429818\n",
      "batch 3272: loss 0.283463\n",
      "batch 3273: loss 0.266979\n",
      "batch 3274: loss 0.193096\n",
      "batch 3275: loss 0.349780\n",
      "batch 3276: loss 0.472589\n",
      "batch 3277: loss 0.355624\n",
      "batch 3278: loss 0.406982\n",
      "batch 3279: loss 0.216644\n",
      "batch 3280: loss 0.391060\n",
      "batch 3281: loss 0.337942\n",
      "batch 3282: loss 0.393753\n",
      "batch 3283: loss 0.196204\n",
      "batch 3284: loss 0.207260\n",
      "batch 3285: loss 0.564288\n",
      "batch 3286: loss 0.244531\n",
      "batch 3287: loss 0.371132\n",
      "batch 3288: loss 0.400999\n",
      "batch 3289: loss 0.317613\n",
      "batch 3290: loss 0.385426\n",
      "batch 3291: loss 0.336331\n",
      "batch 3292: loss 0.447840\n",
      "batch 3293: loss 0.552845\n",
      "batch 3294: loss 0.520893\n",
      "batch 3295: loss 0.319253\n",
      "batch 3296: loss 0.397904\n",
      "batch 3297: loss 0.423692\n",
      "batch 3298: loss 0.325349\n",
      "batch 3299: loss 0.316728\n",
      "batch 3300: loss 0.288232\n",
      "batch 3301: loss 0.327357\n",
      "batch 3302: loss 0.310955\n",
      "batch 3303: loss 0.433522\n",
      "batch 3304: loss 0.389733\n",
      "batch 3305: loss 0.361438\n",
      "batch 3306: loss 0.244161\n",
      "batch 3307: loss 0.310586\n",
      "batch 3308: loss 0.251656\n",
      "batch 3309: loss 0.442969\n",
      "batch 3310: loss 0.463136\n",
      "batch 3311: loss 0.343146\n",
      "batch 3312: loss 0.341274\n",
      "batch 3313: loss 0.320679\n",
      "batch 3314: loss 0.330102\n",
      "batch 3315: loss 0.318275\n",
      "batch 3316: loss 0.273878\n",
      "batch 3317: loss 0.592145\n",
      "batch 3318: loss 0.379468\n",
      "batch 3319: loss 0.208892\n",
      "batch 3320: loss 0.309847\n",
      "batch 3321: loss 0.441987\n",
      "batch 3322: loss 0.188753\n",
      "batch 3323: loss 0.450277\n",
      "batch 3324: loss 0.228652\n",
      "batch 3325: loss 0.395201\n",
      "batch 3326: loss 0.432941\n",
      "batch 3327: loss 0.226751\n",
      "batch 3328: loss 0.318687\n",
      "batch 3329: loss 0.295649\n",
      "batch 3330: loss 0.444879\n",
      "batch 3331: loss 0.458032\n",
      "batch 3332: loss 0.257723\n",
      "batch 3333: loss 0.371182\n",
      "batch 3334: loss 0.486966\n",
      "batch 3335: loss 0.416883\n",
      "batch 3336: loss 0.471569\n",
      "batch 3337: loss 0.336066\n",
      "batch 3338: loss 0.372382\n",
      "batch 3339: loss 0.297720\n",
      "batch 3340: loss 0.241750\n",
      "batch 3341: loss 0.422835\n",
      "batch 3342: loss 0.259967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3343: loss 0.190121\n",
      "batch 3344: loss 0.510209\n",
      "batch 3345: loss 0.355574\n",
      "batch 3346: loss 0.285199\n",
      "batch 3347: loss 0.346809\n",
      "batch 3348: loss 0.258272\n",
      "batch 3349: loss 0.346934\n",
      "batch 3350: loss 0.457260\n",
      "batch 3351: loss 0.499169\n",
      "batch 3352: loss 0.146534\n",
      "batch 3353: loss 0.663676\n",
      "batch 3354: loss 0.442636\n",
      "batch 3355: loss 0.215107\n",
      "batch 3356: loss 0.290177\n",
      "batch 3357: loss 0.437650\n",
      "batch 3358: loss 0.335038\n",
      "batch 3359: loss 0.373917\n",
      "batch 3360: loss 0.327038\n",
      "batch 3361: loss 0.341715\n",
      "batch 3362: loss 0.350452\n",
      "batch 3363: loss 0.208342\n",
      "batch 3364: loss 0.312156\n",
      "batch 3365: loss 0.361146\n",
      "batch 3366: loss 0.476161\n",
      "batch 3367: loss 0.301190\n",
      "batch 3368: loss 0.203891\n",
      "batch 3369: loss 0.290733\n",
      "batch 3370: loss 0.542623\n",
      "batch 3371: loss 0.476892\n",
      "batch 3372: loss 0.319938\n",
      "batch 3373: loss 0.418613\n",
      "batch 3374: loss 0.358793\n",
      "batch 3375: loss 0.327057\n",
      "batch 3376: loss 0.440664\n",
      "batch 3377: loss 0.413186\n",
      "batch 3378: loss 0.481915\n",
      "batch 3379: loss 0.442701\n",
      "batch 3380: loss 0.289548\n",
      "batch 3381: loss 0.341906\n",
      "batch 3382: loss 0.479913\n",
      "batch 3383: loss 0.508884\n",
      "batch 3384: loss 0.474171\n",
      "batch 3385: loss 0.283801\n",
      "batch 3386: loss 0.431865\n",
      "batch 3387: loss 0.479614\n",
      "batch 3388: loss 0.359887\n",
      "batch 3389: loss 0.302103\n",
      "batch 3390: loss 0.414280\n",
      "batch 3391: loss 0.302580\n",
      "batch 3392: loss 0.346818\n",
      "batch 3393: loss 0.308609\n",
      "batch 3394: loss 0.244547\n",
      "batch 3395: loss 0.404258\n",
      "batch 3396: loss 0.346893\n",
      "batch 3397: loss 0.271029\n",
      "batch 3398: loss 0.233868\n",
      "batch 3399: loss 0.259010\n",
      "batch 3400: loss 0.593667\n",
      "batch 3401: loss 0.313955\n",
      "batch 3402: loss 0.455549\n",
      "batch 3403: loss 0.332709\n",
      "batch 3404: loss 0.236092\n",
      "batch 3405: loss 0.460975\n",
      "batch 3406: loss 0.378778\n",
      "batch 3407: loss 0.234165\n",
      "batch 3408: loss 0.322780\n",
      "batch 3409: loss 0.495148\n",
      "batch 3410: loss 0.162845\n",
      "batch 3411: loss 0.414521\n",
      "batch 3412: loss 0.223858\n",
      "batch 3413: loss 0.365495\n",
      "batch 3414: loss 0.463061\n",
      "batch 3415: loss 0.423485\n",
      "batch 3416: loss 0.386911\n",
      "batch 3417: loss 0.351368\n",
      "batch 3418: loss 0.353798\n",
      "batch 3419: loss 0.333857\n",
      "batch 3420: loss 0.410300\n",
      "batch 3421: loss 0.293929\n",
      "batch 3422: loss 0.419276\n",
      "batch 3423: loss 0.310212\n",
      "batch 3424: loss 0.370041\n",
      "batch 3425: loss 0.439854\n",
      "batch 3426: loss 0.227743\n",
      "batch 3427: loss 0.224318\n",
      "batch 3428: loss 0.265978\n",
      "batch 3429: loss 0.313205\n",
      "batch 3430: loss 0.347270\n",
      "batch 3431: loss 0.208900\n",
      "batch 3432: loss 0.282366\n",
      "batch 3433: loss 0.297368\n",
      "batch 3434: loss 0.274873\n",
      "batch 3435: loss 0.307774\n",
      "batch 3436: loss 0.360583\n",
      "batch 3437: loss 0.251969\n",
      "batch 3438: loss 0.336325\n",
      "batch 3439: loss 0.230168\n",
      "batch 3440: loss 0.403809\n",
      "batch 3441: loss 0.643894\n",
      "batch 3442: loss 0.315161\n",
      "batch 3443: loss 0.243833\n",
      "batch 3444: loss 0.247835\n",
      "batch 3445: loss 0.328237\n",
      "batch 3446: loss 0.336878\n",
      "batch 3447: loss 0.459870\n",
      "batch 3448: loss 0.303365\n",
      "batch 3449: loss 0.471682\n",
      "batch 3450: loss 0.363436\n",
      "batch 3451: loss 0.176872\n",
      "batch 3452: loss 0.476297\n",
      "batch 3453: loss 0.287991\n",
      "batch 3454: loss 0.362227\n",
      "batch 3455: loss 0.284429\n",
      "batch 3456: loss 0.381298\n",
      "batch 3457: loss 0.214541\n",
      "batch 3458: loss 0.629184\n",
      "batch 3459: loss 0.422083\n",
      "batch 3460: loss 0.525657\n",
      "batch 3461: loss 0.195055\n",
      "batch 3462: loss 0.268095\n",
      "batch 3463: loss 0.605589\n",
      "batch 3464: loss 0.421439\n",
      "batch 3465: loss 0.452395\n",
      "batch 3466: loss 0.258418\n",
      "batch 3467: loss 0.454392\n",
      "batch 3468: loss 0.387604\n",
      "batch 3469: loss 0.450713\n",
      "batch 3470: loss 0.305225\n",
      "batch 3471: loss 0.332151\n",
      "batch 3472: loss 0.505954\n",
      "batch 3473: loss 0.267759\n",
      "batch 3474: loss 0.304191\n",
      "batch 3475: loss 0.419269\n",
      "batch 3476: loss 0.229979\n",
      "batch 3477: loss 0.252850\n",
      "batch 3478: loss 0.249089\n",
      "batch 3479: loss 0.310059\n",
      "batch 3480: loss 0.183920\n",
      "batch 3481: loss 0.455754\n",
      "batch 3482: loss 0.426820\n",
      "batch 3483: loss 0.411903\n",
      "batch 3484: loss 0.150280\n",
      "batch 3485: loss 0.409612\n",
      "batch 3486: loss 0.430680\n",
      "batch 3487: loss 0.288122\n",
      "batch 3488: loss 0.524895\n",
      "batch 3489: loss 0.379024\n",
      "batch 3490: loss 0.283658\n",
      "batch 3491: loss 0.371551\n",
      "batch 3492: loss 0.207080\n",
      "batch 3493: loss 0.332325\n",
      "batch 3494: loss 0.211128\n",
      "batch 3495: loss 0.237040\n",
      "batch 3496: loss 0.480834\n",
      "batch 3497: loss 0.256517\n",
      "batch 3498: loss 0.287773\n",
      "batch 3499: loss 0.393399\n",
      "batch 3500: loss 0.761154\n",
      "batch 3501: loss 0.717318\n",
      "batch 3502: loss 0.392811\n",
      "batch 3503: loss 0.243528\n",
      "batch 3504: loss 0.323972\n",
      "batch 3505: loss 0.354901\n",
      "batch 3506: loss 0.254103\n",
      "batch 3507: loss 0.491669\n",
      "batch 3508: loss 0.368506\n",
      "batch 3509: loss 0.353165\n",
      "batch 3510: loss 0.315246\n",
      "batch 3511: loss 0.170037\n",
      "batch 3512: loss 0.451274\n",
      "batch 3513: loss 0.337045\n",
      "batch 3514: loss 0.589130\n",
      "batch 3515: loss 0.332094\n",
      "batch 3516: loss 0.358540\n",
      "batch 3517: loss 0.322896\n",
      "batch 3518: loss 0.332793\n",
      "batch 3519: loss 0.320771\n",
      "batch 3520: loss 0.438887\n",
      "batch 3521: loss 0.418483\n",
      "batch 3522: loss 0.360072\n",
      "batch 3523: loss 0.309365\n",
      "batch 3524: loss 0.390756\n",
      "batch 3525: loss 0.483801\n",
      "batch 3526: loss 0.342950\n",
      "batch 3527: loss 0.291354\n",
      "batch 3528: loss 0.413974\n",
      "batch 3529: loss 0.491353\n",
      "batch 3530: loss 0.379237\n",
      "batch 3531: loss 0.363719\n",
      "batch 3532: loss 0.450621\n",
      "batch 3533: loss 0.239663\n",
      "batch 3534: loss 0.444452\n",
      "batch 3535: loss 0.193365\n",
      "batch 3536: loss 0.301022\n",
      "batch 3537: loss 0.272474\n",
      "batch 3538: loss 0.408606\n",
      "batch 3539: loss 0.333362\n",
      "batch 3540: loss 0.292191\n",
      "batch 3541: loss 0.244711\n",
      "batch 3542: loss 0.431232\n",
      "batch 3543: loss 0.206858\n",
      "batch 3544: loss 0.492538\n",
      "batch 3545: loss 0.342540\n",
      "batch 3546: loss 0.321613\n",
      "batch 3547: loss 0.346941\n",
      "batch 3548: loss 0.287731\n",
      "batch 3549: loss 0.294029\n",
      "batch 3550: loss 0.509544\n",
      "batch 3551: loss 0.435519\n",
      "batch 3552: loss 0.349411\n",
      "batch 3553: loss 0.446115\n",
      "batch 3554: loss 0.364630\n",
      "batch 3555: loss 0.282849\n",
      "batch 3556: loss 0.349318\n",
      "batch 3557: loss 0.371170\n",
      "batch 3558: loss 0.476662\n",
      "batch 3559: loss 0.294323\n",
      "batch 3560: loss 0.441742\n",
      "batch 3561: loss 0.340856\n",
      "batch 3562: loss 0.456241\n",
      "batch 3563: loss 0.321389\n",
      "batch 3564: loss 0.858808\n",
      "batch 3565: loss 0.203108\n",
      "batch 3566: loss 0.372256\n",
      "batch 3567: loss 0.386284\n",
      "batch 3568: loss 0.346541\n",
      "batch 3569: loss 0.206297\n",
      "batch 3570: loss 0.509149\n",
      "batch 3571: loss 0.449599\n",
      "batch 3572: loss 0.477383\n",
      "batch 3573: loss 0.341251\n",
      "batch 3574: loss 0.254490\n",
      "batch 3575: loss 0.341335\n",
      "batch 3576: loss 0.232577\n",
      "batch 3577: loss 0.221825\n",
      "batch 3578: loss 0.284652\n",
      "batch 3579: loss 0.242265\n",
      "batch 3580: loss 0.275297\n",
      "batch 3581: loss 0.286205\n",
      "batch 3582: loss 0.274345\n",
      "batch 3583: loss 0.314400\n",
      "batch 3584: loss 0.500672\n",
      "batch 3585: loss 0.386716\n",
      "batch 3586: loss 0.266116\n",
      "batch 3587: loss 0.368996\n",
      "batch 3588: loss 0.074205\n",
      "batch 3589: loss 0.405202\n",
      "batch 3590: loss 0.245130\n",
      "batch 3591: loss 0.439400\n",
      "batch 3592: loss 0.298881\n",
      "batch 3593: loss 0.315910\n",
      "batch 3594: loss 0.225654\n",
      "batch 3595: loss 0.498462\n",
      "batch 3596: loss 0.513171\n",
      "batch 3597: loss 0.271929\n",
      "batch 3598: loss 0.298888\n",
      "batch 3599: loss 0.259530\n",
      "batch 3600: loss 0.428108\n",
      "batch 3601: loss 0.310125\n",
      "batch 3602: loss 0.452119\n",
      "batch 3603: loss 0.310197\n",
      "batch 3604: loss 0.343935\n",
      "batch 3605: loss 0.367900\n",
      "batch 3606: loss 0.339579\n",
      "batch 3607: loss 0.442822\n",
      "batch 3608: loss 0.421185\n",
      "batch 3609: loss 0.420028\n",
      "batch 3610: loss 0.198141\n",
      "batch 3611: loss 0.314691\n",
      "batch 3612: loss 0.329361\n",
      "batch 3613: loss 0.536061\n",
      "batch 3614: loss 0.236530\n",
      "batch 3615: loss 0.355182\n",
      "batch 3616: loss 0.384843\n",
      "batch 3617: loss 0.412605\n",
      "batch 3618: loss 0.269571\n",
      "batch 3619: loss 0.183484\n",
      "batch 3620: loss 0.237696\n",
      "batch 3621: loss 0.435196\n",
      "batch 3622: loss 0.320196\n",
      "batch 3623: loss 0.150687\n",
      "batch 3624: loss 0.418965\n",
      "batch 3625: loss 0.339016\n",
      "batch 3626: loss 0.597525\n",
      "batch 3627: loss 0.207019\n",
      "batch 3628: loss 0.511792\n",
      "batch 3629: loss 0.384502\n",
      "batch 3630: loss 0.295606\n",
      "batch 3631: loss 0.218438\n",
      "batch 3632: loss 0.279446\n",
      "batch 3633: loss 0.580861\n",
      "batch 3634: loss 0.407536\n",
      "batch 3635: loss 0.326109\n",
      "batch 3636: loss 0.310140\n",
      "batch 3637: loss 0.308749\n",
      "batch 3638: loss 0.345465\n",
      "batch 3639: loss 0.151269\n",
      "batch 3640: loss 0.312184\n",
      "batch 3641: loss 0.251511\n",
      "batch 3642: loss 0.323554\n",
      "batch 3643: loss 0.339223\n",
      "batch 3644: loss 0.291027\n",
      "batch 3645: loss 0.406826\n",
      "batch 3646: loss 0.401905\n",
      "batch 3647: loss 0.412579\n",
      "batch 3648: loss 0.362258\n",
      "batch 3649: loss 0.662797\n",
      "batch 3650: loss 0.219131\n",
      "batch 3651: loss 0.514525\n",
      "batch 3652: loss 0.393970\n",
      "batch 3653: loss 0.476659\n",
      "batch 3654: loss 0.239535\n",
      "batch 3655: loss 0.380542\n",
      "batch 3656: loss 0.256542\n",
      "batch 3657: loss 0.337954\n",
      "batch 3658: loss 0.411935\n",
      "batch 3659: loss 0.539557\n",
      "batch 3660: loss 0.427659\n",
      "batch 3661: loss 0.297211\n",
      "batch 3662: loss 0.217432\n",
      "batch 3663: loss 0.461801\n",
      "batch 3664: loss 0.437314\n",
      "batch 3665: loss 0.201081\n",
      "batch 3666: loss 0.174103\n",
      "batch 3667: loss 0.112896\n",
      "batch 3668: loss 0.305058\n",
      "batch 3669: loss 0.359716\n",
      "batch 3670: loss 0.235151\n",
      "batch 3671: loss 0.329822\n",
      "batch 3672: loss 0.391895\n",
      "batch 3673: loss 0.311777\n",
      "batch 3674: loss 0.226855\n",
      "batch 3675: loss 0.296781\n",
      "batch 3676: loss 0.314859\n",
      "batch 3677: loss 0.321619\n",
      "batch 3678: loss 0.611196\n",
      "batch 3679: loss 0.408275\n",
      "batch 3680: loss 0.410933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3681: loss 0.433408\n",
      "batch 3682: loss 0.420029\n",
      "batch 3683: loss 0.472867\n",
      "batch 3684: loss 0.408813\n",
      "batch 3685: loss 0.419656\n",
      "batch 3686: loss 0.237501\n",
      "batch 3687: loss 0.507588\n",
      "batch 3688: loss 0.228639\n",
      "batch 3689: loss 0.353176\n",
      "batch 3690: loss 0.504173\n",
      "batch 3691: loss 0.690427\n",
      "batch 3692: loss 0.342922\n",
      "batch 3693: loss 0.209017\n",
      "batch 3694: loss 0.395219\n",
      "batch 3695: loss 0.339000\n",
      "batch 3696: loss 0.522704\n",
      "batch 3697: loss 0.190564\n",
      "batch 3698: loss 0.295706\n",
      "batch 3699: loss 0.525822\n",
      "batch 3700: loss 0.278535\n",
      "batch 3701: loss 0.347683\n",
      "batch 3702: loss 0.289428\n",
      "batch 3703: loss 0.484333\n",
      "batch 3704: loss 0.319754\n",
      "batch 3705: loss 0.387650\n",
      "batch 3706: loss 0.424171\n",
      "batch 3707: loss 0.422857\n",
      "batch 3708: loss 0.467896\n",
      "batch 3709: loss 0.525025\n",
      "batch 3710: loss 0.348661\n",
      "batch 3711: loss 0.433948\n",
      "batch 3712: loss 0.497373\n",
      "batch 3713: loss 0.177104\n",
      "batch 3714: loss 0.403365\n",
      "batch 3715: loss 0.386632\n",
      "batch 3716: loss 0.280438\n",
      "batch 3717: loss 0.438653\n",
      "batch 3718: loss 0.273628\n",
      "batch 3719: loss 0.313346\n",
      "batch 3720: loss 0.253055\n",
      "batch 3721: loss 0.388509\n",
      "batch 3722: loss 0.220638\n",
      "batch 3723: loss 0.323946\n",
      "batch 3724: loss 0.305119\n",
      "batch 3725: loss 0.175816\n",
      "batch 3726: loss 0.252812\n",
      "batch 3727: loss 0.456906\n",
      "batch 3728: loss 0.359745\n",
      "batch 3729: loss 0.324139\n",
      "batch 3730: loss 0.352731\n",
      "batch 3731: loss 0.494921\n",
      "batch 3732: loss 0.259516\n",
      "batch 3733: loss 0.270131\n",
      "batch 3734: loss 0.314682\n",
      "batch 3735: loss 0.198919\n",
      "batch 3736: loss 0.848103\n",
      "batch 3737: loss 0.465539\n",
      "batch 3738: loss 0.437882\n",
      "batch 3739: loss 0.329334\n",
      "batch 3740: loss 0.424388\n",
      "batch 3741: loss 0.449784\n",
      "batch 3742: loss 0.228524\n",
      "batch 3743: loss 0.259322\n",
      "batch 3744: loss 0.306694\n",
      "batch 3745: loss 0.357641\n",
      "batch 3746: loss 0.412995\n",
      "batch 3747: loss 0.263694\n",
      "batch 3748: loss 0.299381\n",
      "batch 3749: loss 0.300213\n",
      "batch 3750: loss 0.390144\n",
      "batch 3751: loss 0.273001\n",
      "batch 3752: loss 0.312817\n",
      "batch 3753: loss 0.453560\n",
      "batch 3754: loss 0.286164\n",
      "batch 3755: loss 0.311727\n",
      "batch 3756: loss 0.195409\n",
      "batch 3757: loss 0.533212\n",
      "batch 3758: loss 0.405388\n",
      "batch 3759: loss 0.292583\n",
      "batch 3760: loss 0.189886\n",
      "batch 3761: loss 0.324080\n",
      "batch 3762: loss 0.351372\n",
      "batch 3763: loss 0.250743\n",
      "batch 3764: loss 0.435376\n",
      "batch 3765: loss 0.632819\n",
      "batch 3766: loss 0.335364\n",
      "batch 3767: loss 0.331297\n",
      "batch 3768: loss 0.285302\n",
      "batch 3769: loss 0.362760\n",
      "batch 3770: loss 0.373071\n",
      "batch 3771: loss 0.338087\n",
      "batch 3772: loss 0.154178\n",
      "batch 3773: loss 0.250855\n",
      "batch 3774: loss 0.366296\n",
      "batch 3775: loss 0.324735\n",
      "batch 3776: loss 0.247144\n",
      "batch 3777: loss 0.228629\n",
      "batch 3778: loss 0.521354\n",
      "batch 3779: loss 0.309163\n",
      "batch 3780: loss 0.278310\n",
      "batch 3781: loss 0.410443\n",
      "batch 3782: loss 0.389760\n",
      "batch 3783: loss 0.309122\n",
      "batch 3784: loss 0.322505\n",
      "batch 3785: loss 0.260929\n",
      "batch 3786: loss 0.430081\n",
      "batch 3787: loss 0.230418\n",
      "batch 3788: loss 0.234802\n",
      "batch 3789: loss 0.373407\n",
      "batch 3790: loss 0.438314\n",
      "batch 3791: loss 0.287749\n",
      "batch 3792: loss 0.296790\n",
      "batch 3793: loss 0.504844\n",
      "batch 3794: loss 0.355912\n",
      "batch 3795: loss 0.386650\n",
      "batch 3796: loss 0.183049\n",
      "batch 3797: loss 0.217882\n",
      "batch 3798: loss 0.356186\n",
      "batch 3799: loss 0.413867\n",
      "batch 3800: loss 0.162553\n",
      "batch 3801: loss 0.490321\n",
      "batch 3802: loss 0.196229\n",
      "batch 3803: loss 0.339947\n",
      "batch 3804: loss 0.263066\n",
      "batch 3805: loss 0.350776\n",
      "batch 3806: loss 0.337407\n",
      "batch 3807: loss 0.312591\n",
      "batch 3808: loss 0.276723\n",
      "batch 3809: loss 0.290206\n",
      "batch 3810: loss 0.420601\n",
      "batch 3811: loss 0.268465\n",
      "batch 3812: loss 0.374841\n",
      "batch 3813: loss 0.185885\n",
      "batch 3814: loss 0.232972\n",
      "batch 3815: loss 0.295524\n",
      "batch 3816: loss 0.226460\n",
      "batch 3817: loss 0.259147\n",
      "batch 3818: loss 0.673880\n",
      "batch 3819: loss 0.354198\n",
      "batch 3820: loss 0.414060\n",
      "batch 3821: loss 0.502766\n",
      "batch 3822: loss 0.264242\n",
      "batch 3823: loss 0.288046\n",
      "batch 3824: loss 0.520876\n",
      "batch 3825: loss 0.336394\n",
      "batch 3826: loss 0.229112\n",
      "batch 3827: loss 0.251047\n",
      "batch 3828: loss 0.344672\n",
      "batch 3829: loss 0.507998\n",
      "batch 3830: loss 0.380870\n",
      "batch 3831: loss 0.596827\n",
      "batch 3832: loss 0.226606\n",
      "batch 3833: loss 0.410034\n",
      "batch 3834: loss 0.264996\n",
      "batch 3835: loss 0.585801\n",
      "batch 3836: loss 0.309622\n",
      "batch 3837: loss 0.364370\n",
      "batch 3838: loss 0.421052\n",
      "batch 3839: loss 0.460242\n",
      "batch 3840: loss 0.387571\n",
      "batch 3841: loss 0.326166\n",
      "batch 3842: loss 0.292753\n",
      "batch 3843: loss 0.346093\n",
      "batch 3844: loss 0.323420\n",
      "batch 3845: loss 0.217311\n",
      "batch 3846: loss 0.236497\n",
      "batch 3847: loss 0.484197\n",
      "batch 3848: loss 0.456373\n",
      "batch 3849: loss 0.418246\n",
      "batch 3850: loss 0.220319\n",
      "batch 3851: loss 0.355165\n",
      "batch 3852: loss 0.507455\n",
      "batch 3853: loss 0.449538\n",
      "batch 3854: loss 0.263463\n",
      "batch 3855: loss 0.319737\n",
      "batch 3856: loss 0.309233\n",
      "batch 3857: loss 0.325769\n",
      "batch 3858: loss 0.415897\n",
      "batch 3859: loss 0.185961\n",
      "batch 3860: loss 0.292086\n",
      "batch 3861: loss 0.299413\n",
      "batch 3862: loss 0.236417\n",
      "batch 3863: loss 0.275140\n",
      "batch 3864: loss 0.267563\n",
      "batch 3865: loss 0.361295\n",
      "batch 3866: loss 0.425622\n",
      "batch 3867: loss 0.311134\n",
      "batch 3868: loss 0.324087\n",
      "batch 3869: loss 0.448745\n",
      "batch 3870: loss 0.316807\n",
      "batch 3871: loss 0.276386\n",
      "batch 3872: loss 0.358949\n",
      "batch 3873: loss 0.484575\n",
      "batch 3874: loss 0.696203\n",
      "batch 3875: loss 0.362262\n",
      "batch 3876: loss 0.437119\n",
      "batch 3877: loss 0.359255\n",
      "batch 3878: loss 0.219845\n",
      "batch 3879: loss 0.425218\n",
      "batch 3880: loss 0.243885\n",
      "batch 3881: loss 0.532020\n",
      "batch 3882: loss 0.378202\n",
      "batch 3883: loss 0.337245\n",
      "batch 3884: loss 0.307172\n",
      "batch 3885: loss 0.336994\n",
      "batch 3886: loss 0.245507\n",
      "batch 3887: loss 0.313042\n",
      "batch 3888: loss 0.434021\n",
      "batch 3889: loss 0.240349\n",
      "batch 3890: loss 0.485010\n",
      "batch 3891: loss 0.403972\n",
      "batch 3892: loss 0.236645\n",
      "batch 3893: loss 0.339317\n",
      "batch 3894: loss 0.327495\n",
      "batch 3895: loss 0.338548\n",
      "batch 3896: loss 0.537420\n",
      "batch 3897: loss 0.350940\n",
      "batch 3898: loss 0.376058\n",
      "batch 3899: loss 0.409235\n",
      "batch 3900: loss 0.272574\n",
      "batch 3901: loss 0.315873\n",
      "batch 3902: loss 0.209069\n",
      "batch 3903: loss 0.528607\n",
      "batch 3904: loss 0.287734\n",
      "batch 3905: loss 0.177203\n",
      "batch 3906: loss 0.196770\n",
      "batch 3907: loss 0.521127\n",
      "batch 3908: loss 0.225205\n",
      "batch 3909: loss 0.435743\n",
      "batch 3910: loss 0.207480\n",
      "batch 3911: loss 0.550308\n",
      "batch 3912: loss 0.308975\n",
      "batch 3913: loss 0.238031\n",
      "batch 3914: loss 0.394200\n",
      "batch 3915: loss 0.435777\n",
      "batch 3916: loss 0.276220\n",
      "batch 3917: loss 0.292567\n",
      "batch 3918: loss 0.316379\n",
      "batch 3919: loss 0.398498\n",
      "batch 3920: loss 0.621588\n",
      "batch 3921: loss 0.410060\n",
      "batch 3922: loss 0.164707\n",
      "batch 3923: loss 0.366740\n",
      "batch 3924: loss 0.235998\n",
      "batch 3925: loss 0.387819\n",
      "batch 3926: loss 0.393789\n",
      "batch 3927: loss 0.345496\n",
      "batch 3928: loss 0.124779\n",
      "batch 3929: loss 0.246799\n",
      "batch 3930: loss 0.400415\n",
      "batch 3931: loss 0.423274\n",
      "batch 3932: loss 0.227306\n",
      "batch 3933: loss 0.363257\n",
      "batch 3934: loss 0.336449\n",
      "batch 3935: loss 0.245474\n",
      "batch 3936: loss 0.299823\n",
      "batch 3937: loss 0.206777\n",
      "batch 3938: loss 0.372910\n",
      "batch 3939: loss 0.260504\n",
      "batch 3940: loss 0.501040\n",
      "batch 3941: loss 0.269456\n",
      "batch 3942: loss 0.238993\n",
      "batch 3943: loss 0.356666\n",
      "batch 3944: loss 0.363580\n",
      "batch 3945: loss 0.334581\n",
      "batch 3946: loss 0.135948\n",
      "batch 3947: loss 0.433681\n",
      "batch 3948: loss 0.396846\n",
      "batch 3949: loss 0.394083\n",
      "batch 3950: loss 0.247586\n",
      "batch 3951: loss 0.365378\n",
      "batch 3952: loss 0.274116\n",
      "batch 3953: loss 0.343495\n",
      "batch 3954: loss 0.210677\n",
      "batch 3955: loss 0.313431\n",
      "batch 3956: loss 0.276836\n",
      "batch 3957: loss 0.285241\n",
      "batch 3958: loss 0.353080\n",
      "batch 3959: loss 0.375837\n",
      "batch 3960: loss 0.469052\n",
      "batch 3961: loss 0.421974\n",
      "batch 3962: loss 0.288315\n",
      "batch 3963: loss 0.259070\n",
      "batch 3964: loss 0.226894\n",
      "batch 3965: loss 0.425456\n",
      "batch 3966: loss 0.253502\n",
      "batch 3967: loss 0.301043\n",
      "batch 3968: loss 0.202994\n",
      "batch 3969: loss 0.388953\n",
      "batch 3970: loss 0.212826\n",
      "batch 3971: loss 0.509822\n",
      "batch 3972: loss 0.455305\n",
      "batch 3973: loss 0.337104\n",
      "batch 3974: loss 0.229738\n",
      "batch 3975: loss 0.171552\n",
      "batch 3976: loss 0.491584\n",
      "batch 3977: loss 0.305701\n",
      "batch 3978: loss 0.233543\n",
      "batch 3979: loss 0.309170\n",
      "batch 3980: loss 0.252010\n",
      "batch 3981: loss 0.321394\n",
      "batch 3982: loss 0.424758\n",
      "batch 3983: loss 0.265626\n",
      "batch 3984: loss 0.300571\n",
      "batch 3985: loss 0.461827\n",
      "batch 3986: loss 0.537441\n",
      "batch 3987: loss 0.467260\n",
      "batch 3988: loss 0.423206\n",
      "batch 3989: loss 0.320305\n",
      "batch 3990: loss 0.265122\n",
      "batch 3991: loss 0.404933\n",
      "batch 3992: loss 0.347063\n",
      "batch 3993: loss 0.329810\n",
      "batch 3994: loss 0.349654\n",
      "batch 3995: loss 0.372174\n",
      "batch 3996: loss 0.349356\n",
      "batch 3997: loss 0.373531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3998: loss 0.313370\n",
      "batch 3999: loss 0.286129\n",
      "batch 4000: loss 0.308772\n",
      "batch 4001: loss 0.269822\n",
      "batch 4002: loss 0.253977\n",
      "batch 4003: loss 0.185759\n",
      "batch 4004: loss 0.320629\n",
      "batch 4005: loss 0.348682\n",
      "batch 4006: loss 0.324272\n",
      "batch 4007: loss 0.363078\n",
      "batch 4008: loss 0.229351\n",
      "batch 4009: loss 0.301746\n",
      "batch 4010: loss 0.424591\n",
      "batch 4011: loss 0.177785\n",
      "batch 4012: loss 0.386351\n",
      "batch 4013: loss 0.263561\n",
      "batch 4014: loss 0.244472\n",
      "batch 4015: loss 0.413026\n",
      "batch 4016: loss 0.538065\n",
      "batch 4017: loss 0.366936\n",
      "batch 4018: loss 0.224314\n",
      "batch 4019: loss 0.301881\n",
      "batch 4020: loss 0.290309\n",
      "batch 4021: loss 0.387533\n",
      "batch 4022: loss 0.361675\n",
      "batch 4023: loss 0.371511\n",
      "batch 4024: loss 0.376935\n",
      "batch 4025: loss 0.309438\n",
      "batch 4026: loss 0.261034\n",
      "batch 4027: loss 0.329258\n",
      "batch 4028: loss 0.422308\n",
      "batch 4029: loss 0.279675\n",
      "batch 4030: loss 0.468174\n",
      "batch 4031: loss 0.276528\n",
      "batch 4032: loss 0.435355\n",
      "batch 4033: loss 0.338777\n",
      "batch 4034: loss 0.295423\n",
      "batch 4035: loss 0.264033\n",
      "batch 4036: loss 0.384048\n",
      "batch 4037: loss 0.304052\n",
      "batch 4038: loss 0.390002\n",
      "batch 4039: loss 0.242846\n",
      "batch 4040: loss 0.357089\n",
      "batch 4041: loss 0.306268\n",
      "batch 4042: loss 0.380042\n",
      "batch 4043: loss 0.599361\n",
      "batch 4044: loss 0.543212\n",
      "batch 4045: loss 0.306715\n",
      "batch 4046: loss 0.350080\n",
      "batch 4047: loss 0.236146\n",
      "batch 4048: loss 0.312077\n",
      "batch 4049: loss 0.190110\n",
      "batch 4050: loss 0.271170\n",
      "batch 4051: loss 0.372916\n",
      "batch 4052: loss 0.322940\n",
      "batch 4053: loss 0.369345\n",
      "batch 4054: loss 0.229206\n",
      "batch 4055: loss 0.302214\n",
      "batch 4056: loss 0.147952\n",
      "batch 4057: loss 0.324429\n",
      "batch 4058: loss 0.546153\n",
      "batch 4059: loss 0.386940\n",
      "batch 4060: loss 0.301324\n",
      "batch 4061: loss 0.445172\n",
      "batch 4062: loss 0.208897\n",
      "batch 4063: loss 0.394517\n",
      "batch 4064: loss 0.220849\n",
      "batch 4065: loss 0.427120\n",
      "batch 4066: loss 0.345914\n",
      "batch 4067: loss 0.163306\n",
      "batch 4068: loss 0.332253\n",
      "batch 4069: loss 0.479952\n",
      "batch 4070: loss 0.350071\n",
      "batch 4071: loss 0.594508\n",
      "batch 4072: loss 0.207127\n",
      "batch 4073: loss 0.279608\n",
      "batch 4074: loss 0.303651\n",
      "batch 4075: loss 0.373738\n",
      "batch 4076: loss 0.235310\n",
      "batch 4077: loss 0.200453\n",
      "batch 4078: loss 0.223149\n",
      "batch 4079: loss 0.648775\n",
      "batch 4080: loss 0.500431\n",
      "batch 4081: loss 0.320800\n",
      "batch 4082: loss 0.300376\n",
      "batch 4083: loss 0.482735\n",
      "batch 4084: loss 0.333218\n",
      "batch 4085: loss 0.342019\n",
      "batch 4086: loss 0.447484\n",
      "batch 4087: loss 0.573553\n",
      "batch 4088: loss 0.368947\n",
      "batch 4089: loss 0.297042\n",
      "batch 4090: loss 0.326860\n",
      "batch 4091: loss 0.342773\n",
      "batch 4092: loss 0.455314\n",
      "batch 4093: loss 0.287565\n",
      "batch 4094: loss 0.236057\n",
      "batch 4095: loss 0.434928\n",
      "batch 4096: loss 0.373415\n",
      "batch 4097: loss 0.270513\n",
      "batch 4098: loss 0.368457\n",
      "batch 4099: loss 0.336300\n",
      "batch 4100: loss 0.517585\n",
      "batch 4101: loss 0.338629\n",
      "batch 4102: loss 0.266196\n",
      "batch 4103: loss 0.384032\n",
      "batch 4104: loss 0.527676\n",
      "batch 4105: loss 0.325703\n",
      "batch 4106: loss 0.478115\n",
      "batch 4107: loss 0.252118\n",
      "batch 4108: loss 0.334586\n",
      "batch 4109: loss 0.341041\n",
      "batch 4110: loss 0.525331\n",
      "batch 4111: loss 0.190093\n",
      "batch 4112: loss 0.411044\n",
      "batch 4113: loss 0.200758\n",
      "batch 4114: loss 0.202263\n",
      "batch 4115: loss 0.324059\n",
      "batch 4116: loss 0.307952\n",
      "batch 4117: loss 0.245303\n",
      "batch 4118: loss 0.310029\n",
      "batch 4119: loss 0.256921\n",
      "batch 4120: loss 0.366774\n",
      "batch 4121: loss 0.449259\n",
      "batch 4122: loss 0.315059\n",
      "batch 4123: loss 0.465488\n",
      "batch 4124: loss 0.360209\n",
      "batch 4125: loss 0.257120\n",
      "batch 4126: loss 0.306843\n",
      "batch 4127: loss 0.304174\n",
      "batch 4128: loss 0.488569\n",
      "batch 4129: loss 0.338439\n",
      "batch 4130: loss 0.162272\n",
      "batch 4131: loss 0.358747\n",
      "batch 4132: loss 0.310758\n",
      "batch 4133: loss 0.511822\n",
      "batch 4134: loss 0.528702\n",
      "batch 4135: loss 0.188163\n",
      "batch 4136: loss 0.366133\n",
      "batch 4137: loss 0.275879\n",
      "batch 4138: loss 0.191230\n",
      "batch 4139: loss 0.399379\n",
      "batch 4140: loss 0.299195\n",
      "batch 4141: loss 0.506406\n",
      "batch 4142: loss 0.353300\n",
      "batch 4143: loss 0.385236\n",
      "batch 4144: loss 0.182865\n",
      "batch 4145: loss 0.239943\n",
      "batch 4146: loss 0.444030\n",
      "batch 4147: loss 0.472302\n",
      "batch 4148: loss 0.314131\n",
      "batch 4149: loss 0.310771\n",
      "batch 4150: loss 0.383088\n",
      "batch 4151: loss 0.485562\n",
      "batch 4152: loss 0.344489\n",
      "batch 4153: loss 0.440647\n",
      "batch 4154: loss 0.219914\n",
      "batch 4155: loss 0.413178\n",
      "batch 4156: loss 0.520117\n",
      "batch 4157: loss 0.272312\n",
      "batch 4158: loss 0.294975\n",
      "batch 4159: loss 0.285431\n",
      "batch 4160: loss 0.372631\n",
      "batch 4161: loss 0.522892\n",
      "batch 4162: loss 0.605625\n",
      "batch 4163: loss 0.283530\n",
      "batch 4164: loss 0.477035\n",
      "batch 4165: loss 0.520590\n",
      "batch 4166: loss 0.313140\n",
      "batch 4167: loss 0.438949\n",
      "batch 4168: loss 0.340466\n",
      "batch 4169: loss 0.351929\n",
      "batch 4170: loss 0.202907\n",
      "batch 4171: loss 0.280428\n",
      "batch 4172: loss 0.258262\n",
      "batch 4173: loss 0.289733\n",
      "batch 4174: loss 0.375746\n",
      "batch 4175: loss 0.496016\n",
      "batch 4176: loss 0.134170\n",
      "batch 4177: loss 0.254960\n",
      "batch 4178: loss 0.456401\n",
      "batch 4179: loss 0.252082\n",
      "batch 4180: loss 0.393462\n",
      "batch 4181: loss 0.335420\n",
      "batch 4182: loss 0.263361\n",
      "batch 4183: loss 0.295008\n",
      "batch 4184: loss 0.617998\n",
      "batch 4185: loss 0.380012\n",
      "batch 4186: loss 0.320700\n",
      "batch 4187: loss 0.275177\n",
      "batch 4188: loss 0.374036\n",
      "batch 4189: loss 0.360239\n",
      "batch 4190: loss 0.349456\n",
      "batch 4191: loss 0.275294\n",
      "batch 4192: loss 0.362994\n",
      "batch 4193: loss 0.218344\n",
      "batch 4194: loss 0.372499\n",
      "batch 4195: loss 0.157841\n",
      "batch 4196: loss 0.426904\n",
      "batch 4197: loss 0.227589\n",
      "batch 4198: loss 0.289520\n",
      "batch 4199: loss 0.240437\n",
      "batch 4200: loss 0.424512\n",
      "batch 4201: loss 0.222292\n",
      "batch 4202: loss 0.218862\n",
      "batch 4203: loss 0.372978\n",
      "batch 4204: loss 0.232890\n",
      "batch 4205: loss 0.291711\n",
      "batch 4206: loss 0.224338\n",
      "batch 4207: loss 0.269073\n",
      "batch 4208: loss 0.207033\n",
      "batch 4209: loss 0.359939\n",
      "batch 4210: loss 0.264632\n",
      "batch 4211: loss 0.277892\n",
      "batch 4212: loss 0.259448\n",
      "batch 4213: loss 0.392582\n",
      "batch 4214: loss 0.449576\n",
      "batch 4215: loss 0.422240\n",
      "batch 4216: loss 0.290316\n",
      "batch 4217: loss 0.233139\n",
      "batch 4218: loss 0.351644\n",
      "batch 4219: loss 0.293806\n",
      "batch 4220: loss 0.395746\n",
      "batch 4221: loss 0.122223\n",
      "batch 4222: loss 0.243184\n",
      "batch 4223: loss 0.586010\n",
      "batch 4224: loss 0.394381\n",
      "batch 4225: loss 0.357341\n",
      "batch 4226: loss 0.321287\n",
      "batch 4227: loss 0.422414\n",
      "batch 4228: loss 0.297502\n",
      "batch 4229: loss 0.325779\n",
      "batch 4230: loss 0.335183\n",
      "batch 4231: loss 0.260862\n",
      "batch 4232: loss 0.405326\n",
      "batch 4233: loss 0.293867\n",
      "batch 4234: loss 0.258910\n",
      "batch 4235: loss 0.195367\n",
      "batch 4236: loss 0.346689\n",
      "batch 4237: loss 0.391254\n",
      "batch 4238: loss 0.272366\n",
      "batch 4239: loss 0.414894\n",
      "batch 4240: loss 0.452156\n",
      "batch 4241: loss 0.327052\n",
      "batch 4242: loss 0.276868\n",
      "batch 4243: loss 0.196385\n",
      "batch 4244: loss 0.272839\n",
      "batch 4245: loss 0.212232\n",
      "batch 4246: loss 0.198528\n",
      "batch 4247: loss 0.370725\n",
      "batch 4248: loss 0.277961\n",
      "batch 4249: loss 0.237777\n",
      "batch 4250: loss 0.449065\n",
      "batch 4251: loss 0.239472\n",
      "batch 4252: loss 0.344954\n",
      "batch 4253: loss 0.579075\n",
      "batch 4254: loss 0.506289\n",
      "batch 4255: loss 0.592713\n",
      "batch 4256: loss 0.234183\n",
      "batch 4257: loss 0.271356\n",
      "batch 4258: loss 0.291441\n",
      "batch 4259: loss 0.357500\n",
      "batch 4260: loss 0.227280\n",
      "batch 4261: loss 0.366723\n",
      "batch 4262: loss 0.319324\n",
      "batch 4263: loss 0.424045\n",
      "batch 4264: loss 0.301024\n",
      "batch 4265: loss 0.297765\n",
      "batch 4266: loss 0.181395\n",
      "batch 4267: loss 0.218373\n",
      "batch 4268: loss 0.289102\n",
      "batch 4269: loss 0.493688\n",
      "batch 4270: loss 0.260787\n",
      "batch 4271: loss 0.403220\n",
      "batch 4272: loss 0.270593\n",
      "batch 4273: loss 0.366141\n",
      "batch 4274: loss 0.442398\n",
      "batch 4275: loss 0.377302\n",
      "batch 4276: loss 0.327002\n",
      "batch 4277: loss 0.429001\n",
      "batch 4278: loss 0.453951\n",
      "batch 4279: loss 0.334490\n",
      "batch 4280: loss 0.277372\n",
      "batch 4281: loss 0.334502\n",
      "batch 4282: loss 0.421312\n",
      "batch 4283: loss 0.349972\n",
      "batch 4284: loss 0.295812\n",
      "batch 4285: loss 0.346947\n",
      "batch 4286: loss 0.415697\n",
      "batch 4287: loss 0.344894\n",
      "batch 4288: loss 0.223095\n",
      "batch 4289: loss 0.395018\n",
      "batch 4290: loss 0.276418\n",
      "batch 4291: loss 0.181262\n",
      "batch 4292: loss 0.423447\n",
      "batch 4293: loss 0.304451\n",
      "batch 4294: loss 0.416179\n",
      "batch 4295: loss 0.361702\n",
      "batch 4296: loss 0.368895\n",
      "batch 4297: loss 0.385535\n",
      "batch 4298: loss 0.395584\n",
      "batch 4299: loss 0.422167\n",
      "batch 4300: loss 0.255822\n",
      "batch 4301: loss 0.394091\n",
      "batch 4302: loss 0.365838\n",
      "batch 4303: loss 0.359784\n",
      "batch 4304: loss 0.591065\n",
      "batch 4305: loss 0.297222\n",
      "batch 4306: loss 0.164258\n",
      "batch 4307: loss 0.350218\n",
      "batch 4308: loss 0.247916\n",
      "batch 4309: loss 0.339731\n",
      "batch 4310: loss 0.282009\n",
      "batch 4311: loss 0.363708\n",
      "batch 4312: loss 0.190157\n",
      "batch 4313: loss 0.286598\n",
      "batch 4314: loss 0.342363\n",
      "batch 4315: loss 0.279073\n",
      "batch 4316: loss 0.199873\n",
      "batch 4317: loss 0.313302\n",
      "batch 4318: loss 0.297339\n",
      "batch 4319: loss 0.253917\n",
      "batch 4320: loss 0.426120\n",
      "batch 4321: loss 0.250485\n",
      "batch 4322: loss 0.596494\n",
      "batch 4323: loss 0.284580\n",
      "batch 4324: loss 0.295404\n",
      "batch 4325: loss 0.610760\n",
      "batch 4326: loss 0.221594\n",
      "batch 4327: loss 0.350572\n",
      "batch 4328: loss 0.331920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4329: loss 0.534869\n",
      "batch 4330: loss 0.364586\n",
      "batch 4331: loss 0.314240\n",
      "batch 4332: loss 0.318916\n",
      "batch 4333: loss 0.579276\n",
      "batch 4334: loss 0.267886\n",
      "batch 4335: loss 0.309612\n",
      "batch 4336: loss 0.356897\n",
      "batch 4337: loss 0.212487\n",
      "batch 4338: loss 0.238795\n",
      "batch 4339: loss 0.303732\n",
      "batch 4340: loss 0.274833\n",
      "batch 4341: loss 0.472182\n",
      "batch 4342: loss 0.220212\n",
      "batch 4343: loss 0.341749\n",
      "batch 4344: loss 0.360059\n",
      "batch 4345: loss 0.149715\n",
      "batch 4346: loss 0.182361\n",
      "batch 4347: loss 0.366923\n",
      "batch 4348: loss 0.328959\n",
      "batch 4349: loss 0.389874\n",
      "batch 4350: loss 0.362405\n",
      "batch 4351: loss 0.366468\n",
      "batch 4352: loss 0.246260\n",
      "batch 4353: loss 0.270007\n",
      "batch 4354: loss 0.267604\n",
      "batch 4355: loss 0.187567\n",
      "batch 4356: loss 0.490375\n",
      "batch 4357: loss 0.206403\n",
      "batch 4358: loss 0.277616\n",
      "batch 4359: loss 0.359171\n",
      "batch 4360: loss 0.311151\n",
      "batch 4361: loss 0.298635\n",
      "batch 4362: loss 0.441447\n",
      "batch 4363: loss 0.291793\n",
      "batch 4364: loss 0.271640\n",
      "batch 4365: loss 0.471473\n",
      "batch 4366: loss 0.309472\n",
      "batch 4367: loss 0.452900\n",
      "batch 4368: loss 0.249419\n",
      "batch 4369: loss 0.321300\n",
      "batch 4370: loss 0.317559\n",
      "batch 4371: loss 0.358451\n",
      "batch 4372: loss 0.248967\n",
      "batch 4373: loss 0.341262\n",
      "batch 4374: loss 0.282029\n",
      "batch 4375: loss 0.364322\n",
      "batch 4376: loss 0.243339\n",
      "batch 4377: loss 0.446021\n",
      "batch 4378: loss 0.142556\n",
      "batch 4379: loss 0.313361\n",
      "batch 4380: loss 0.339530\n",
      "batch 4381: loss 0.361316\n",
      "batch 4382: loss 0.197667\n",
      "batch 4383: loss 0.398387\n",
      "batch 4384: loss 0.395759\n",
      "batch 4385: loss 0.484964\n",
      "batch 4386: loss 0.297701\n",
      "batch 4387: loss 0.302239\n",
      "batch 4388: loss 0.441621\n",
      "batch 4389: loss 0.271437\n",
      "batch 4390: loss 0.418007\n",
      "batch 4391: loss 0.298970\n",
      "batch 4392: loss 0.191193\n",
      "batch 4393: loss 0.319744\n",
      "batch 4394: loss 0.444985\n",
      "batch 4395: loss 0.372540\n",
      "batch 4396: loss 0.511660\n",
      "batch 4397: loss 0.196457\n",
      "batch 4398: loss 0.352760\n",
      "batch 4399: loss 0.648883\n",
      "batch 4400: loss 0.225479\n",
      "batch 4401: loss 0.370079\n",
      "batch 4402: loss 0.344714\n",
      "batch 4403: loss 0.172567\n",
      "batch 4404: loss 0.495207\n",
      "batch 4405: loss 0.419190\n",
      "batch 4406: loss 0.367479\n",
      "batch 4407: loss 0.287070\n",
      "batch 4408: loss 0.145253\n",
      "batch 4409: loss 0.323240\n",
      "batch 4410: loss 0.238288\n",
      "batch 4411: loss 0.266980\n",
      "batch 4412: loss 0.329996\n",
      "batch 4413: loss 0.579857\n",
      "batch 4414: loss 0.290417\n",
      "batch 4415: loss 0.365797\n",
      "batch 4416: loss 0.365043\n",
      "batch 4417: loss 0.292175\n",
      "batch 4418: loss 0.220771\n",
      "batch 4419: loss 0.395827\n",
      "batch 4420: loss 0.261739\n",
      "batch 4421: loss 0.322891\n",
      "batch 4422: loss 0.176689\n",
      "batch 4423: loss 0.326189\n",
      "batch 4424: loss 0.230660\n",
      "batch 4425: loss 0.275781\n",
      "batch 4426: loss 0.315790\n",
      "batch 4427: loss 0.428924\n",
      "batch 4428: loss 0.362226\n",
      "batch 4429: loss 0.286931\n",
      "batch 4430: loss 0.401028\n",
      "batch 4431: loss 0.266800\n",
      "batch 4432: loss 0.186992\n",
      "batch 4433: loss 0.520724\n",
      "batch 4434: loss 0.380377\n",
      "batch 4435: loss 0.531858\n",
      "batch 4436: loss 0.195367\n",
      "batch 4437: loss 0.401904\n",
      "batch 4438: loss 0.224875\n",
      "batch 4439: loss 0.315155\n",
      "batch 4440: loss 0.245084\n",
      "batch 4441: loss 0.345635\n",
      "batch 4442: loss 0.603081\n",
      "batch 4443: loss 0.211973\n",
      "batch 4444: loss 0.410129\n",
      "batch 4445: loss 0.350531\n",
      "batch 4446: loss 0.245569\n",
      "batch 4447: loss 0.471673\n",
      "batch 4448: loss 0.513318\n",
      "batch 4449: loss 0.407257\n",
      "batch 4450: loss 0.197955\n",
      "batch 4451: loss 0.268941\n",
      "batch 4452: loss 0.355215\n",
      "batch 4453: loss 0.355155\n",
      "batch 4454: loss 0.282040\n",
      "batch 4455: loss 0.429915\n",
      "batch 4456: loss 0.301466\n",
      "batch 4457: loss 0.294485\n",
      "batch 4458: loss 0.385218\n",
      "batch 4459: loss 0.300278\n",
      "batch 4460: loss 0.393478\n",
      "batch 4461: loss 0.379387\n",
      "batch 4462: loss 0.426579\n",
      "batch 4463: loss 0.265747\n",
      "batch 4464: loss 0.445581\n",
      "batch 4465: loss 0.318069\n",
      "batch 4466: loss 0.380621\n",
      "batch 4467: loss 0.208515\n",
      "batch 4468: loss 0.281766\n",
      "batch 4469: loss 0.429304\n",
      "batch 4470: loss 0.393758\n",
      "batch 4471: loss 0.317418\n",
      "batch 4472: loss 0.427660\n",
      "batch 4473: loss 0.316709\n",
      "batch 4474: loss 0.211944\n",
      "batch 4475: loss 0.485947\n",
      "batch 4476: loss 0.293405\n",
      "batch 4477: loss 0.281466\n",
      "batch 4478: loss 0.391072\n",
      "batch 4479: loss 0.402404\n",
      "batch 4480: loss 0.308119\n",
      "batch 4481: loss 0.285500\n",
      "batch 4482: loss 0.290021\n",
      "batch 4483: loss 0.318626\n",
      "batch 4484: loss 0.248183\n",
      "batch 4485: loss 0.329255\n",
      "batch 4486: loss 0.371313\n",
      "batch 4487: loss 0.267321\n",
      "batch 4488: loss 0.281565\n",
      "batch 4489: loss 0.400608\n",
      "batch 4490: loss 0.251405\n",
      "batch 4491: loss 0.370848\n",
      "batch 4492: loss 0.441066\n",
      "batch 4493: loss 0.244648\n",
      "batch 4494: loss 0.557740\n",
      "batch 4495: loss 0.404727\n",
      "batch 4496: loss 0.263559\n",
      "batch 4497: loss 0.495830\n",
      "batch 4498: loss 0.520778\n",
      "batch 4499: loss 0.234872\n",
      "batch 4500: loss 0.483693\n",
      "batch 4501: loss 0.424155\n",
      "batch 4502: loss 0.263423\n",
      "batch 4503: loss 0.213848\n",
      "batch 4504: loss 0.374846\n",
      "batch 4505: loss 0.371145\n",
      "batch 4506: loss 0.192978\n",
      "batch 4507: loss 0.459710\n",
      "batch 4508: loss 0.383581\n",
      "batch 4509: loss 0.413450\n",
      "batch 4510: loss 0.193771\n",
      "batch 4511: loss 0.420493\n",
      "batch 4512: loss 0.267736\n",
      "batch 4513: loss 0.273314\n",
      "batch 4514: loss 0.375810\n",
      "batch 4515: loss 0.332604\n",
      "batch 4516: loss 0.433849\n",
      "batch 4517: loss 0.434544\n",
      "batch 4518: loss 0.281489\n",
      "batch 4519: loss 0.345017\n",
      "batch 4520: loss 0.324884\n",
      "batch 4521: loss 0.243674\n",
      "batch 4522: loss 0.144686\n",
      "batch 4523: loss 0.522338\n",
      "batch 4524: loss 0.396937\n",
      "batch 4525: loss 0.350010\n",
      "batch 4526: loss 0.331295\n",
      "batch 4527: loss 0.323983\n",
      "batch 4528: loss 0.312697\n",
      "batch 4529: loss 0.447806\n",
      "batch 4530: loss 0.303434\n",
      "batch 4531: loss 0.300975\n",
      "batch 4532: loss 0.210014\n",
      "batch 4533: loss 0.370277\n",
      "batch 4534: loss 0.396728\n",
      "batch 4535: loss 0.383791\n",
      "batch 4536: loss 0.469242\n",
      "batch 4537: loss 0.287216\n",
      "batch 4538: loss 0.563863\n",
      "batch 4539: loss 0.421637\n",
      "batch 4540: loss 0.354433\n",
      "batch 4541: loss 0.196495\n",
      "batch 4542: loss 0.460644\n",
      "batch 4543: loss 0.295625\n",
      "batch 4544: loss 0.414732\n",
      "batch 4545: loss 0.249574\n",
      "batch 4546: loss 0.262075\n",
      "batch 4547: loss 0.220607\n",
      "batch 4548: loss 0.446030\n",
      "batch 4549: loss 0.208176\n",
      "batch 4550: loss 0.288678\n",
      "batch 4551: loss 0.276985\n",
      "batch 4552: loss 0.364547\n",
      "batch 4553: loss 0.233189\n",
      "batch 4554: loss 0.445188\n",
      "batch 4555: loss 0.168786\n",
      "batch 4556: loss 0.196672\n",
      "batch 4557: loss 0.270967\n",
      "batch 4558: loss 0.282484\n",
      "batch 4559: loss 0.528125\n",
      "batch 4560: loss 0.354187\n",
      "batch 4561: loss 0.245082\n",
      "batch 4562: loss 0.295550\n",
      "batch 4563: loss 0.346372\n",
      "batch 4564: loss 0.392849\n",
      "batch 4565: loss 0.544831\n",
      "batch 4566: loss 0.309780\n",
      "batch 4567: loss 0.378757\n",
      "batch 4568: loss 0.394069\n",
      "batch 4569: loss 0.159811\n",
      "batch 4570: loss 0.370757\n",
      "batch 4571: loss 0.321420\n",
      "batch 4572: loss 0.364899\n",
      "batch 4573: loss 0.316740\n",
      "batch 4574: loss 0.323486\n",
      "batch 4575: loss 0.529189\n",
      "batch 4576: loss 0.331633\n",
      "batch 4577: loss 0.298339\n",
      "batch 4578: loss 0.286112\n",
      "batch 4579: loss 0.392954\n",
      "batch 4580: loss 0.247166\n",
      "batch 4581: loss 0.244099\n",
      "batch 4582: loss 0.273520\n",
      "batch 4583: loss 0.362397\n",
      "batch 4584: loss 0.389068\n",
      "batch 4585: loss 0.387015\n",
      "batch 4586: loss 0.364855\n",
      "batch 4587: loss 0.559239\n",
      "batch 4588: loss 0.350985\n",
      "batch 4589: loss 0.424756\n",
      "batch 4590: loss 0.252461\n",
      "batch 4591: loss 0.359393\n",
      "batch 4592: loss 0.312172\n",
      "batch 4593: loss 0.298067\n",
      "batch 4594: loss 0.230427\n",
      "batch 4595: loss 0.359076\n",
      "batch 4596: loss 0.427074\n",
      "batch 4597: loss 0.384650\n",
      "batch 4598: loss 0.317564\n",
      "batch 4599: loss 0.356587\n",
      "batch 4600: loss 0.323941\n",
      "batch 4601: loss 0.263936\n",
      "batch 4602: loss 0.322393\n",
      "batch 4603: loss 0.420600\n",
      "batch 4604: loss 0.505926\n",
      "batch 4605: loss 0.433064\n",
      "batch 4606: loss 0.537133\n",
      "batch 4607: loss 0.231521\n",
      "batch 4608: loss 0.499502\n",
      "batch 4609: loss 0.341906\n",
      "batch 4610: loss 0.253220\n",
      "batch 4611: loss 0.261379\n",
      "batch 4612: loss 0.241634\n",
      "batch 4613: loss 0.380717\n",
      "batch 4614: loss 0.379181\n",
      "batch 4615: loss 0.288893\n",
      "batch 4616: loss 0.267891\n",
      "batch 4617: loss 0.361906\n",
      "batch 4618: loss 0.252613\n",
      "batch 4619: loss 0.265373\n",
      "batch 4620: loss 0.392206\n",
      "batch 4621: loss 0.348834\n",
      "batch 4622: loss 0.399636\n",
      "batch 4623: loss 0.327449\n",
      "batch 4624: loss 0.440040\n",
      "batch 4625: loss 0.339929\n",
      "batch 4626: loss 0.420564\n",
      "batch 4627: loss 0.333792\n",
      "batch 4628: loss 0.284131\n",
      "batch 4629: loss 0.244977\n",
      "batch 4630: loss 0.358931\n",
      "batch 4631: loss 0.403633\n",
      "batch 4632: loss 0.301290\n",
      "batch 4633: loss 0.285107\n",
      "batch 4634: loss 0.270238\n",
      "batch 4635: loss 0.348795\n",
      "batch 4636: loss 0.241508\n",
      "batch 4637: loss 0.208971\n",
      "batch 4638: loss 0.351081\n",
      "batch 4639: loss 0.360496\n",
      "batch 4640: loss 0.318666\n",
      "batch 4641: loss 0.325058\n",
      "batch 4642: loss 0.282878\n",
      "batch 4643: loss 0.300835\n",
      "batch 4644: loss 0.333888\n",
      "batch 4645: loss 0.224563\n",
      "batch 4646: loss 0.313347\n",
      "batch 4647: loss 0.242953\n",
      "batch 4648: loss 0.278422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4649: loss 0.344619\n",
      "batch 4650: loss 0.355773\n",
      "batch 4651: loss 0.193928\n",
      "batch 4652: loss 0.302872\n",
      "batch 4653: loss 0.574316\n",
      "batch 4654: loss 0.407967\n",
      "batch 4655: loss 0.223194\n",
      "batch 4656: loss 0.380764\n",
      "batch 4657: loss 0.303605\n",
      "batch 4658: loss 0.188167\n",
      "batch 4659: loss 0.433320\n",
      "batch 4660: loss 0.298572\n",
      "batch 4661: loss 0.416831\n",
      "batch 4662: loss 0.382791\n",
      "batch 4663: loss 0.268535\n",
      "batch 4664: loss 0.309056\n",
      "batch 4665: loss 0.320277\n",
      "batch 4666: loss 0.451846\n",
      "batch 4667: loss 0.393716\n",
      "batch 4668: loss 0.287797\n",
      "batch 4669: loss 0.219512\n",
      "batch 4670: loss 0.464636\n",
      "batch 4671: loss 0.277352\n",
      "batch 4672: loss 0.298466\n",
      "batch 4673: loss 0.345960\n",
      "batch 4674: loss 0.343150\n",
      "batch 4675: loss 0.331291\n",
      "batch 4676: loss 0.250059\n",
      "batch 4677: loss 0.244678\n",
      "batch 4678: loss 0.177059\n",
      "batch 4679: loss 0.366697\n",
      "batch 4680: loss 0.342802\n",
      "batch 4681: loss 0.290029\n",
      "batch 4682: loss 0.256885\n",
      "batch 4683: loss 0.486709\n",
      "batch 4684: loss 0.306756\n",
      "batch 4685: loss 0.374865\n",
      "batch 4686: loss 0.355435\n",
      "batch 4687: loss 0.313286\n",
      "batch 4688: loss 0.344175\n",
      "batch 4689: loss 0.282171\n",
      "batch 4690: loss 0.336815\n",
      "batch 4691: loss 0.276091\n",
      "batch 4692: loss 0.335765\n",
      "batch 4693: loss 0.387975\n",
      "batch 4694: loss 0.518099\n",
      "batch 4695: loss 0.284039\n",
      "batch 4696: loss 0.222675\n",
      "batch 4697: loss 0.217315\n",
      "batch 4698: loss 0.329892\n",
      "batch 4699: loss 0.298496\n",
      "batch 4700: loss 0.488838\n",
      "batch 4701: loss 0.384972\n",
      "batch 4702: loss 0.298447\n",
      "batch 4703: loss 0.253019\n",
      "batch 4704: loss 0.381732\n",
      "batch 4705: loss 0.211785\n",
      "batch 4706: loss 0.207291\n",
      "batch 4707: loss 0.258711\n",
      "batch 4708: loss 0.321398\n",
      "batch 4709: loss 0.367271\n",
      "batch 4710: loss 0.182374\n",
      "batch 4711: loss 0.402246\n",
      "batch 4712: loss 0.359546\n",
      "batch 4713: loss 0.560453\n",
      "batch 4714: loss 0.299762\n",
      "batch 4715: loss 0.245423\n",
      "batch 4716: loss 0.307256\n",
      "batch 4717: loss 0.480300\n",
      "batch 4718: loss 0.473916\n",
      "batch 4719: loss 0.244257\n",
      "batch 4720: loss 0.210112\n",
      "batch 4721: loss 0.457626\n",
      "batch 4722: loss 0.212644\n",
      "batch 4723: loss 0.352793\n",
      "batch 4724: loss 0.282209\n",
      "batch 4725: loss 0.442808\n",
      "batch 4726: loss 0.317098\n",
      "batch 4727: loss 0.344830\n",
      "batch 4728: loss 0.217921\n",
      "batch 4729: loss 0.311234\n",
      "batch 4730: loss 0.392359\n",
      "batch 4731: loss 0.234782\n",
      "batch 4732: loss 0.316991\n",
      "batch 4733: loss 0.198854\n",
      "batch 4734: loss 0.228046\n",
      "batch 4735: loss 0.305962\n",
      "batch 4736: loss 0.260905\n",
      "batch 4737: loss 0.294667\n",
      "batch 4738: loss 0.355560\n",
      "batch 4739: loss 0.378475\n",
      "batch 4740: loss 0.391717\n",
      "batch 4741: loss 0.236493\n",
      "batch 4742: loss 0.237392\n",
      "batch 4743: loss 0.326115\n",
      "batch 4744: loss 0.220475\n",
      "batch 4745: loss 0.273816\n",
      "batch 4746: loss 0.289845\n",
      "batch 4747: loss 0.343309\n",
      "batch 4748: loss 0.327030\n",
      "batch 4749: loss 0.321741\n",
      "batch 4750: loss 0.230527\n",
      "batch 4751: loss 0.367382\n",
      "batch 4752: loss 0.418310\n",
      "batch 4753: loss 0.406111\n",
      "batch 4754: loss 0.315754\n",
      "batch 4755: loss 0.218874\n",
      "batch 4756: loss 0.353751\n",
      "batch 4757: loss 0.350738\n",
      "batch 4758: loss 0.141070\n",
      "batch 4759: loss 0.357834\n",
      "batch 4760: loss 0.366014\n",
      "batch 4761: loss 0.507630\n",
      "batch 4762: loss 0.311031\n",
      "batch 4763: loss 0.296591\n",
      "batch 4764: loss 0.262350\n",
      "batch 4765: loss 0.197635\n",
      "batch 4766: loss 0.239311\n",
      "batch 4767: loss 0.302747\n",
      "batch 4768: loss 0.388249\n",
      "batch 4769: loss 0.390744\n",
      "batch 4770: loss 0.192522\n",
      "batch 4771: loss 0.345087\n",
      "batch 4772: loss 0.314252\n",
      "batch 4773: loss 0.382131\n",
      "batch 4774: loss 0.222277\n",
      "batch 4775: loss 0.441390\n",
      "batch 4776: loss 0.247780\n",
      "batch 4777: loss 0.425348\n",
      "batch 4778: loss 0.297883\n",
      "batch 4779: loss 0.317545\n",
      "batch 4780: loss 0.237539\n",
      "batch 4781: loss 0.255863\n",
      "batch 4782: loss 0.215147\n",
      "batch 4783: loss 0.438840\n",
      "batch 4784: loss 0.222126\n",
      "batch 4785: loss 0.286163\n",
      "batch 4786: loss 0.379026\n",
      "batch 4787: loss 0.246586\n",
      "batch 4788: loss 0.321309\n",
      "batch 4789: loss 0.183584\n",
      "batch 4790: loss 0.189303\n",
      "batch 4791: loss 0.186689\n",
      "batch 4792: loss 0.331757\n",
      "batch 4793: loss 0.350393\n",
      "batch 4794: loss 0.141746\n",
      "batch 4795: loss 0.290164\n",
      "batch 4796: loss 0.235363\n",
      "batch 4797: loss 0.495159\n",
      "batch 4798: loss 0.446470\n",
      "batch 4799: loss 0.454657\n",
      "batch 4800: loss 0.291920\n",
      "batch 4801: loss 0.343021\n",
      "batch 4802: loss 0.371609\n",
      "batch 4803: loss 0.306198\n",
      "batch 4804: loss 0.300599\n",
      "batch 4805: loss 0.338119\n",
      "batch 4806: loss 0.193978\n",
      "batch 4807: loss 0.247833\n",
      "batch 4808: loss 0.229900\n",
      "batch 4809: loss 0.345389\n",
      "batch 4810: loss 0.344067\n",
      "batch 4811: loss 0.223736\n",
      "batch 4812: loss 0.329643\n",
      "batch 4813: loss 0.234152\n",
      "batch 4814: loss 0.192588\n",
      "batch 4815: loss 0.337744\n",
      "batch 4816: loss 0.316834\n",
      "batch 4817: loss 0.227130\n",
      "batch 4818: loss 0.203053\n",
      "batch 4819: loss 0.381715\n",
      "batch 4820: loss 0.181570\n",
      "batch 4821: loss 0.224595\n",
      "batch 4822: loss 0.364003\n",
      "batch 4823: loss 0.382047\n",
      "batch 4824: loss 0.492347\n",
      "batch 4825: loss 0.413032\n",
      "batch 4826: loss 0.420221\n",
      "batch 4827: loss 0.402184\n",
      "batch 4828: loss 0.338192\n",
      "batch 4829: loss 0.236030\n",
      "batch 4830: loss 0.260268\n",
      "batch 4831: loss 0.401158\n",
      "batch 4832: loss 0.221347\n",
      "batch 4833: loss 0.347585\n",
      "batch 4834: loss 0.290822\n",
      "batch 4835: loss 0.284450\n",
      "batch 4836: loss 0.544071\n",
      "batch 4837: loss 0.246992\n",
      "batch 4838: loss 0.483625\n",
      "batch 4839: loss 0.407069\n",
      "batch 4840: loss 0.235668\n",
      "batch 4841: loss 0.473576\n",
      "batch 4842: loss 0.368642\n",
      "batch 4843: loss 0.265802\n",
      "batch 4844: loss 0.304582\n",
      "batch 4845: loss 0.406293\n",
      "batch 4846: loss 0.178133\n",
      "batch 4847: loss 0.473458\n",
      "batch 4848: loss 0.177412\n",
      "batch 4849: loss 0.278263\n",
      "batch 4850: loss 0.150914\n",
      "batch 4851: loss 0.257384\n",
      "batch 4852: loss 0.330699\n",
      "batch 4853: loss 0.292936\n",
      "batch 4854: loss 0.214050\n",
      "batch 4855: loss 0.177170\n",
      "batch 4856: loss 0.349252\n",
      "batch 4857: loss 0.386465\n",
      "batch 4858: loss 0.399422\n",
      "batch 4859: loss 0.358789\n",
      "batch 4860: loss 0.368812\n",
      "batch 4861: loss 0.231410\n",
      "batch 4862: loss 0.350581\n",
      "batch 4863: loss 0.279550\n",
      "batch 4864: loss 0.194831\n",
      "batch 4865: loss 0.320181\n",
      "batch 4866: loss 0.178513\n",
      "batch 4867: loss 0.403040\n",
      "batch 4868: loss 0.416728\n",
      "batch 4869: loss 0.278672\n",
      "batch 4870: loss 0.407037\n",
      "batch 4871: loss 0.275590\n",
      "batch 4872: loss 0.260265\n",
      "batch 4873: loss 0.301217\n",
      "batch 4874: loss 0.357986\n",
      "batch 4875: loss 0.407768\n",
      "batch 4876: loss 0.314309\n",
      "batch 4877: loss 0.290387\n",
      "batch 4878: loss 0.226883\n",
      "batch 4879: loss 0.240785\n",
      "batch 4880: loss 0.191016\n",
      "batch 4881: loss 0.400722\n",
      "batch 4882: loss 0.141570\n",
      "batch 4883: loss 0.631052\n",
      "batch 4884: loss 0.454448\n",
      "batch 4885: loss 0.511939\n",
      "batch 4886: loss 0.244685\n",
      "batch 4887: loss 0.569218\n",
      "batch 4888: loss 0.171882\n",
      "batch 4889: loss 0.378823\n",
      "batch 4890: loss 0.435121\n",
      "batch 4891: loss 0.345840\n",
      "batch 4892: loss 0.335970\n",
      "batch 4893: loss 0.328167\n",
      "batch 4894: loss 0.412592\n",
      "batch 4895: loss 0.316536\n",
      "batch 4896: loss 0.269444\n",
      "batch 4897: loss 0.499926\n",
      "batch 4898: loss 0.367672\n",
      "batch 4899: loss 0.128723\n",
      "batch 4900: loss 0.414888\n",
      "batch 4901: loss 0.256690\n",
      "batch 4902: loss 0.328465\n",
      "batch 4903: loss 0.218422\n",
      "batch 4904: loss 0.319644\n",
      "batch 4905: loss 0.301681\n",
      "batch 4906: loss 0.577300\n",
      "batch 4907: loss 0.330000\n",
      "batch 4908: loss 0.232847\n",
      "batch 4909: loss 0.437574\n",
      "batch 4910: loss 0.348244\n",
      "batch 4911: loss 0.184905\n",
      "batch 4912: loss 0.410126\n",
      "batch 4913: loss 0.184530\n",
      "batch 4914: loss 0.521770\n",
      "batch 4915: loss 0.289785\n",
      "batch 4916: loss 0.241657\n",
      "batch 4917: loss 0.202301\n",
      "batch 4918: loss 0.303380\n",
      "batch 4919: loss 0.392923\n",
      "batch 4920: loss 0.551192\n",
      "batch 4921: loss 0.325129\n",
      "batch 4922: loss 0.332726\n",
      "batch 4923: loss 0.320470\n",
      "batch 4924: loss 0.241964\n",
      "batch 4925: loss 0.413237\n",
      "batch 4926: loss 0.394160\n",
      "batch 4927: loss 0.492738\n",
      "batch 4928: loss 0.391447\n",
      "batch 4929: loss 0.220469\n",
      "batch 4930: loss 0.364916\n",
      "batch 4931: loss 0.751818\n",
      "batch 4932: loss 0.303751\n",
      "batch 4933: loss 0.267553\n",
      "batch 4934: loss 0.262961\n",
      "batch 4935: loss 0.156204\n",
      "batch 4936: loss 0.479839\n",
      "batch 4937: loss 0.466799\n",
      "batch 4938: loss 0.186251\n",
      "batch 4939: loss 0.375137\n",
      "batch 4940: loss 0.333363\n",
      "batch 4941: loss 0.212500\n",
      "batch 4942: loss 0.188422\n",
      "batch 4943: loss 0.292672\n",
      "batch 4944: loss 0.192395\n",
      "batch 4945: loss 0.263362\n",
      "batch 4946: loss 0.277412\n",
      "batch 4947: loss 0.298508\n",
      "batch 4948: loss 0.371531\n",
      "batch 4949: loss 0.359529\n",
      "batch 4950: loss 0.231313\n",
      "batch 4951: loss 0.351597\n",
      "batch 4952: loss 0.278887\n",
      "batch 4953: loss 0.146994\n",
      "batch 4954: loss 0.285778\n",
      "batch 4955: loss 0.364603\n",
      "batch 4956: loss 0.205422\n",
      "batch 4957: loss 0.234769\n",
      "batch 4958: loss 0.192261\n",
      "batch 4959: loss 0.519525\n",
      "batch 4960: loss 0.514062\n",
      "batch 4961: loss 0.361423\n",
      "batch 4962: loss 0.421075\n",
      "batch 4963: loss 0.482612\n",
      "batch 4964: loss 0.325669\n",
      "batch 4965: loss 0.346660\n",
      "batch 4966: loss 0.559056\n",
      "batch 4967: loss 0.274547\n",
      "batch 4968: loss 0.216353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4969: loss 0.362894\n",
      "batch 4970: loss 0.310885\n",
      "batch 4971: loss 0.412122\n",
      "batch 4972: loss 0.355550\n",
      "batch 4973: loss 0.408989\n",
      "batch 4974: loss 0.305767\n",
      "batch 4975: loss 0.343017\n",
      "batch 4976: loss 0.458871\n",
      "batch 4977: loss 0.193138\n",
      "batch 4978: loss 0.162269\n",
      "batch 4979: loss 0.336476\n",
      "batch 4980: loss 0.359510\n",
      "batch 4981: loss 0.338455\n",
      "batch 4982: loss 0.180851\n",
      "batch 4983: loss 0.194239\n",
      "batch 4984: loss 0.267463\n",
      "batch 4985: loss 0.229498\n",
      "batch 4986: loss 0.253244\n",
      "batch 4987: loss 0.197824\n",
      "batch 4988: loss 0.370272\n",
      "batch 4989: loss 0.486611\n",
      "batch 4990: loss 0.303510\n",
      "batch 4991: loss 0.324826\n",
      "batch 4992: loss 0.497641\n",
      "batch 4993: loss 0.179943\n",
      "batch 4994: loss 0.427349\n",
      "batch 4995: loss 0.295779\n",
      "batch 4996: loss 0.345455\n",
      "batch 4997: loss 0.432800\n",
      "batch 4998: loss 0.203785\n",
      "batch 4999: loss 0.365421\n",
      "batch 5000: loss 0.356777\n",
      "batch 5001: loss 0.272704\n",
      "batch 5002: loss 0.197764\n",
      "batch 5003: loss 0.311141\n",
      "batch 5004: loss 0.428708\n",
      "batch 5005: loss 0.280507\n",
      "batch 5006: loss 0.241052\n",
      "batch 5007: loss 0.256207\n",
      "batch 5008: loss 0.519908\n",
      "batch 5009: loss 0.338903\n",
      "batch 5010: loss 0.424606\n",
      "batch 5011: loss 0.250543\n",
      "batch 5012: loss 0.167284\n",
      "batch 5013: loss 0.231234\n",
      "batch 5014: loss 0.457154\n",
      "batch 5015: loss 0.227764\n",
      "batch 5016: loss 0.370676\n",
      "batch 5017: loss 0.231157\n",
      "batch 5018: loss 0.182348\n",
      "batch 5019: loss 0.309035\n",
      "batch 5020: loss 0.375294\n",
      "batch 5021: loss 0.286803\n",
      "batch 5022: loss 0.254561\n",
      "batch 5023: loss 0.307872\n",
      "batch 5024: loss 0.216120\n",
      "batch 5025: loss 0.230678\n",
      "batch 5026: loss 0.285971\n",
      "batch 5027: loss 0.402684\n",
      "batch 5028: loss 0.389088\n",
      "batch 5029: loss 0.321502\n",
      "batch 5030: loss 0.645409\n",
      "batch 5031: loss 0.306225\n",
      "batch 5032: loss 0.359747\n",
      "batch 5033: loss 0.296904\n",
      "batch 5034: loss 0.250878\n",
      "batch 5035: loss 0.235256\n",
      "batch 5036: loss 0.248651\n",
      "batch 5037: loss 0.281330\n",
      "batch 5038: loss 0.303269\n",
      "batch 5039: loss 0.379842\n",
      "batch 5040: loss 0.253525\n",
      "batch 5041: loss 0.313047\n",
      "batch 5042: loss 0.338558\n",
      "batch 5043: loss 0.313102\n",
      "batch 5044: loss 0.249709\n",
      "batch 5045: loss 0.240526\n",
      "batch 5046: loss 0.283865\n",
      "batch 5047: loss 0.312031\n",
      "batch 5048: loss 0.472695\n",
      "batch 5049: loss 0.185140\n",
      "batch 5050: loss 0.251095\n",
      "batch 5051: loss 0.297210\n",
      "batch 5052: loss 0.325852\n",
      "batch 5053: loss 0.195843\n",
      "batch 5054: loss 0.248431\n",
      "batch 5055: loss 0.193349\n",
      "batch 5056: loss 0.348981\n",
      "batch 5057: loss 0.315741\n",
      "batch 5058: loss 0.239000\n",
      "batch 5059: loss 0.417033\n",
      "batch 5060: loss 0.212929\n",
      "batch 5061: loss 0.327274\n",
      "batch 5062: loss 0.297994\n",
      "batch 5063: loss 0.340942\n",
      "batch 5064: loss 0.178725\n",
      "batch 5065: loss 0.468347\n",
      "batch 5066: loss 0.423807\n",
      "batch 5067: loss 0.343742\n",
      "batch 5068: loss 0.253060\n",
      "batch 5069: loss 0.291665\n",
      "batch 5070: loss 0.317791\n",
      "batch 5071: loss 0.323512\n",
      "batch 5072: loss 0.444015\n",
      "batch 5073: loss 0.548744\n",
      "batch 5074: loss 0.225713\n",
      "batch 5075: loss 0.458652\n",
      "batch 5076: loss 0.392843\n",
      "batch 5077: loss 0.410574\n",
      "batch 5078: loss 0.526420\n",
      "batch 5079: loss 0.257263\n",
      "batch 5080: loss 0.384128\n",
      "batch 5081: loss 0.299826\n",
      "batch 5082: loss 0.400882\n",
      "batch 5083: loss 0.358727\n",
      "batch 5084: loss 0.299658\n",
      "batch 5085: loss 0.188847\n",
      "batch 5086: loss 0.236550\n",
      "batch 5087: loss 0.237836\n",
      "batch 5088: loss 0.213880\n",
      "batch 5089: loss 0.178069\n",
      "batch 5090: loss 0.416331\n",
      "batch 5091: loss 0.215004\n",
      "batch 5092: loss 0.356644\n",
      "batch 5093: loss 0.300703\n",
      "batch 5094: loss 0.661571\n",
      "batch 5095: loss 0.359198\n",
      "batch 5096: loss 0.422633\n",
      "batch 5097: loss 0.324121\n",
      "batch 5098: loss 0.241096\n",
      "batch 5099: loss 0.305037\n",
      "batch 5100: loss 0.208878\n",
      "batch 5101: loss 0.251301\n",
      "batch 5102: loss 0.351564\n",
      "batch 5103: loss 0.333034\n",
      "batch 5104: loss 0.528213\n",
      "batch 5105: loss 0.353755\n",
      "batch 5106: loss 0.328197\n",
      "batch 5107: loss 0.365007\n",
      "batch 5108: loss 0.339386\n",
      "batch 5109: loss 0.225725\n",
      "batch 5110: loss 0.258466\n",
      "batch 5111: loss 0.389368\n",
      "batch 5112: loss 0.300327\n",
      "batch 5113: loss 0.372798\n",
      "batch 5114: loss 0.411742\n",
      "batch 5115: loss 0.402132\n",
      "batch 5116: loss 0.260771\n",
      "batch 5117: loss 0.271149\n",
      "batch 5118: loss 0.436639\n",
      "batch 5119: loss 0.254835\n",
      "batch 5120: loss 0.254477\n",
      "batch 5121: loss 0.201875\n",
      "batch 5122: loss 0.177879\n",
      "batch 5123: loss 0.392858\n",
      "batch 5124: loss 0.171508\n",
      "batch 5125: loss 0.249136\n",
      "batch 5126: loss 0.244993\n",
      "batch 5127: loss 0.376083\n",
      "batch 5128: loss 0.292410\n",
      "batch 5129: loss 0.388372\n",
      "batch 5130: loss 0.400451\n",
      "batch 5131: loss 0.221910\n",
      "batch 5132: loss 0.445045\n",
      "batch 5133: loss 0.445818\n",
      "batch 5134: loss 0.403131\n",
      "batch 5135: loss 0.173075\n",
      "batch 5136: loss 0.474760\n",
      "batch 5137: loss 0.306925\n",
      "batch 5138: loss 0.363806\n",
      "batch 5139: loss 0.211116\n",
      "batch 5140: loss 0.385765\n",
      "batch 5141: loss 0.276317\n",
      "batch 5142: loss 0.300616\n",
      "batch 5143: loss 0.300808\n",
      "batch 5144: loss 0.418561\n",
      "batch 5145: loss 0.469515\n",
      "batch 5146: loss 0.324213\n",
      "batch 5147: loss 0.237538\n",
      "batch 5148: loss 0.247979\n",
      "batch 5149: loss 0.300977\n",
      "batch 5150: loss 0.321568\n",
      "batch 5151: loss 0.253724\n",
      "batch 5152: loss 0.251654\n",
      "batch 5153: loss 0.276238\n",
      "batch 5154: loss 0.082640\n",
      "batch 5155: loss 0.352499\n",
      "batch 5156: loss 0.245239\n",
      "batch 5157: loss 0.185614\n",
      "batch 5158: loss 0.424358\n",
      "batch 5159: loss 0.335771\n",
      "batch 5160: loss 0.378553\n",
      "batch 5161: loss 0.174436\n",
      "batch 5162: loss 0.286096\n",
      "batch 5163: loss 0.327797\n",
      "batch 5164: loss 0.232870\n",
      "batch 5165: loss 0.210178\n",
      "batch 5166: loss 0.283395\n",
      "batch 5167: loss 0.208987\n",
      "batch 5168: loss 0.585925\n",
      "batch 5169: loss 0.322218\n",
      "batch 5170: loss 0.330056\n",
      "batch 5171: loss 0.384160\n",
      "batch 5172: loss 0.247258\n",
      "batch 5173: loss 0.390683\n",
      "batch 5174: loss 0.547891\n",
      "batch 5175: loss 0.396128\n",
      "batch 5176: loss 0.382652\n",
      "batch 5177: loss 0.321747\n",
      "batch 5178: loss 0.321300\n",
      "batch 5179: loss 0.382671\n",
      "batch 5180: loss 0.342006\n",
      "batch 5181: loss 0.202590\n",
      "batch 5182: loss 0.308894\n",
      "batch 5183: loss 0.254863\n",
      "batch 5184: loss 0.403979\n",
      "batch 5185: loss 0.344840\n",
      "batch 5186: loss 0.388937\n",
      "batch 5187: loss 0.267659\n",
      "batch 5188: loss 0.430111\n",
      "batch 5189: loss 0.157114\n",
      "batch 5190: loss 0.602521\n",
      "batch 5191: loss 0.215902\n",
      "batch 5192: loss 0.303138\n",
      "batch 5193: loss 0.327302\n",
      "batch 5194: loss 0.363986\n",
      "batch 5195: loss 0.359809\n",
      "batch 5196: loss 0.406407\n",
      "batch 5197: loss 0.396238\n",
      "batch 5198: loss 0.314430\n",
      "batch 5199: loss 0.266353\n",
      "batch 5200: loss 0.396402\n",
      "batch 5201: loss 0.294203\n",
      "batch 5202: loss 0.221630\n",
      "batch 5203: loss 0.369524\n",
      "batch 5204: loss 0.395448\n",
      "batch 5205: loss 0.292740\n",
      "batch 5206: loss 0.319661\n",
      "batch 5207: loss 0.305291\n",
      "batch 5208: loss 0.250416\n",
      "batch 5209: loss 0.236652\n",
      "batch 5210: loss 0.288145\n",
      "batch 5211: loss 0.212050\n",
      "batch 5212: loss 0.321820\n",
      "batch 5213: loss 0.432862\n",
      "batch 5214: loss 0.272018\n",
      "batch 5215: loss 0.358669\n",
      "batch 5216: loss 0.209267\n",
      "batch 5217: loss 0.230658\n",
      "batch 5218: loss 0.592018\n",
      "batch 5219: loss 0.246316\n",
      "batch 5220: loss 0.348413\n",
      "batch 5221: loss 0.376079\n",
      "batch 5222: loss 0.456150\n",
      "batch 5223: loss 0.450164\n",
      "batch 5224: loss 0.359061\n",
      "batch 5225: loss 0.167280\n",
      "batch 5226: loss 0.436327\n",
      "batch 5227: loss 0.191023\n",
      "batch 5228: loss 0.280161\n",
      "batch 5229: loss 0.423872\n",
      "batch 5230: loss 0.167378\n",
      "batch 5231: loss 0.312294\n",
      "batch 5232: loss 0.205212\n",
      "batch 5233: loss 0.230952\n",
      "batch 5234: loss 0.252306\n",
      "batch 5235: loss 0.244292\n",
      "batch 5236: loss 0.431712\n",
      "batch 5237: loss 0.383754\n",
      "batch 5238: loss 0.416310\n",
      "batch 5239: loss 0.392863\n",
      "batch 5240: loss 0.373538\n",
      "batch 5241: loss 0.174252\n",
      "batch 5242: loss 0.354098\n",
      "batch 5243: loss 0.289548\n",
      "batch 5244: loss 0.222125\n",
      "batch 5245: loss 0.283995\n",
      "batch 5246: loss 0.370348\n",
      "batch 5247: loss 0.197436\n",
      "batch 5248: loss 0.216114\n",
      "batch 5249: loss 0.243283\n",
      "batch 5250: loss 0.399705\n",
      "batch 5251: loss 0.174516\n",
      "batch 5252: loss 0.229715\n",
      "batch 5253: loss 0.305462\n",
      "batch 5254: loss 0.371004\n",
      "batch 5255: loss 0.278313\n",
      "batch 5256: loss 0.234584\n",
      "batch 5257: loss 0.368377\n",
      "batch 5258: loss 0.282294\n",
      "batch 5259: loss 0.545068\n",
      "batch 5260: loss 0.398256\n",
      "batch 5261: loss 0.305508\n",
      "batch 5262: loss 0.108953\n",
      "batch 5263: loss 0.329352\n",
      "batch 5264: loss 0.281696\n",
      "batch 5265: loss 0.248168\n",
      "batch 5266: loss 0.156343\n",
      "batch 5267: loss 0.159155\n",
      "batch 5268: loss 0.493832\n",
      "batch 5269: loss 0.470946\n",
      "batch 5270: loss 0.254576\n",
      "batch 5271: loss 0.887619\n",
      "batch 5272: loss 0.321492\n",
      "batch 5273: loss 0.270950\n",
      "batch 5274: loss 0.529643\n",
      "batch 5275: loss 0.255705\n",
      "batch 5276: loss 0.321973\n",
      "batch 5277: loss 0.302120\n",
      "batch 5278: loss 0.347085\n",
      "batch 5279: loss 0.295413\n",
      "batch 5280: loss 0.378816\n",
      "batch 5281: loss 0.568654\n",
      "batch 5282: loss 0.313645\n",
      "batch 5283: loss 0.223208\n",
      "batch 5284: loss 0.232941\n",
      "batch 5285: loss 0.292229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5286: loss 0.217389\n",
      "batch 5287: loss 0.336750\n",
      "batch 5288: loss 0.236941\n",
      "batch 5289: loss 0.447980\n",
      "batch 5290: loss 0.668284\n",
      "batch 5291: loss 0.272877\n",
      "batch 5292: loss 0.271931\n",
      "batch 5293: loss 0.226669\n",
      "batch 5294: loss 0.122236\n",
      "batch 5295: loss 0.563261\n",
      "batch 5296: loss 0.367247\n",
      "batch 5297: loss 0.258638\n",
      "batch 5298: loss 0.325083\n",
      "batch 5299: loss 0.557532\n",
      "batch 5300: loss 0.316798\n",
      "batch 5301: loss 0.370768\n",
      "batch 5302: loss 0.213468\n",
      "batch 5303: loss 0.164034\n",
      "batch 5304: loss 0.403642\n",
      "batch 5305: loss 0.263582\n",
      "batch 5306: loss 0.376544\n",
      "batch 5307: loss 0.300007\n",
      "batch 5308: loss 0.441219\n",
      "batch 5309: loss 0.329667\n",
      "batch 5310: loss 0.400552\n",
      "batch 5311: loss 0.288302\n",
      "batch 5312: loss 0.261652\n",
      "batch 5313: loss 0.293427\n",
      "batch 5314: loss 0.201144\n",
      "batch 5315: loss 0.171387\n",
      "batch 5316: loss 0.212864\n",
      "batch 5317: loss 0.230946\n",
      "batch 5318: loss 0.179237\n",
      "batch 5319: loss 0.208611\n",
      "batch 5320: loss 0.377539\n",
      "batch 5321: loss 0.311187\n",
      "batch 5322: loss 0.799118\n",
      "batch 5323: loss 0.357538\n",
      "batch 5324: loss 0.162933\n",
      "batch 5325: loss 0.268507\n",
      "batch 5326: loss 0.403911\n",
      "batch 5327: loss 0.489612\n",
      "batch 5328: loss 0.306740\n",
      "batch 5329: loss 0.479418\n",
      "batch 5330: loss 0.211882\n",
      "batch 5331: loss 0.297679\n",
      "batch 5332: loss 0.216531\n",
      "batch 5333: loss 0.317820\n",
      "batch 5334: loss 0.518355\n",
      "batch 5335: loss 0.226267\n",
      "batch 5336: loss 0.457277\n",
      "batch 5337: loss 0.430659\n",
      "batch 5338: loss 0.184905\n",
      "batch 5339: loss 0.494169\n",
      "batch 5340: loss 0.300316\n",
      "batch 5341: loss 0.222412\n",
      "batch 5342: loss 0.251483\n",
      "batch 5343: loss 0.263720\n",
      "batch 5344: loss 0.197551\n",
      "batch 5345: loss 0.226354\n",
      "batch 5346: loss 0.322040\n",
      "batch 5347: loss 0.400164\n",
      "batch 5348: loss 0.193930\n",
      "batch 5349: loss 0.372297\n",
      "batch 5350: loss 0.380424\n",
      "batch 5351: loss 0.119463\n",
      "batch 5352: loss 0.370612\n",
      "batch 5353: loss 0.359990\n",
      "batch 5354: loss 0.352429\n",
      "batch 5355: loss 0.302592\n",
      "batch 5356: loss 0.351621\n",
      "batch 5357: loss 0.420249\n",
      "batch 5358: loss 0.266716\n",
      "batch 5359: loss 0.259003\n",
      "batch 5360: loss 0.311920\n",
      "batch 5361: loss 0.252460\n",
      "batch 5362: loss 0.423672\n",
      "batch 5363: loss 0.323013\n",
      "batch 5364: loss 0.378715\n",
      "batch 5365: loss 0.317251\n",
      "batch 5366: loss 0.361667\n",
      "batch 5367: loss 0.493015\n",
      "batch 5368: loss 0.339463\n",
      "batch 5369: loss 0.123743\n",
      "batch 5370: loss 0.341793\n",
      "batch 5371: loss 0.176996\n",
      "batch 5372: loss 0.378924\n",
      "batch 5373: loss 0.302643\n",
      "batch 5374: loss 0.328045\n",
      "batch 5375: loss 0.280932\n",
      "batch 5376: loss 0.284519\n",
      "batch 5377: loss 0.211457\n",
      "batch 5378: loss 0.172904\n",
      "batch 5379: loss 0.370849\n",
      "batch 5380: loss 0.252917\n",
      "batch 5381: loss 0.383615\n",
      "batch 5382: loss 0.112914\n",
      "batch 5383: loss 0.641902\n",
      "batch 5384: loss 0.444277\n",
      "batch 5385: loss 0.413679\n",
      "batch 5386: loss 0.277036\n",
      "batch 5387: loss 0.350016\n",
      "batch 5388: loss 0.311069\n",
      "batch 5389: loss 0.275576\n",
      "batch 5390: loss 0.202324\n",
      "batch 5391: loss 0.299833\n",
      "batch 5392: loss 0.221580\n",
      "batch 5393: loss 0.321432\n",
      "batch 5394: loss 0.301292\n",
      "batch 5395: loss 0.447693\n",
      "batch 5396: loss 0.353889\n",
      "batch 5397: loss 0.361500\n",
      "batch 5398: loss 0.377518\n",
      "batch 5399: loss 0.287360\n",
      "batch 5400: loss 0.337026\n",
      "batch 5401: loss 0.288843\n",
      "batch 5402: loss 0.492190\n",
      "batch 5403: loss 0.215825\n",
      "batch 5404: loss 0.306518\n",
      "batch 5405: loss 0.243879\n",
      "batch 5406: loss 0.269784\n",
      "batch 5407: loss 0.243451\n",
      "batch 5408: loss 0.303519\n",
      "batch 5409: loss 0.165835\n",
      "batch 5410: loss 0.360840\n",
      "batch 5411: loss 0.334584\n",
      "batch 5412: loss 0.267317\n",
      "batch 5413: loss 0.307260\n",
      "batch 5414: loss 0.399849\n",
      "batch 5415: loss 0.386173\n",
      "batch 5416: loss 0.269069\n",
      "batch 5417: loss 0.231174\n",
      "batch 5418: loss 0.236589\n",
      "batch 5419: loss 0.314507\n",
      "batch 5420: loss 0.234572\n",
      "batch 5421: loss 0.396552\n",
      "batch 5422: loss 0.378426\n",
      "batch 5423: loss 0.412078\n",
      "batch 5424: loss 0.450850\n",
      "batch 5425: loss 0.207511\n",
      "batch 5426: loss 0.304789\n",
      "batch 5427: loss 0.329486\n",
      "batch 5428: loss 0.209856\n",
      "batch 5429: loss 0.196492\n",
      "batch 5430: loss 0.261020\n",
      "batch 5431: loss 0.258079\n",
      "batch 5432: loss 0.210830\n",
      "batch 5433: loss 0.296648\n",
      "batch 5434: loss 0.255960\n",
      "batch 5435: loss 0.157534\n",
      "batch 5436: loss 0.235571\n",
      "batch 5437: loss 0.308194\n",
      "batch 5438: loss 0.263150\n",
      "batch 5439: loss 0.351709\n",
      "batch 5440: loss 0.221408\n",
      "batch 5441: loss 0.291652\n",
      "batch 5442: loss 0.239449\n",
      "batch 5443: loss 0.285183\n",
      "batch 5444: loss 0.411281\n",
      "batch 5445: loss 0.326991\n",
      "batch 5446: loss 0.208104\n",
      "batch 5447: loss 0.239765\n",
      "batch 5448: loss 0.429177\n",
      "batch 5449: loss 0.318828\n",
      "batch 5450: loss 0.287227\n",
      "batch 5451: loss 0.188192\n",
      "batch 5452: loss 0.482587\n",
      "batch 5453: loss 0.443953\n",
      "batch 5454: loss 0.298377\n",
      "batch 5455: loss 0.239741\n",
      "batch 5456: loss 0.357887\n",
      "batch 5457: loss 0.317029\n",
      "batch 5458: loss 0.349865\n",
      "batch 5459: loss 0.399406\n",
      "batch 5460: loss 0.144880\n",
      "batch 5461: loss 0.304647\n",
      "batch 5462: loss 0.348623\n",
      "batch 5463: loss 0.297965\n",
      "batch 5464: loss 0.190989\n",
      "batch 5465: loss 0.517860\n",
      "batch 5466: loss 0.165320\n",
      "batch 5467: loss 0.212323\n",
      "batch 5468: loss 0.290581\n",
      "batch 5469: loss 0.313308\n",
      "batch 5470: loss 0.257157\n",
      "batch 5471: loss 0.350248\n",
      "batch 5472: loss 0.293091\n",
      "batch 5473: loss 0.280208\n",
      "batch 5474: loss 0.338685\n",
      "batch 5475: loss 0.273868\n",
      "batch 5476: loss 0.187700\n",
      "batch 5477: loss 0.316959\n",
      "batch 5478: loss 0.358907\n",
      "batch 5479: loss 0.423312\n",
      "batch 5480: loss 0.548682\n",
      "batch 5481: loss 0.387593\n",
      "batch 5482: loss 0.436126\n",
      "batch 5483: loss 0.335698\n",
      "batch 5484: loss 0.463144\n",
      "batch 5485: loss 0.337506\n",
      "batch 5486: loss 0.267490\n",
      "batch 5487: loss 0.339400\n",
      "batch 5488: loss 0.299935\n",
      "batch 5489: loss 0.422584\n",
      "batch 5490: loss 0.238903\n",
      "batch 5491: loss 0.263152\n",
      "batch 5492: loss 0.306711\n",
      "batch 5493: loss 0.276118\n",
      "batch 5494: loss 0.559199\n",
      "batch 5495: loss 0.226072\n",
      "batch 5496: loss 0.189321\n",
      "batch 5497: loss 0.232413\n",
      "batch 5498: loss 0.255353\n",
      "batch 5499: loss 0.444738\n",
      "batch 5500: loss 0.167804\n",
      "batch 5501: loss 0.242047\n",
      "batch 5502: loss 0.543157\n",
      "batch 5503: loss 0.292550\n",
      "batch 5504: loss 0.302712\n",
      "batch 5505: loss 0.259630\n",
      "batch 5506: loss 0.271142\n",
      "batch 5507: loss 0.251287\n",
      "batch 5508: loss 0.230766\n",
      "batch 5509: loss 0.614595\n",
      "batch 5510: loss 0.322574\n",
      "batch 5511: loss 0.446946\n",
      "batch 5512: loss 0.395649\n",
      "batch 5513: loss 0.281122\n",
      "batch 5514: loss 0.198162\n",
      "batch 5515: loss 0.367138\n",
      "batch 5516: loss 0.307799\n",
      "batch 5517: loss 0.229178\n",
      "batch 5518: loss 0.160734\n",
      "batch 5519: loss 0.342634\n",
      "batch 5520: loss 0.180311\n",
      "batch 5521: loss 0.261054\n",
      "batch 5522: loss 0.340292\n",
      "batch 5523: loss 0.228365\n",
      "batch 5524: loss 0.199012\n",
      "batch 5525: loss 0.430317\n",
      "batch 5526: loss 0.202452\n",
      "batch 5527: loss 0.263857\n",
      "batch 5528: loss 0.327844\n",
      "batch 5529: loss 0.352376\n",
      "batch 5530: loss 0.546984\n",
      "batch 5531: loss 0.258908\n",
      "batch 5532: loss 0.150219\n",
      "batch 5533: loss 0.581367\n",
      "batch 5534: loss 0.139251\n",
      "batch 5535: loss 0.244075\n",
      "batch 5536: loss 0.290068\n",
      "batch 5537: loss 0.267310\n",
      "batch 5538: loss 0.283590\n",
      "batch 5539: loss 0.179407\n",
      "batch 5540: loss 0.398803\n",
      "batch 5541: loss 0.338963\n",
      "batch 5542: loss 0.341335\n",
      "batch 5543: loss 0.249890\n",
      "batch 5544: loss 0.323785\n",
      "batch 5545: loss 0.287789\n",
      "batch 5546: loss 0.307119\n",
      "batch 5547: loss 0.204305\n",
      "batch 5548: loss 0.482558\n",
      "batch 5549: loss 0.332432\n",
      "batch 5550: loss 0.398419\n",
      "batch 5551: loss 0.421794\n",
      "batch 5552: loss 0.332632\n",
      "batch 5553: loss 0.350205\n",
      "batch 5554: loss 0.285114\n",
      "batch 5555: loss 0.168097\n",
      "batch 5556: loss 0.268941\n",
      "batch 5557: loss 0.349147\n",
      "batch 5558: loss 0.353855\n",
      "batch 5559: loss 0.348371\n",
      "batch 5560: loss 0.312357\n",
      "batch 5561: loss 0.225722\n",
      "batch 5562: loss 0.349204\n",
      "batch 5563: loss 0.160677\n",
      "batch 5564: loss 0.341913\n",
      "batch 5565: loss 0.364084\n",
      "batch 5566: loss 0.196851\n",
      "batch 5567: loss 0.273620\n",
      "batch 5568: loss 0.288757\n",
      "batch 5569: loss 0.314262\n",
      "batch 5570: loss 0.294314\n",
      "batch 5571: loss 0.421620\n",
      "batch 5572: loss 0.234496\n",
      "batch 5573: loss 0.270407\n",
      "batch 5574: loss 0.368861\n",
      "batch 5575: loss 0.660420\n",
      "batch 5576: loss 0.211090\n",
      "batch 5577: loss 0.465501\n",
      "batch 5578: loss 0.265804\n",
      "batch 5579: loss 0.371594\n",
      "batch 5580: loss 0.229193\n",
      "batch 5581: loss 0.160698\n",
      "batch 5582: loss 0.260950\n",
      "batch 5583: loss 0.373351\n",
      "batch 5584: loss 0.546176\n",
      "batch 5585: loss 0.351029\n",
      "batch 5586: loss 0.293105\n",
      "batch 5587: loss 0.351327\n",
      "batch 5588: loss 0.349206\n",
      "batch 5589: loss 0.286176\n",
      "batch 5590: loss 0.209020\n",
      "batch 5591: loss 0.461682\n",
      "batch 5592: loss 0.394113\n",
      "batch 5593: loss 0.407370\n",
      "batch 5594: loss 0.179895\n",
      "batch 5595: loss 0.204193\n",
      "batch 5596: loss 0.405279\n",
      "batch 5597: loss 0.339314\n",
      "batch 5598: loss 0.310386\n",
      "batch 5599: loss 0.294958\n",
      "batch 5600: loss 0.255306\n",
      "batch 5601: loss 0.280551\n",
      "batch 5602: loss 0.266356\n",
      "batch 5603: loss 0.232085\n",
      "batch 5604: loss 0.487129\n",
      "batch 5605: loss 0.346620\n",
      "batch 5606: loss 0.348717\n",
      "batch 5607: loss 0.196337\n",
      "batch 5608: loss 0.365760\n",
      "batch 5609: loss 0.388556\n",
      "batch 5610: loss 0.123856\n",
      "batch 5611: loss 0.213224\n",
      "batch 5612: loss 0.418366\n",
      "batch 5613: loss 0.238559\n",
      "batch 5614: loss 0.323151\n",
      "batch 5615: loss 0.483680\n",
      "batch 5616: loss 0.188962\n",
      "batch 5617: loss 0.584030\n",
      "batch 5618: loss 0.187908\n",
      "batch 5619: loss 0.527201\n",
      "batch 5620: loss 0.321257\n",
      "batch 5621: loss 0.289390\n",
      "batch 5622: loss 0.243281\n",
      "batch 5623: loss 0.374198\n",
      "batch 5624: loss 0.446949\n",
      "batch 5625: loss 0.383970\n",
      "batch 5626: loss 0.299405\n",
      "batch 5627: loss 0.284363\n",
      "batch 5628: loss 0.309696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5629: loss 0.378315\n",
      "batch 5630: loss 0.146096\n",
      "batch 5631: loss 0.281239\n",
      "batch 5632: loss 0.426213\n",
      "batch 5633: loss 0.237540\n",
      "batch 5634: loss 0.292622\n",
      "batch 5635: loss 0.390762\n",
      "batch 5636: loss 0.469980\n",
      "batch 5637: loss 0.328108\n",
      "batch 5638: loss 0.447359\n",
      "batch 5639: loss 0.266574\n",
      "batch 5640: loss 0.303157\n",
      "batch 5641: loss 0.192851\n",
      "batch 5642: loss 0.248874\n",
      "batch 5643: loss 0.233113\n",
      "batch 5644: loss 0.312435\n",
      "batch 5645: loss 0.230928\n",
      "batch 5646: loss 0.305087\n",
      "batch 5647: loss 0.200762\n",
      "batch 5648: loss 0.411284\n",
      "batch 5649: loss 0.275000\n",
      "batch 5650: loss 0.443336\n",
      "batch 5651: loss 0.485838\n",
      "batch 5652: loss 0.155252\n",
      "batch 5653: loss 0.416618\n",
      "batch 5654: loss 0.322819\n",
      "batch 5655: loss 0.202510\n",
      "batch 5656: loss 0.366865\n",
      "batch 5657: loss 0.497065\n",
      "batch 5658: loss 0.269639\n",
      "batch 5659: loss 0.279006\n",
      "batch 5660: loss 0.234161\n",
      "batch 5661: loss 0.352677\n",
      "batch 5662: loss 0.286405\n",
      "batch 5663: loss 0.396766\n",
      "batch 5664: loss 0.515921\n",
      "batch 5665: loss 0.461198\n",
      "batch 5666: loss 0.504294\n",
      "batch 5667: loss 0.325495\n",
      "batch 5668: loss 0.300315\n",
      "batch 5669: loss 0.288618\n",
      "batch 5670: loss 0.267369\n",
      "batch 5671: loss 0.288647\n",
      "batch 5672: loss 0.252098\n",
      "batch 5673: loss 0.186651\n",
      "batch 5674: loss 0.253000\n",
      "batch 5675: loss 0.247508\n",
      "batch 5676: loss 0.337646\n",
      "batch 5677: loss 0.369601\n",
      "batch 5678: loss 0.350880\n",
      "batch 5679: loss 0.244150\n",
      "batch 5680: loss 0.361149\n",
      "batch 5681: loss 0.280604\n",
      "batch 5682: loss 0.230542\n",
      "batch 5683: loss 0.329793\n",
      "batch 5684: loss 0.442374\n",
      "batch 5685: loss 0.153114\n",
      "batch 5686: loss 0.326285\n",
      "batch 5687: loss 0.297191\n",
      "batch 5688: loss 0.373926\n",
      "batch 5689: loss 0.454488\n",
      "batch 5690: loss 0.380727\n",
      "batch 5691: loss 0.269901\n",
      "batch 5692: loss 0.212038\n",
      "batch 5693: loss 0.281523\n",
      "batch 5694: loss 0.292785\n",
      "batch 5695: loss 0.398168\n",
      "batch 5696: loss 0.485432\n",
      "batch 5697: loss 0.294924\n",
      "batch 5698: loss 0.228381\n",
      "batch 5699: loss 0.128000\n",
      "batch 5700: loss 0.323281\n",
      "batch 5701: loss 0.255763\n",
      "batch 5702: loss 0.401869\n",
      "batch 5703: loss 0.282066\n",
      "batch 5704: loss 0.414514\n",
      "batch 5705: loss 0.192362\n",
      "batch 5706: loss 0.259859\n",
      "batch 5707: loss 0.272009\n",
      "batch 5708: loss 0.349008\n",
      "batch 5709: loss 0.308808\n",
      "batch 5710: loss 0.283898\n",
      "batch 5711: loss 0.254303\n",
      "batch 5712: loss 0.275007\n",
      "batch 5713: loss 0.211180\n",
      "batch 5714: loss 0.397549\n",
      "batch 5715: loss 0.513918\n",
      "batch 5716: loss 0.214092\n",
      "batch 5717: loss 0.361327\n",
      "batch 5718: loss 0.500235\n",
      "batch 5719: loss 0.297935\n",
      "batch 5720: loss 0.406545\n",
      "batch 5721: loss 0.238048\n",
      "batch 5722: loss 0.228813\n",
      "batch 5723: loss 0.244283\n",
      "batch 5724: loss 0.177802\n",
      "batch 5725: loss 0.350570\n",
      "batch 5726: loss 0.443161\n",
      "batch 5727: loss 0.254255\n",
      "batch 5728: loss 0.361085\n",
      "batch 5729: loss 0.416009\n",
      "batch 5730: loss 0.276711\n",
      "batch 5731: loss 0.277177\n",
      "batch 5732: loss 0.317817\n",
      "batch 5733: loss 0.322714\n",
      "batch 5734: loss 0.232297\n",
      "batch 5735: loss 0.244213\n",
      "batch 5736: loss 0.281947\n",
      "batch 5737: loss 0.370461\n",
      "batch 5738: loss 0.271105\n",
      "batch 5739: loss 0.413207\n",
      "batch 5740: loss 0.352015\n",
      "batch 5741: loss 0.268297\n",
      "batch 5742: loss 0.484546\n",
      "batch 5743: loss 0.451053\n",
      "batch 5744: loss 0.103687\n",
      "batch 5745: loss 0.241759\n",
      "batch 5746: loss 0.437158\n",
      "batch 5747: loss 0.219591\n",
      "batch 5748: loss 0.322996\n",
      "batch 5749: loss 0.486665\n",
      "batch 5750: loss 0.270545\n",
      "batch 5751: loss 0.270260\n",
      "batch 5752: loss 0.353895\n",
      "batch 5753: loss 0.277618\n",
      "batch 5754: loss 0.354604\n",
      "batch 5755: loss 0.230283\n",
      "batch 5756: loss 0.255715\n",
      "batch 5757: loss 0.306250\n",
      "batch 5758: loss 0.209855\n",
      "batch 5759: loss 0.304338\n",
      "batch 5760: loss 0.538892\n",
      "batch 5761: loss 0.275618\n",
      "batch 5762: loss 0.280643\n",
      "batch 5763: loss 0.383968\n",
      "batch 5764: loss 0.570352\n",
      "batch 5765: loss 0.345847\n",
      "batch 5766: loss 0.351133\n",
      "batch 5767: loss 0.391097\n",
      "batch 5768: loss 0.559294\n",
      "batch 5769: loss 0.382688\n",
      "batch 5770: loss 0.279895\n",
      "batch 5771: loss 0.383390\n",
      "batch 5772: loss 0.328428\n",
      "batch 5773: loss 0.336617\n",
      "batch 5774: loss 0.253602\n",
      "batch 5775: loss 0.311604\n",
      "batch 5776: loss 0.325352\n",
      "batch 5777: loss 0.199624\n",
      "batch 5778: loss 0.228834\n",
      "batch 5779: loss 0.451486\n",
      "batch 5780: loss 0.347008\n",
      "batch 5781: loss 0.339142\n",
      "batch 5782: loss 0.208959\n",
      "batch 5783: loss 0.322264\n",
      "batch 5784: loss 0.405939\n",
      "batch 5785: loss 0.221984\n",
      "batch 5786: loss 0.369599\n",
      "batch 5787: loss 0.420389\n",
      "batch 5788: loss 0.137570\n",
      "batch 5789: loss 0.399494\n",
      "batch 5790: loss 0.165944\n",
      "batch 5791: loss 0.327867\n",
      "batch 5792: loss 0.296086\n",
      "batch 5793: loss 0.307236\n",
      "batch 5794: loss 0.312016\n",
      "batch 5795: loss 0.370025\n",
      "batch 5796: loss 0.273903\n",
      "batch 5797: loss 0.242876\n",
      "batch 5798: loss 0.398737\n",
      "batch 5799: loss 0.345664\n",
      "batch 5800: loss 0.318171\n",
      "batch 5801: loss 0.519350\n",
      "batch 5802: loss 0.263438\n",
      "batch 5803: loss 0.314669\n",
      "batch 5804: loss 0.307196\n",
      "batch 5805: loss 0.217268\n",
      "batch 5806: loss 0.293957\n",
      "batch 5807: loss 0.262736\n",
      "batch 5808: loss 0.235275\n",
      "batch 5809: loss 0.254711\n",
      "batch 5810: loss 0.207914\n",
      "batch 5811: loss 0.524310\n",
      "batch 5812: loss 0.261230\n",
      "batch 5813: loss 0.349026\n",
      "batch 5814: loss 0.486476\n",
      "batch 5815: loss 0.343411\n",
      "batch 5816: loss 0.381745\n",
      "batch 5817: loss 0.208806\n",
      "batch 5818: loss 0.318667\n",
      "batch 5819: loss 0.196007\n",
      "batch 5820: loss 0.364018\n",
      "batch 5821: loss 0.412449\n",
      "batch 5822: loss 0.218473\n",
      "batch 5823: loss 0.391866\n",
      "batch 5824: loss 0.363270\n",
      "batch 5825: loss 0.404755\n",
      "batch 5826: loss 0.191749\n",
      "batch 5827: loss 0.398095\n",
      "batch 5828: loss 0.303823\n",
      "batch 5829: loss 0.452437\n",
      "batch 5830: loss 0.158432\n",
      "batch 5831: loss 0.419743\n",
      "batch 5832: loss 0.260807\n",
      "batch 5833: loss 0.370672\n",
      "batch 5834: loss 0.321488\n",
      "batch 5835: loss 0.209013\n",
      "batch 5836: loss 0.156183\n",
      "batch 5837: loss 0.260541\n",
      "batch 5838: loss 0.512769\n",
      "batch 5839: loss 0.150689\n",
      "batch 5840: loss 0.238234\n",
      "batch 5841: loss 0.181244\n",
      "batch 5842: loss 0.277654\n",
      "batch 5843: loss 0.247457\n",
      "batch 5844: loss 0.322973\n",
      "batch 5845: loss 0.169488\n",
      "batch 5846: loss 0.270156\n",
      "batch 5847: loss 0.248500\n",
      "batch 5848: loss 0.329420\n",
      "batch 5849: loss 0.513916\n",
      "batch 5850: loss 0.340049\n",
      "batch 5851: loss 0.400952\n",
      "batch 5852: loss 0.297353\n",
      "batch 5853: loss 0.319085\n",
      "batch 5854: loss 0.179712\n",
      "batch 5855: loss 0.164118\n",
      "batch 5856: loss 0.257348\n",
      "batch 5857: loss 0.225787\n",
      "batch 5858: loss 0.436069\n",
      "batch 5859: loss 0.287009\n",
      "batch 5860: loss 0.356366\n",
      "batch 5861: loss 0.290718\n",
      "batch 5862: loss 0.401793\n",
      "batch 5863: loss 0.358078\n",
      "batch 5864: loss 0.103305\n",
      "batch 5865: loss 0.203470\n",
      "batch 5866: loss 0.272830\n",
      "batch 5867: loss 0.424541\n",
      "batch 5868: loss 0.325272\n",
      "batch 5869: loss 0.258261\n",
      "batch 5870: loss 0.298586\n",
      "batch 5871: loss 0.293652\n",
      "batch 5872: loss 0.403112\n",
      "batch 5873: loss 0.282304\n",
      "batch 5874: loss 0.247426\n",
      "batch 5875: loss 0.560898\n",
      "batch 5876: loss 0.301791\n",
      "batch 5877: loss 0.338232\n",
      "batch 5878: loss 0.275388\n",
      "batch 5879: loss 0.231129\n",
      "batch 5880: loss 0.163669\n",
      "batch 5881: loss 0.415780\n",
      "batch 5882: loss 0.268611\n",
      "batch 5883: loss 0.357700\n",
      "batch 5884: loss 0.347241\n",
      "batch 5885: loss 0.207877\n",
      "batch 5886: loss 0.521768\n",
      "batch 5887: loss 0.315783\n",
      "batch 5888: loss 0.279286\n",
      "batch 5889: loss 0.296242\n",
      "batch 5890: loss 0.320636\n",
      "batch 5891: loss 0.238093\n",
      "batch 5892: loss 0.381388\n",
      "batch 5893: loss 0.295134\n",
      "batch 5894: loss 0.188361\n",
      "batch 5895: loss 0.287740\n",
      "batch 5896: loss 0.437331\n",
      "batch 5897: loss 0.317931\n",
      "batch 5898: loss 0.296062\n",
      "batch 5899: loss 0.332372\n",
      "batch 5900: loss 0.219404\n",
      "batch 5901: loss 0.317108\n",
      "batch 5902: loss 0.222363\n",
      "batch 5903: loss 0.494251\n",
      "batch 5904: loss 0.391144\n",
      "batch 5905: loss 0.274428\n",
      "batch 5906: loss 0.227612\n",
      "batch 5907: loss 0.334783\n",
      "batch 5908: loss 0.316599\n",
      "batch 5909: loss 0.263565\n",
      "batch 5910: loss 0.393291\n",
      "batch 5911: loss 0.220909\n",
      "batch 5912: loss 0.283425\n",
      "batch 5913: loss 0.318590\n",
      "batch 5914: loss 0.211789\n",
      "batch 5915: loss 0.230187\n",
      "batch 5916: loss 0.171937\n",
      "batch 5917: loss 0.390592\n",
      "batch 5918: loss 0.352051\n",
      "batch 5919: loss 0.353317\n",
      "batch 5920: loss 0.225782\n",
      "batch 5921: loss 0.366992\n",
      "batch 5922: loss 0.188025\n",
      "batch 5923: loss 0.170236\n",
      "batch 5924: loss 0.283471\n",
      "batch 5925: loss 0.144520\n",
      "batch 5926: loss 0.313587\n",
      "batch 5927: loss 0.242926\n",
      "batch 5928: loss 0.176047\n",
      "batch 5929: loss 0.207136\n",
      "batch 5930: loss 0.304085\n",
      "batch 5931: loss 0.262404\n",
      "batch 5932: loss 0.426854\n",
      "batch 5933: loss 0.374964\n",
      "batch 5934: loss 0.290448\n",
      "batch 5935: loss 0.266262\n",
      "batch 5936: loss 0.250166\n",
      "batch 5937: loss 0.337798\n",
      "batch 5938: loss 0.194957\n",
      "batch 5939: loss 0.312007\n",
      "batch 5940: loss 0.214370\n",
      "batch 5941: loss 0.347033\n",
      "batch 5942: loss 0.379551\n",
      "batch 5943: loss 0.266333\n",
      "batch 5944: loss 0.213453\n",
      "batch 5945: loss 0.300362\n",
      "batch 5946: loss 0.210968\n",
      "batch 5947: loss 0.364794\n",
      "batch 5948: loss 0.140063\n",
      "batch 5949: loss 0.329909\n",
      "batch 5950: loss 0.273826\n",
      "batch 5951: loss 0.193498\n",
      "batch 5952: loss 0.451263\n",
      "batch 5953: loss 0.310450\n",
      "batch 5954: loss 0.179468\n",
      "batch 5955: loss 0.236795\n",
      "batch 5956: loss 0.272174\n",
      "batch 5957: loss 0.411933\n",
      "batch 5958: loss 0.362200\n",
      "batch 5959: loss 0.452366\n",
      "batch 5960: loss 0.380410\n",
      "batch 5961: loss 0.202526\n",
      "batch 5962: loss 0.222100\n",
      "batch 5963: loss 0.271992\n",
      "batch 5964: loss 0.482215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5965: loss 0.281418\n",
      "batch 5966: loss 0.512075\n",
      "batch 5967: loss 0.171127\n",
      "batch 5968: loss 0.493684\n",
      "batch 5969: loss 0.382854\n",
      "batch 5970: loss 0.283124\n",
      "batch 5971: loss 0.313297\n",
      "batch 5972: loss 0.272003\n",
      "batch 5973: loss 0.271540\n",
      "batch 5974: loss 0.566423\n",
      "batch 5975: loss 0.421532\n",
      "batch 5976: loss 0.481453\n",
      "batch 5977: loss 0.379104\n",
      "batch 5978: loss 0.084789\n",
      "batch 5979: loss 0.231094\n",
      "batch 5980: loss 0.323972\n",
      "batch 5981: loss 0.203748\n",
      "batch 5982: loss 0.331460\n",
      "batch 5983: loss 0.320321\n",
      "batch 5984: loss 0.209457\n",
      "batch 5985: loss 0.388184\n",
      "batch 5986: loss 0.423866\n",
      "batch 5987: loss 0.259332\n",
      "batch 5988: loss 0.337803\n",
      "batch 5989: loss 0.317849\n",
      "batch 5990: loss 0.250905\n",
      "batch 5991: loss 0.307744\n",
      "batch 5992: loss 0.281469\n",
      "batch 5993: loss 0.452471\n",
      "batch 5994: loss 0.443259\n",
      "batch 5995: loss 0.244160\n",
      "batch 5996: loss 0.219937\n",
      "batch 5997: loss 0.290030\n",
      "batch 5998: loss 0.289733\n",
      "batch 5999: loss 0.579096\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"mlp_3\" with a weight list of length 4, but the layer was expecting 0 weights. Provided weights: [array([[-0.02785639, -0.01034638,  0.05131935, .....",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ed6a9f542ee9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# transfer the weights from the trained model to this model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1808\u001b[0m           \u001b[1;34m'with a weight list of length %s, but the layer was '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m           \u001b[1;34m'expecting %s weights. Provided weights: %s...'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m           (self.name, len(weights), expected_num_weights, str(weights)[:50]))\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m     \u001b[0mweight_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"mlp_3\" with a weight list of length 4, but the layer was expecting 0 weights. Provided weights: [array([[-0.02785639, -0.01034638,  0.05131935, ....."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.tra\n",
    "    def call(self, inputs, training=None):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dropout_layer(x, training=self.training)\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.dropout_layer(x, training=self.training)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP(training=True)\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    \n",
    "# build a clone of the model with dropouts deactivated in test phase\n",
    "test_model = MLP(training=False)\n",
    "# transfer the weights from the trained model to this model\n",
    "test_model.set_weights(model.get_weights())\n",
    "\n",
    "\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = test_model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在網路加上Dropout(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 0s - loss: 14.1817 - accuracy: 0.3163 - val_loss: 9.7888 - val_accuracy: 0.4633\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 8.7625 - accuracy: 0.5245 - val_loss: 7.5016 - val_accuracy: 0.6594\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 6.6568 - accuracy: 0.6931 - val_loss: 5.6912 - val_accuracy: 0.8068\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 5.0692 - accuracy: 0.8050 - val_loss: 4.3518 - val_accuracy: 0.8623\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 3.8899 - accuracy: 0.8499 - val_loss: 3.3255 - val_accuracy: 0.8871\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 3.0141 - accuracy: 0.8778 - val_loss: 2.6012 - val_accuracy: 0.8973\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 2.3885 - accuracy: 0.8901 - val_loss: 2.0736 - val_accuracy: 0.9047\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 1.9300 - accuracy: 0.8965 - val_loss: 1.6778 - val_accuracy: 0.9108\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 1.5942 - accuracy: 0.8996 - val_loss: 1.3969 - val_accuracy: 0.9160\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 1.3507 - accuracy: 0.9006 - val_loss: 1.1873 - val_accuracy: 0.9177\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 1.1610 - accuracy: 0.9067 - val_loss: 1.0232 - val_accuracy: 0.9194\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 1.0261 - accuracy: 0.9053 - val_loss: 0.9001 - val_accuracy: 0.9224\n",
      "Epoch 13/100\n",
      "165/165 - 1s - loss: 0.9265 - accuracy: 0.9076 - val_loss: 0.8203 - val_accuracy: 0.9229\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.8451 - accuracy: 0.9106 - val_loss: 0.7562 - val_accuracy: 0.9226\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.7921 - accuracy: 0.9109 - val_loss: 0.7041 - val_accuracy: 0.9251\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.7452 - accuracy: 0.9116 - val_loss: 0.6655 - val_accuracy: 0.9271\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.7087 - accuracy: 0.9122 - val_loss: 0.6463 - val_accuracy: 0.9230\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.6897 - accuracy: 0.9131 - val_loss: 0.6134 - val_accuracy: 0.9278\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.6669 - accuracy: 0.9141 - val_loss: 0.6054 - val_accuracy: 0.9262\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.6586 - accuracy: 0.9139 - val_loss: 0.5865 - val_accuracy: 0.9278\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.6358 - accuracy: 0.9161 - val_loss: 0.5725 - val_accuracy: 0.9277\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.6308 - accuracy: 0.9152 - val_loss: 0.5648 - val_accuracy: 0.9284\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.6157 - accuracy: 0.9175 - val_loss: 0.5606 - val_accuracy: 0.9270\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.6105 - accuracy: 0.9155 - val_loss: 0.5491 - val_accuracy: 0.9287\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.5979 - accuracy: 0.9168 - val_loss: 0.5461 - val_accuracy: 0.9298\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.5948 - accuracy: 0.9171 - val_loss: 0.5411 - val_accuracy: 0.9294\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.5897 - accuracy: 0.9173 - val_loss: 0.5352 - val_accuracy: 0.9313\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.5829 - accuracy: 0.9187 - val_loss: 0.5293 - val_accuracy: 0.9303\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.5759 - accuracy: 0.9200 - val_loss: 0.5245 - val_accuracy: 0.9292\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.5671 - accuracy: 0.9209 - val_loss: 0.5227 - val_accuracy: 0.9298\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9213 - val_loss: 0.5199 - val_accuracy: 0.9303\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9203 - val_loss: 0.5171 - val_accuracy: 0.9290\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.5604 - accuracy: 0.9198 - val_loss: 0.5185 - val_accuracy: 0.9292\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.5582 - accuracy: 0.9210 - val_loss: 0.5112 - val_accuracy: 0.9312\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.5570 - accuracy: 0.9205 - val_loss: 0.5140 - val_accuracy: 0.9309\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.5487 - accuracy: 0.9221 - val_loss: 0.5092 - val_accuracy: 0.9297\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.5438 - accuracy: 0.9223 - val_loss: 0.5055 - val_accuracy: 0.9309\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.5444 - accuracy: 0.9219 - val_loss: 0.5001 - val_accuracy: 0.9326\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.5390 - accuracy: 0.9240 - val_loss: 0.4971 - val_accuracy: 0.9332\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.5379 - accuracy: 0.9242 - val_loss: 0.4976 - val_accuracy: 0.9321\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.5374 - accuracy: 0.9237 - val_loss: 0.4995 - val_accuracy: 0.9306\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.5348 - accuracy: 0.9245 - val_loss: 0.4930 - val_accuracy: 0.9322\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.5343 - accuracy: 0.9242 - val_loss: 0.4905 - val_accuracy: 0.9332\n",
      "Epoch 44/100\n",
      "165/165 - 1s - loss: 0.5264 - accuracy: 0.9253 - val_loss: 0.4931 - val_accuracy: 0.9322\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.5281 - accuracy: 0.9244 - val_loss: 0.4850 - val_accuracy: 0.9335\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.5234 - accuracy: 0.9256 - val_loss: 0.4875 - val_accuracy: 0.9323\n",
      "Epoch 47/100\n",
      "165/165 - 1s - loss: 0.5276 - accuracy: 0.9245 - val_loss: 0.4881 - val_accuracy: 0.9334\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.5199 - accuracy: 0.9250 - val_loss: 0.4839 - val_accuracy: 0.9316\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.5149 - accuracy: 0.9263 - val_loss: 0.4797 - val_accuracy: 0.9343\n",
      "Epoch 50/100\n",
      "165/165 - 1s - loss: 0.5153 - accuracy: 0.9250 - val_loss: 0.4886 - val_accuracy: 0.9321\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.5123 - accuracy: 0.9273 - val_loss: 0.4800 - val_accuracy: 0.9329\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.5116 - accuracy: 0.9266 - val_loss: 0.4745 - val_accuracy: 0.9337\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.5094 - accuracy: 0.9259 - val_loss: 0.4779 - val_accuracy: 0.9329\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.5077 - accuracy: 0.9267 - val_loss: 0.4780 - val_accuracy: 0.9316\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.5067 - accuracy: 0.9287 - val_loss: 0.4776 - val_accuracy: 0.9331\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9278 - val_loss: 0.4728 - val_accuracy: 0.9337\n",
      "Epoch 57/100\n",
      "165/165 - 1s - loss: 0.5076 - accuracy: 0.9266 - val_loss: 0.4782 - val_accuracy: 0.9333\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9289 - val_loss: 0.4702 - val_accuracy: 0.9336\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.4998 - accuracy: 0.9287 - val_loss: 0.4792 - val_accuracy: 0.9318\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.4975 - accuracy: 0.9294 - val_loss: 0.4742 - val_accuracy: 0.9326\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.5011 - accuracy: 0.9300 - val_loss: 0.4796 - val_accuracy: 0.9316\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.4969 - accuracy: 0.9286 - val_loss: 0.4752 - val_accuracy: 0.9314\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9288 - val_loss: 0.4718 - val_accuracy: 0.9326\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.4955 - accuracy: 0.9271 - val_loss: 0.4777 - val_accuracy: 0.9327\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.4984 - accuracy: 0.9280 - val_loss: 0.4702 - val_accuracy: 0.9345\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.4913 - accuracy: 0.9291 - val_loss: 0.4693 - val_accuracy: 0.9334\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.4889 - accuracy: 0.9300 - val_loss: 0.4777 - val_accuracy: 0.9301\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9278 - val_loss: 0.4670 - val_accuracy: 0.9341\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.4850 - accuracy: 0.9293 - val_loss: 0.4667 - val_accuracy: 0.9334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.4867 - accuracy: 0.9314 - val_loss: 0.4743 - val_accuracy: 0.9331\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.4890 - accuracy: 0.9289 - val_loss: 0.4623 - val_accuracy: 0.9336\n",
      "Epoch 72/100\n",
      "165/165 - 1s - loss: 0.4828 - accuracy: 0.9304 - val_loss: 0.4732 - val_accuracy: 0.9339\n",
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.4846 - accuracy: 0.9300 - val_loss: 0.4667 - val_accuracy: 0.9334\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.4848 - accuracy: 0.9302 - val_loss: 0.4707 - val_accuracy: 0.9324\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9306 - val_loss: 0.4600 - val_accuracy: 0.9340\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.4789 - accuracy: 0.9309 - val_loss: 0.4571 - val_accuracy: 0.9346\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9298 - val_loss: 0.4627 - val_accuracy: 0.9338\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.4753 - accuracy: 0.9305 - val_loss: 0.4604 - val_accuracy: 0.9335\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.4816 - accuracy: 0.9292 - val_loss: 0.4573 - val_accuracy: 0.9334\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9320 - val_loss: 0.4720 - val_accuracy: 0.9319\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.4755 - accuracy: 0.9318 - val_loss: 0.4676 - val_accuracy: 0.9329\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.4739 - accuracy: 0.9315 - val_loss: 0.4653 - val_accuracy: 0.9318\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.4750 - accuracy: 0.9308 - val_loss: 0.4587 - val_accuracy: 0.9337\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.4709 - accuracy: 0.9312 - val_loss: 0.4622 - val_accuracy: 0.9342\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9322 - val_loss: 0.4605 - val_accuracy: 0.9322\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9324 - val_loss: 0.4633 - val_accuracy: 0.9346\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9318 - val_loss: 0.4550 - val_accuracy: 0.9338\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.4642 - accuracy: 0.9352 - val_loss: 0.4486 - val_accuracy: 0.9373\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.4582 - accuracy: 0.9360 - val_loss: 0.4470 - val_accuracy: 0.9382\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.4615 - accuracy: 0.9343 - val_loss: 0.4427 - val_accuracy: 0.9377\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.4569 - accuracy: 0.9354 - val_loss: 0.4481 - val_accuracy: 0.9366\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.4549 - accuracy: 0.9365 - val_loss: 0.4426 - val_accuracy: 0.9384\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.4567 - accuracy: 0.9356 - val_loss: 0.4424 - val_accuracy: 0.9369\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.4518 - accuracy: 0.9362 - val_loss: 0.4480 - val_accuracy: 0.9379\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.4573 - accuracy: 0.9364 - val_loss: 0.4424 - val_accuracy: 0.9392\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.4529 - accuracy: 0.9360 - val_loss: 0.4448 - val_accuracy: 0.9369\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.4499 - accuracy: 0.9362 - val_loss: 0.4388 - val_accuracy: 0.9394\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.4468 - accuracy: 0.9372 - val_loss: 0.4426 - val_accuracy: 0.9377\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.4513 - accuracy: 0.9356 - val_loss: 0.4489 - val_accuracy: 0.9382\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.4455 - accuracy: 0.9381 - val_loss: 0.4421 - val_accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 601us/step - loss: 0.4262 - accuracy: 0.9404\n",
      "[0.42615756392478943, 0.9404000043869019]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
